--- commit message suggestion ---
Prompt:
You write high-quality git commit messages.

        Model configuration:
        - Model: gpt-5-codex
        - Reasoning effort: high

        Diff for the staged changes:
        ```diff
        diff --git a/Makefile b/Makefile
index 4699614..3bc2db7 100644
--- a/Makefile
+++ b/Makefile
@@ -9,10 +9,9 @@ SHARED_CODESPELL_IGNORE = config/codespell_ignore_words.txt
 SHARED_PYRIGHT_TARGETS = ci_tools
 SHARED_PYLINT_TARGETS = ci_tools
 SHARED_PYTEST_TARGET = tests
-SHARED_PYTEST_COV_TARGET = scripts.complexity_guard
+SHARED_PYTEST_COV_TARGET = ci_tools
 SHARED_PYTEST_THRESHOLD = 80
-SHARED_PYTEST_EXTRA = --strict-markers --cov-report=term -W error
-PYLINT_ARGS = --ignore=ci_tools/vendor --disable=C0114,C0115,C0116,C0301,C0302,R0912,R0914,R0915,R0911,R0913,R1732,W1510,R0801
+SHARED_PYTEST_EXTRA = --strict-markers --cov-report=term
 COMPLEXITY_GUARD_PATH = scripts/complexity_guard.py
 COMPLEXITY_GUARD_ARGS = --root $(SHARED_SOURCE_ROOT) --max-cyclomatic 10 --max-cognitive 15
 MODULE_GUARD_ARGS = --root $(SHARED_SOURCE_ROOT) --max-module-lines 400
diff --git a/ci_shared.mk b/ci_shared.mk
index 291ab26..c5ee8c6 100644
--- a/ci_shared.mk
+++ b/ci_shared.mk
@@ -5,6 +5,23 @@

 # Shared variables (can be overridden in individual Makefiles)
 FORMAT_TARGETS ?= src tests
+SHARED_SOURCE_ROOT ?= src
+SHARED_TEST_ROOT ?= tests
+SHARED_DOC_ROOT ?= .
+SHARED_CODESPELL_IGNORE ?= config/codespell_ignore_words.txt
+SHARED_PYRIGHT_TARGETS ?= $(SHARED_SOURCE_ROOT)
+SHARED_PYLINT_TARGETS ?= $(SHARED_SOURCE_ROOT)
+SHARED_PYTEST_TARGET ?= $(SHARED_TEST_ROOT)
+SHARED_PYTEST_COV_TARGET ?= $(SHARED_SOURCE_ROOT)
+SHARED_PYTEST_THRESHOLD ?= 80
+SHARED_PYTEST_EXTRA ?=
+COMPLEXITY_GUARD_PATH ?= scripts/complexity_guard.py
+COMPLEXITY_GUARD_ARGS ?= --root $(SHARED_SOURCE_ROOT) --max-cyclomatic 10 --max-cognitive 15
+MODULE_GUARD_ARGS ?= --root $(SHARED_SOURCE_ROOT) --max-module-lines 400
+FUNCTION_GUARD_ARGS ?= --root $(SHARED_SOURCE_ROOT) --max-function-lines 80
+METHOD_COUNT_GUARD_ARGS ?= --root $(SHARED_SOURCE_ROOT) --max-public-methods 15 --max-total-methods 25
+PYLINT_ARGS ?=
+
 PYTEST_NODES ?= 7
 PYTHON ?= python
 # MAX_CLASS_LINES moved to config/ci_config.json
@@ -17,24 +34,26 @@ shared-checks:
 	@echo "Running shared CI checks..."
 	isort --profile black $(FORMAT_TARGETS)
 	black $(FORMAT_TARGETS)
-	$(PYTHON) -m compileall src tests
-	codespell --skip=".git,artifacts,models,node_modules,logs,htmlcov,*.json,*.csv" --quiet-level=2 --ignore-words=ci_shared/config/codespell_ignore_words.txt
+	codespell --skip=".git,artifacts,models,node_modules,logs,htmlcov,*.json,*.csv" --quiet-level=2 --ignore-words=$(SHARED_CODESPELL_IGNORE)
 	vulture $(FORMAT_TARGETS) --min-confidence 80
 	deptry --config pyproject.toml $(FORMAT_TARGETS)
 	$(PYTHON) -m ci_tools.scripts.policy_guard
 	$(PYTHON) -m ci_tools.scripts.data_guard
-	$(PYTHON) -m ci_tools.scripts.structure_guard
-	$(PYTHON) ci_shared/scripts/complexity_guard.py --root src --max-cyclomatic 10 --max-cognitive 15
-	$(PYTHON) -m ci_tools.scripts.module_guard --root src --max-module-lines 400
-	$(PYTHON) -m ci_tools.scripts.function_size_guard --root src --max-function-lines 80
-	$(PYTHON) -m ci_tools.scripts.inheritance_guard --root src --max-depth 2
-	$(PYTHON) -m ci_tools.scripts.method_count_guard
-	$(PYTHON) -m ci_tools.scripts.dependency_guard --root src --max-instantiations 5
-	$(PYTHON) -m ci_tools.scripts.unused_module_guard --root src --strict
-	$(PYTHON) -m ci_tools.scripts.documentation_guard --root .
-	ruff check --target-version=py310 --fix src tests
-	pyright src
-	pylint -j $(PYTEST_NODES) src tests
-	pytest -n $(PYTEST_NODES) tests/ --cov=src --cov-fail-under=80 --strict-markers --cov-report=term -W error
+	$(PYTHON) -m ci_tools.scripts.structure_guard --root $(SHARED_SOURCE_ROOT)
+	$(PYTHON) $(COMPLEXITY_GUARD_PATH) $(COMPLEXITY_GUARD_ARGS)
+	$(PYTHON) -m ci_tools.scripts.module_guard $(MODULE_GUARD_ARGS)
+	$(PYTHON) -m ci_tools.scripts.function_size_guard $(FUNCTION_GUARD_ARGS)
+	$(PYTHON) -m ci_tools.scripts.inheritance_guard --root $(SHARED_SOURCE_ROOT) --max-depth 2
+	$(PYTHON) -m ci_tools.scripts.method_count_guard $(METHOD_COUNT_GUARD_ARGS)
+	$(PYTHON) -m ci_tools.scripts.dependency_guard --root $(SHARED_SOURCE_ROOT) --max-instantiations 5
+	$(PYTHON) -m ci_tools.scripts.unused_module_guard --root $(SHARED_SOURCE_ROOT) --strict
+	$(PYTHON) -m ci_tools.scripts.documentation_guard --root $(SHARED_DOC_ROOT)
+	ruff check --target-version=py310 --fix $(SHARED_SOURCE_ROOT) $(SHARED_TEST_ROOT)
+	pyright --warnings $(SHARED_PYRIGHT_TARGETS)
+	pylint -j $(PYTEST_NODES) $(PYLINT_ARGS) $(SHARED_PYLINT_TARGETS)
+	@find . -name "*.pyc" -delete 2>/dev/null || true
+	@find . -type d -name __pycache__ -exec rm -rf {} + 2>/dev/null || true
+	pytest -n $(PYTEST_NODES) $(SHARED_PYTEST_TARGET) --cov=$(SHARED_PYTEST_COV_TARGET) --cov-fail-under=$(SHARED_PYTEST_THRESHOLD) $(SHARED_PYTEST_EXTRA)
 	$(PYTHON) -m ci_tools.scripts.coverage_guard --threshold 80 --data-file "$(CURDIR)/.coverage"
+	$(PYTHON) -m compileall $(SHARED_SOURCE_ROOT) $(SHARED_TEST_ROOT)
 	@echo "✅ All shared CI checks passed!"
diff --git a/ci_tools/__main__.py b/ci_tools/__main__.py
index 12930ad..2543892 100644
--- a/ci_tools/__main__.py
+++ b/ci_tools/__main__.py
@@ -1,5 +1,6 @@
-from .ci import main
+"""Console entry point for the shared CI tooling package."""

+from .ci import main

 if __name__ == "__main__":
     raise SystemExit(main())
diff --git a/ci_tools/_messages.py b/ci_tools/_messages.py
new file mode 100644
index 0000000..df46a6e
--- /dev/null
+++ b/ci_tools/_messages.py
@@ -0,0 +1,15 @@
+"""Shared helpers for formatting exception messages."""
+
+from __future__ import annotations
+
+from typing import Optional
+
+
+def format_default_message(default_message: str, detail: Optional[str]) -> str:
+    """Return the formatted message used by our exception helpers."""
+    if detail is None:
+        return default_message
+    return f"{default_message}: {detail}"
+
+
+__all__ = ["format_default_message"]
diff --git a/ci_tools/ci.py b/ci_tools/ci.py
index 1899dae..24600d4 100644
--- a/ci_tools/ci.py
+++ b/ci_tools/ci.py
@@ -2,13 +2,60 @@

 from __future__ import annotations

-from . import ci_runtime as _ci_runtime
+from .ci_runtime import (
+    PatchPrompt,
+    apply_patch,
+    build_codex_command,
+    build_failure_context,
+    commit_and_push,
+    configure_runtime,
+    extract_coverage_deficits,
+    extract_unified_diff,
+    finalize_worktree,
+    gather_file_diff,
+    gather_git_diff,
+    gather_git_status,
+    has_unified_diff_header,
+    invoke_codex,
+    log_codex_interaction,
+    main,
+    patch_looks_risky,
+    perform_dry_run,
+    request_and_apply_patches,
+    request_codex_patch,
+    request_commit_message,
+    run_command,
+    run_repair_iterations,
+    tail_text,
+    truncate_diff_summary,
+    truncate_error,
+)

-__all__ = tuple(getattr(_ci_runtime, "__all__", ()))
-for _name in __all__:
-    globals()[_name] = getattr(_ci_runtime, _name)
-
-# Explicit aliases for static type checkers
-main = _ci_runtime.main
-gather_git_diff = _ci_runtime.gather_git_diff
-request_commit_message = _ci_runtime.request_commit_message
+__all__ = [
+    "apply_patch",
+    "build_codex_command",
+    "build_failure_context",
+    "commit_and_push",
+    "configure_runtime",
+    "extract_coverage_deficits",
+    "extract_unified_diff",
+    "finalize_worktree",
+    "gather_file_diff",
+    "gather_git_diff",
+    "gather_git_status",
+    "has_unified_diff_header",
+    "invoke_codex",
+    "log_codex_interaction",
+    "main",
+    "patch_looks_risky",
+    "perform_dry_run",
+    "request_and_apply_patches",
+    "request_codex_patch",
+    "request_commit_message",
+    "run_command",
+    "run_repair_iterations",
+    "tail_text",
+    "truncate_diff_summary",
+    "truncate_error",
+    "PatchPrompt",
+]
diff --git a/ci_tools/ci_runtime/__init__.py b/ci_tools/ci_runtime/__init__.py
index aae3341..83df837 100644
--- a/ci_tools/ci_runtime/__init__.py
+++ b/ci_tools/ci_runtime/__init__.py
@@ -2,51 +2,63 @@

 from __future__ import annotations

-from .codex import (
-    build_codex_command,
-    extract_unified_diff,
-    has_unified_diff_header,
-    invoke_codex,
-    request_codex_patch,
-    risky_pattern_in_diff,
-    truncate_diff_summary,
-    truncate_error,
-)
-from .coverage import extract_coverage_deficits
-from .failures import build_failure_context
-from .messaging import commit_and_push, request_commit_message
-from .patch_cycle import request_and_apply_patches
-from .patching import apply_patch, patch_looks_risky
-from .process import (
-    gather_file_diff,
-    gather_git_diff,
-    gather_git_status,
-    log_codex_interaction,
-    run_command,
-    tail_text,
-)
-from .workflow import (
-    configure_runtime,
-    finalize_worktree,
-    main,
-    perform_dry_run,
-    run_repair_iterations,
-)
+from typing import TYPE_CHECKING

+from . import codex as _codex
+from . import coverage as _coverage
+from . import failures as _failures
+from . import messaging as _messaging
+from . import patch_cycle as _patch_cycle
+from . import patching as _patching
+from . import process as _process
+from . import workflow as _workflow
+from .models import PatchPrompt
+
+if TYPE_CHECKING:  # pragma: no cover - type checking helper block
+    build_codex_command = _codex.build_codex_command
+    extract_unified_diff = _codex.extract_unified_diff
+    has_unified_diff_header = _codex.has_unified_diff_header
+    invoke_codex = _codex.invoke_codex
+    request_codex_patch = _codex.request_codex_patch
+    risky_pattern_in_diff = _codex.risky_pattern_in_diff
+    truncate_diff_summary = _codex.truncate_diff_summary
+    truncate_error = _codex.truncate_error
+    extract_coverage_deficits = _coverage.extract_coverage_deficits
+    build_failure_context = _failures.build_failure_context
+    commit_and_push = _messaging.commit_and_push
+    request_commit_message = _messaging.request_commit_message
+    request_and_apply_patches = _patch_cycle.request_and_apply_patches
+    apply_patch = _patching.apply_patch
+    patch_looks_risky = _patching.patch_looks_risky
+    gather_file_diff = _process.gather_file_diff
+    gather_git_diff = _process.gather_git_diff
+    gather_git_status = _process.gather_git_status
+    log_codex_interaction = _process.log_codex_interaction
+    run_command = _process.run_command
+    tail_text = _process.tail_text
+    configure_runtime = _workflow.configure_runtime
+    finalize_worktree = _workflow.finalize_worktree
+    main = _workflow.main
+    perform_dry_run = _workflow.perform_dry_run
+    run_repair_iterations = _workflow.run_repair_iterations
+
+_MODULE_EXPORTS = [
+    (_codex, _codex.__all__),
+    (_coverage, _coverage.__all__),
+    (_failures, _failures.__all__),
+    (_messaging, _messaging.__all__),
+    (_patch_cycle, _patch_cycle.__all__),
+    (_patching, _patching.__all__),
+    (_process, _process.__all__),
+    (_workflow, _workflow.__all__),
+]
+
+# Static __all__ definition required for pyright analysis
+# pylint: disable=duplicate-code  # Intentional duplication of module exports
 __all__ = [
-    "configure_runtime",
-    "perform_dry_run",
-    "run_repair_iterations",
-    "finalize_worktree",
-    "main",
-    "request_commit_message",
-    "commit_and_push",
-    "run_command",
-    "tail_text",
-    "gather_git_diff",
-    "gather_git_status",
-    "gather_file_diff",
-    "log_codex_interaction",
+    # models
+    "PatchPrompt",
+    # codex
     "build_codex_command",
     "invoke_codex",
     "request_codex_patch",
@@ -55,9 +67,35 @@ __all__ = [
     "has_unified_diff_header",
     "truncate_diff_summary",
     "risky_pattern_in_diff",
-    "patch_looks_risky",
-    "apply_patch",
+    # coverage
     "extract_coverage_deficits",
+    # failures
     "build_failure_context",
+    # messaging
+    "request_commit_message",
+    "commit_and_push",
+    # patch_cycle
     "request_and_apply_patches",
+    # patching
+    "patch_looks_risky",
+    "apply_patch",
+    # process
+    "run_command",
+    "tail_text",
+    "gather_git_diff",
+    "gather_git_status",
+    "gather_file_diff",
+    "log_codex_interaction",
+    # workflow
+    "main",
+    "configure_runtime",
+    "perform_dry_run",
+    "run_repair_iterations",
+    "finalize_worktree",
 ]
+
+for module, exports in _MODULE_EXPORTS:
+    for name in exports:
+        globals()[name] = getattr(module, name)
+
+globals()["PatchPrompt"] = PatchPrompt
diff --git a/ci_tools/ci_runtime/codex.py b/ci_tools/ci_runtime/codex.py
index 265f2b7..534e468 100644
--- a/ci_tools/ci_runtime/codex.py
+++ b/ci_tools/ci_runtime/codex.py
@@ -9,11 +9,12 @@ import threading
 from typing import Optional

 from .config import RISKY_PATTERNS
-from .models import CodexCliError
+from .models import CodexCliError, PatchPrompt
 from .process import log_codex_interaction


 def build_codex_command(model: str, reasoning_effort: Optional[str]) -> list[str]:
+    """Return the CLI invocation for the codex binary."""
     command = ["codex", "exec", "--model", model, "-"]
     if reasoning_effort:
         command.insert(-1, "-c")
@@ -22,6 +23,7 @@ def build_codex_command(model: str, reasoning_effort: Optional[str]) -> list[str


 def _feed_prompt(process: subprocess.Popen[str], prompt: str) -> None:
+    """Send the prompt to the Codex subprocess and close stdin."""
     try:
         if process.stdin:
             process.stdin.write(prompt)
@@ -31,6 +33,7 @@ def _feed_prompt(process: subprocess.Popen[str], prompt: str) -> None:


 def _stream_output(process: subprocess.Popen[str]) -> tuple[list[str], list[str]]:
+    """Read stdout and stderr from the Codex subprocess concurrently."""
     stdout_lines: list[str] = []
     stderr_lines: list[str] = []

@@ -69,21 +72,23 @@ def invoke_codex(
     description: str,
     reasoning_effort: Optional[str],
 ) -> str:
+    """Execute the Codex CLI and return the assistant's response text."""
     command = build_codex_command(model, reasoning_effort)
-    process = subprocess.Popen(
+    with subprocess.Popen(
         command,
         stdin=subprocess.PIPE,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
         text=True,
         bufsize=1,
-    )
-    feeder = threading.Thread(target=_feed_prompt, args=(process, prompt), daemon=True)
-    feeder.start()
-    feeder.join()
-
-    stdout_lines, stderr_lines = _stream_output(process)
-    returncode = process.wait()
+    ) as process:
+        feeder = threading.Thread(
+            target=_feed_prompt, args=(process, prompt), daemon=True
+        )
+        feeder.start()
+        feeder.join()
+        stdout_lines, stderr_lines = _stream_output(process)
+        returncode = process.wait()
     stdout = "".join(stdout_lines).strip()
     stderr = "".join(stderr_lines).strip()

@@ -99,6 +104,7 @@ def invoke_codex(


 def truncate_error(error: Optional[str], limit: int = 2000) -> str:
+    """Shorten an error message for inclusion in Codex prompts."""
     if not error:
         return "(none)"
     text = error.strip()
@@ -108,6 +114,7 @@ def truncate_error(error: Optional[str], limit: int = 2000) -> str:


 def extract_unified_diff(response_text: str) -> Optional[str]:
+    """Return the first diff block extracted from a Codex response."""
     if not response_text:
         return None
     if response_text.strip().upper() == "NOOP":
@@ -123,6 +130,7 @@ def extract_unified_diff(response_text: str) -> Optional[str]:


 def has_unified_diff_header(diff_text: str) -> bool:
+    """Return True if the text contains the expected unified diff headers."""
     return bool(re.search(r"^(diff --git|--- |\+\+\+ )", diff_text, re.MULTILINE))


@@ -130,47 +138,40 @@ def request_codex_patch(
     *,
     model: str,
     reasoning_effort: str,
-    command: str,
-    log_excerpt: str,
-    summary: str,
-    focused_diff: str,
-    git_diff: str,
-    git_status: str,
-    iteration: int,
-    patch_error: Optional[str],
-    attempt: int,
+    prompt: PatchPrompt,
 ) -> str:
-    prompt = textwrap.dedent(
+    """Ask Codex for a patch diff based on the supplied failure context."""
+    prompt_text = textwrap.dedent(
         f"""\
         You are currently iterating on automated CI repairs.

         Context:
-        - CI command: `{command}`
-        - Iteration: {iteration}
-        - Patch attempt: {attempt}
+        - CI command: `{prompt.command}`
+        - Iteration: {prompt.iteration}
+        - Patch attempt: {prompt.attempt}
         - Git status:
-        {git_status or '(clean)'}
+        {prompt.git_status or '(clean)'}

         Failure summary:
-        {summary or '(not detected)'}
+        {prompt.failure_context.summary or '(not detected)'}

         Focused diff for implicated files:
         ```diff
-        {focused_diff or '/* no focused diff */'}
+        {prompt.failure_context.focused_diff or '/* no focused diff */'}
         ```

         Current diff (unstaged working tree):
         ```diff
-        {git_diff or '/* no diff */'}
+        {prompt.git_diff or '/* no diff */'}
         ```

         Latest CI failure log (tail):
         ```
-        {log_excerpt}
+        {prompt.failure_context.log_excerpt}
         ```

         Previous patch apply error:
-        {truncate_error(patch_error)}
+        {truncate_error(prompt.patch_error)}

         Instructions:
         - Respond ONLY with a unified diff (include `diff --git`, `---`, and `+++` lines) that can be applied with `patch -p1`.
@@ -180,7 +181,7 @@ def request_codex_patch(
         """
     )
     return invoke_codex(
-        prompt,
+        prompt_text,
         model=model,
         description="patch suggestion",
         reasoning_effort=reasoning_effort,
@@ -190,6 +191,7 @@ def request_codex_patch(
 def truncate_diff_summary(
     diff_text: str, line_limit: int
 ) -> tuple[bool, Optional[str]]:
+    """Return whether a diff exceeds the allowed change budget."""
     changed_lines = sum(
         1 for line in diff_text.splitlines() if line.startswith(("+", "-"))
     )
@@ -202,6 +204,7 @@ def truncate_diff_summary(


 def risky_pattern_in_diff(diff_text: str) -> Optional[str]:
+    """Return the first risky pattern matched within the diff text."""
     for pattern in RISKY_PATTERNS:
         if pattern.search(diff_text):
             return pattern.pattern
diff --git a/ci_tools/ci_runtime/config.py b/ci_tools/ci_runtime/config.py
index f41a480..740f135 100644
--- a/ci_tools/ci_runtime/config.py
+++ b/ci_tools/ci_runtime/config.py
@@ -66,31 +66,31 @@ def load_repo_config(repo_root: Path) -> dict[str, Any]:
     return {}


-def _coerce_repo_context(config: dict[str, Any], default: str) -> str:
+def _coerce_repo_context(config: dict[str, Any], initial: str) -> str:
     raw = config.get("repo_context")
     if isinstance(raw, str):
         return raw
-    return default
+    return initial


 def _coerce_protected_prefixes(
     config: dict[str, Any],
-    default: Iterable[str],
+    initial: Iterable[str],
 ) -> tuple[str, ...]:
     raw = config.get("protected_path_prefixes")
     if isinstance(raw, (list, tuple, set)):
         return tuple(str(item) for item in raw)
-    return tuple(default)
+    return tuple(initial)


-def _coerce_coverage_threshold(config: dict[str, Any], default: float) -> float:
+def _coerce_coverage_threshold(config: dict[str, Any], initial: float) -> float:
     raw = config.get("coverage_threshold")
     if isinstance(raw, (int, float, str)):
         try:
             return float(raw)
         except (TypeError, ValueError):  # pragma: no cover - defensive
-            return default
-    return default
+            return initial
+    return initial


 REPO_ROOT = detect_repo_root()
diff --git a/ci_tools/ci_runtime/coverage.py b/ci_tools/ci_runtime/coverage.py
index 2b0d3ed..795108d 100644
--- a/ci_tools/ci_runtime/coverage.py
+++ b/ci_tools/ci_runtime/coverage.py
@@ -9,6 +9,7 @@ from .models import CoverageCheckResult, CoverageDeficit


 def _find_coverage_table(lines: list[str]) -> Optional[list[str]]:
+    """Return the lines that compose the coverage table in the pytest report."""
     header_index: Optional[int] = None
     for idx, line in enumerate(lines):
         stripped = line.strip()
@@ -29,6 +30,7 @@ def _parse_coverage_entries(
     rows: list[str],
     threshold: float,
 ) -> list[CoverageDeficit]:
+    """Parse coverage table rows and collect deficits below the given threshold."""
     deficits: list[CoverageDeficit] = []
     for row in rows:
         stripped = row.strip()
@@ -55,6 +57,7 @@ def _parse_coverage_entries(
 def extract_coverage_deficits(
     output: str, *, threshold: float = COVERAGE_THRESHOLD
 ) -> Optional[CoverageCheckResult]:
+    """Extract modules that fall below the coverage threshold from pytest output."""
     if not output:
         return None
     table = _find_coverage_table(output.splitlines())
diff --git a/ci_tools/ci_runtime/environment.py b/ci_tools/ci_runtime/environment.py
index 96e4e4e..87ca2c4 100644
--- a/ci_tools/ci_runtime/environment.py
+++ b/ci_tools/ci_runtime/environment.py
@@ -7,6 +7,7 @@ from pathlib import Path


 def load_env_file(path: str) -> dict[str, str]:
+    """Parse a simple KEY=VALUE dotenv file into a dictionary."""
     env_path = Path(path).expanduser()
     if not env_path.is_file():
         return {}
@@ -24,6 +25,7 @@ def load_env_file(path: str) -> dict[str, str]:


 def load_env_settings(env_path: str) -> None:
+    """Populate os.environ with defaults from a dotenv-style file."""
     env_values = load_env_file(env_path)
     for key, value in env_values.items():
         os.environ.setdefault(key, value)
diff --git a/ci_tools/ci_runtime/failures.py b/ci_tools/ci_runtime/failures.py
index acb2e01..a2246ef 100644
--- a/ci_tools/ci_runtime/failures.py
+++ b/ci_tools/ci_runtime/failures.py
@@ -15,6 +15,7 @@ from .process import gather_file_diff, tail_text


 def _gather_focused_diff(implicated_files: Iterable[str]) -> str:
+    """Return the per-file git diff for files implicated by the failure."""
     blocks: list[str] = []
     for rel_path in implicated_files:
         diff = gather_file_diff(rel_path)
@@ -24,6 +25,7 @@ def _gather_focused_diff(implicated_files: Iterable[str]) -> str:


 def _render_coverage_context(report: CoverageCheckResult) -> tuple[str, str, list[str]]:
+    """Derive coverage summary text and implicated file list."""
     deficits = [f"- {item.path}: {item.coverage:.1f}%" for item in report.deficits]
     intro = (
         "Coverage guard triggered: add or expand tests so each listed module "
@@ -49,6 +51,7 @@ def build_failure_context(
     result: CommandResult,
     coverage_report: Optional[CoverageCheckResult],
 ) -> FailureContext:
+    """Compile the information Codex needs to produce a follow-up patch."""
     if coverage_report is not None:
         summary, log_excerpt, implicated = _render_coverage_context(coverage_report)
         print(
diff --git a/ci_tools/ci_runtime/heuristics.py b/ci_tools/ci_runtime/heuristics.py
index 2ffb0db..c75c8a2 100644
--- a/ci_tools/ci_runtime/heuristics.py
+++ b/ci_tools/ci_runtime/heuristics.py
@@ -18,6 +18,7 @@ ATTRIBUTE_ERROR_PATTERN = re.compile(


 def detect_missing_symbol_error(log_excerpt: str) -> Optional[str]:
+    """Return a guidance string when an ImportError indicates a missing symbol."""
     match = IMPORT_ERROR_PATTERN.search(log_excerpt)
     if not match:
         return None
@@ -29,6 +30,7 @@ def detect_missing_symbol_error(log_excerpt: str) -> Optional[str]:


 def detect_attribute_error(log_excerpt: str) -> Optional[str]:
+    """Return a guidance string for AttributeError messages referencing repo files."""
     match = ATTRIBUTE_ERROR_PATTERN.search(log_excerpt)
     if not match:
         return None
@@ -56,6 +58,7 @@ def detect_attribute_error(log_excerpt: str) -> Optional[str]:


 def summarize_failure(log_excerpt: str) -> tuple[str, List[str]]:
+    """Summarize failing file locations detected in the log excerpt."""
     lines = log_excerpt.splitlines()
     pyright_matches: List[Tuple[str, str]] = []
     for line in lines:
diff --git a/ci_tools/ci_runtime/messaging.py b/ci_tools/ci_runtime/messaging.py
index ed4f01c..601eca3 100644
--- a/ci_tools/ci_runtime/messaging.py
+++ b/ci_tools/ci_runtime/messaging.py
@@ -20,22 +20,27 @@ def request_commit_message(
     extra_context: str,
     detailed: bool = False,
 ) -> tuple[str, List[str]]:
+    """Ask Codex to produce a commit message for the staged diff."""
     effort_display = reasoning_effort or "default"
     if detailed:
         instructions = textwrap.dedent(
             """\
             Produce a git commit message consisting of:
             - A concise subject line (≤72 characters) that summarizes what changed using past tense.
-            - After a blank line, a short bullet list (five bullets or fewer, each starting with "- ") that highlights key changes using past tense.
+            - After a blank line, include ≤5 bullet points (each starting with "- ").
+            - Each bullet should summarise the key changes using past tense verbs.
             Avoid trailing periods on the subject line.
-            Rely on the diff provided below for context and do not run shell commands such as `diff --git`.
+            Rely on the diff provided below for context instead of running shell commands.
+            Avoid invoking tools such as `diff --git`.
             """
         )
     else:
-        instructions = (
-            "Provide a single-line commit message in past tense (no trailing punctuation). "
-            "Use the diff shown above for context instead of executing shell commands like `diff --git`."
-        )
+        instructions = textwrap.dedent(
+            """\
+            Provide a single-line commit message in past tense (no trailing punctuation).
+            Use the diff shown above instead of running shell commands such as `diff --git`.
+            """
+        ).strip()
     extra_block = extra_context.strip()
     prompt = textwrap.dedent(
         f"""\
@@ -76,6 +81,7 @@ def commit_and_push(
     *,
     push: bool,
 ) -> None:
+    """Create a commit locally and optionally push it to the configured remote."""
     print("[info] Creating commit...")
     commit_args = ["git", "commit", "-m", summary]
     body_text = "\n".join(body_lines).strip()
@@ -89,7 +95,7 @@ def commit_and_push(
     if not push:
         return

-    remote = os.environ.get("GIT_REMOTE", "origin")
+    remote = os.environ.get("GIT_REMOTE") or "origin"
     branch_result = run_command(
         ["git", "rev-parse", "--abbrev-ref", "HEAD"], check=True
     )
diff --git a/ci_tools/ci_runtime/models.py b/ci_tools/ci_runtime/models.py
index d71740e..b0a1f7f 100644
--- a/ci_tools/ci_runtime/models.py
+++ b/ci_tools/ci_runtime/models.py
@@ -6,6 +6,8 @@ import subprocess
 from dataclasses import dataclass
 from typing import Iterable, Optional

+from .._messages import format_default_message
+

 class CiError(RuntimeError):
     """Base class for CI automation runtime failures."""
@@ -13,12 +15,9 @@ class CiError(RuntimeError):
     default_message = "CI automation failure"

     def __init__(self, *, detail: Optional[str] = None) -> None:
+        """Initialise the exception with an optional detail string."""
         self.detail = detail
-        message = (
-            self.default_message
-            if detail is None
-            else f"{self.default_message}: {detail}"
-        )
+        message = format_default_message(self.default_message, detail)
         super().__init__(message)


@@ -29,6 +28,7 @@ class CodexCliError(CiError):

     @classmethod
     def exit_status(cls, *, returncode: int, output: Optional[str]) -> "CodexCliError":
+        """Build an error containing the CLI exit status and captured output."""
         normalized = (output or "").strip() or "(no output)"
         detail = f"exit status {returncode} ({normalized})"
         return cls(detail=detail)
@@ -41,6 +41,7 @@ class CommitMessageError(CiError):

     @classmethod
     def empty_response(cls) -> "CommitMessageError":
+        """Return an error signalling the Codex response was blank."""
         return cls()


@@ -50,13 +51,10 @@ class CiAbort(SystemExit):
     default_message = "CI automation aborted"

     def __init__(self, *, detail: Optional[str] = None, code: int = 1) -> None:
+        """Initialise the abort with a user-facing detail string."""
         self.detail = detail
         self.exit_code = code
-        message = (
-            self.default_message
-            if detail is None
-            else f"{self.default_message}: {detail}"
-        )
+        message = format_default_message(self.default_message, detail)
         super().__init__(message)
         self.code = code

@@ -68,6 +66,7 @@ class GitCommandAbort(CiAbort):

     @classmethod
     def commit_failed(cls, exc: subprocess.CalledProcessError) -> "GitCommandAbort":
+        """Return an error capturing a failed git commit invocation."""
         output = (exc.stderr or exc.output or "").strip()
         detail = f"'git commit' exited with status {exc.returncode}"
         if output:
@@ -76,6 +75,7 @@ class GitCommandAbort(CiAbort):

     @classmethod
     def push_failed(cls, exc: subprocess.CalledProcessError) -> "GitCommandAbort":
+        """Return an error capturing a failed git push invocation."""
         output = (exc.stderr or exc.output or "").strip()
         detail = f"'git push' exited with status {exc.returncode}"
         if output:
@@ -90,6 +90,7 @@ class RepositoryStateAbort(CiAbort):

     @classmethod
     def detached_head(cls) -> "RepositoryStateAbort":
+        """Factory raised when running outside a branch (detached HEAD)."""
         return cls(
             detail="detached HEAD detected; checkout a branch before running ci.py"
         )
@@ -104,6 +105,7 @@ class ModelSelectionAbort(CiAbort):
     def unsupported_model(
         cls, *, received: str, required: str
     ) -> "ModelSelectionAbort":
+        """Factory when a CLI caller passes an unsupported model."""
         return cls(detail=f"requires `{required}` but received `{received}`")


@@ -116,6 +118,7 @@ class ReasoningEffortAbort(CiAbort):
     def unsupported_choice(
         cls, *, received: str, allowed: Iterable[str]
     ) -> "ReasoningEffortAbort":
+        """Factory when the reasoning effort flag is not recognised."""
         choices = ", ".join(allowed)
         return cls(detail=f"expected one of {choices}; received `{received}`")

@@ -127,18 +130,22 @@ class PatchLifecycleAbort(CiAbort):

     @classmethod
     def attempts_exhausted(cls) -> "PatchLifecycleAbort":
+        """Factory when Codex could not produce a valid patch in time."""
         return cls(detail="unable to obtain a valid patch after multiple attempts")

     @classmethod
     def missing_patch(cls) -> "PatchLifecycleAbort":
+        """Factory when Codex responded without a usable patch diff."""
         return cls(detail="Codex returned an empty or NOOP patch response")

     @classmethod
     def user_declined(cls) -> "PatchLifecycleAbort":
+        """Factory when the user stops automation during a patch cycle."""
         return cls(detail="user declined CI automation")

     @classmethod
     def retries_exhausted(cls) -> "PatchLifecycleAbort":
+        """Factory when patch retries were exhausted after repeated failures."""
         return cls(
             detail="Codex patches failed after exhausting retries; manual review required"
         )
@@ -146,21 +153,27 @@ class PatchLifecycleAbort(CiAbort):

 @dataclass
 class CommandResult:
+    """Captured output from a completed subprocess invocation."""
+
     returncode: int
     stdout: str
     stderr: str

     @property
     def ok(self) -> bool:
+        """Return True when the process exited successfully."""
         return self.returncode == 0

     @property
     def combined_output(self) -> str:
+        """Return stdout and stderr concatenated together."""
         return f"{self.stdout}{self.stderr}"


 @dataclass
 class RuntimeOptions:  # pylint: disable=too-many-instance-attributes
+    """Configuration flags governing how the CI workflow runs."""
+
     command_tokens: list[str]
     command_env: dict[str, str]
     patch_approval_mode: str
@@ -174,6 +187,8 @@ class RuntimeOptions:  # pylint: disable=too-many-instance-attributes

 @dataclass
 class FailureContext:
+    """Summary of the most recent CI failure provided to Codex."""
+
     log_excerpt: str
     summary: str
     implicated_files: list[str]
@@ -183,16 +198,20 @@ class FailureContext:

 @dataclass
 class PatchAttemptState:
+    """Track Codex patch attempts and retry budget."""
+
     max_attempts: int
     patch_attempt: int = 1
     extra_retry_budget: int = 3
     last_error: Optional[str] = None

     def ensure_budget(self) -> None:
+        """Abort when the patch attempt counter exceeds the allowed budget."""
         if self.patch_attempt > self.max_attempts:
             raise PatchLifecycleAbort.attempts_exhausted()

     def record_failure(self, message: str, *, retryable: bool) -> None:
+        """Record a patch failure and expand the budget when retries remain."""
         self.last_error = message
         if self.patch_attempt >= self.max_attempts:
             if retryable and self.extra_retry_budget > 0:
@@ -214,6 +233,7 @@ class PatchApplyError(CiError):

     @classmethod
     def git_apply_failed(cls, *, output: str) -> "PatchApplyError":
+        """Factory when `git apply` fails to dry-run or apply the diff."""
         normalized = (output or "").strip() or "(no output)"
         detail = f"`git apply` failed: {normalized}"
         return cls(detail=detail, retryable=True)
@@ -222,6 +242,7 @@ class PatchApplyError(CiError):
     def preflight_failed(
         cls, *, check_output: str, dry_output: str
     ) -> "PatchApplyError":
+        """Factory when both git and patch dry-runs are unable to apply."""
         detail = (
             "Patch dry-run failed.\n"
             f"git apply --check output:\n{(check_output or '').strip() or '(none)'}\n\n"
@@ -231,6 +252,7 @@ class PatchApplyError(CiError):

     @classmethod
     def patch_exit(cls, *, returncode: int, output: str) -> "PatchApplyError":
+        """Factory when the POSIX `patch` utility exits with a non-zero code."""
         normalized = (output or "").strip() or "(no output)"
         detail = f"`patch` exited with status {returncode}: {normalized}"
         return cls(detail=detail, retryable=True)
@@ -238,17 +260,34 @@ class PatchApplyError(CiError):

 @dataclass
 class CoverageDeficit:
+    """Coverage percentage for a single module below the configured threshold."""
+
     path: str
     coverage: float


 @dataclass
 class CoverageCheckResult:
+    """Aggregate report returned by the coverage guard."""
+
     table_text: str
     deficits: list[CoverageDeficit]
     threshold: float


+@dataclass
+class PatchPrompt:
+    """Contextual information sent to Codex when requesting a patch."""
+
+    command: str
+    failure_context: FailureContext
+    git_diff: str
+    git_status: str
+    iteration: int
+    patch_error: Optional[str]
+    attempt: int
+
+
 __all__ = [
     "CiError",
     "CodexCliError",
@@ -266,4 +305,5 @@ __all__ = [
     "PatchAttemptState",
     "CoverageDeficit",
     "CoverageCheckResult",
+    "PatchPrompt",
 ]
diff --git a/ci_tools/ci_runtime/patch_cycle.py b/ci_tools/ci_runtime/patch_cycle.py
index 99eaf14..b9f3ade 100644
--- a/ci_tools/ci_runtime/patch_cycle.py
+++ b/ci_tools/ci_runtime/patch_cycle.py
@@ -14,34 +14,19 @@ from .models import (
     PatchApplyError,
     PatchAttemptState,
     PatchLifecycleAbort,
+    PatchPrompt,
     RuntimeOptions,
 )
 from .patching import apply_patch, patch_looks_risky
 from .process import gather_git_diff, gather_git_status


-def _obtain_patch_diff(
-    *,
-    args,
-    options: RuntimeOptions,
-    failure_ctx: FailureContext,
-    iteration: int,
-    git_status: str,
-    git_diff: str,
-    state: PatchAttemptState,
-) -> str:
+def _obtain_patch_diff(*, options: RuntimeOptions, prompt: PatchPrompt) -> str:
+    """Request a patch from Codex and return its diff text."""
     response = request_codex_patch(
         model=options.model_name,
         reasoning_effort=options.reasoning_effort,
-        command=args.command,
-        log_excerpt=failure_ctx.log_excerpt,
-        summary=failure_ctx.summary,
-        focused_diff=failure_ctx.focused_diff,
-        git_diff=git_diff,
-        git_status=git_status,
-        iteration=iteration,
-        patch_error=state.last_error,
-        attempt=state.patch_attempt,
+        prompt=prompt,
     )
     diff_text = extract_unified_diff(response or "")
     if not diff_text:
@@ -55,6 +40,7 @@ def _validate_patch_candidate(
     seen_patches: Set[str],
     max_patch_lines: int,
 ) -> Optional[str]:
+    """Return a validation error string when the diff should be rejected."""
     if diff_text in seen_patches:
         return "Duplicate patch received; provide an alternative diff."
     seen_patches.add(diff_text)
@@ -71,6 +57,7 @@ def _apply_patch_candidate(
     *,
     state: PatchAttemptState,
 ) -> bool:
+    """Apply the diff and update state, returning True on success."""
     try:
         apply_patch(diff_text)
     except PatchApplyError as exc:
@@ -88,6 +75,7 @@ def _should_apply_patch(
     approval_mode: str,
     attempt: int,
 ) -> bool:
+    """Return True when the user (or automation) approves applying the patch."""
     if approval_mode == "auto":
         print(f"[codex] Auto-approving patch attempt {attempt}.")
         return True
@@ -107,23 +95,23 @@ def request_and_apply_patches(
     options: RuntimeOptions,
     failure_ctx: FailureContext,
     iteration: int,
-    git_status: str,
-    git_diff: str,
     seen_patches: Set[str],
 ) -> None:
+    """Iteratively request and apply patches until one succeeds or retries are exhausted."""
     state = PatchAttemptState(max_attempts=args.patch_retries + 1)
     while True:
         state.ensure_budget()
         print(f"[codex] Requesting patch attempt {state.patch_attempt}...")
-        diff_text = _obtain_patch_diff(
-            args=args,
-            options=options,
-            failure_ctx=failure_ctx,
+        prompt = PatchPrompt(
+            command=args.command,
+            failure_context=failure_ctx,
+            git_diff=gather_git_diff(staged=False),
+            git_status=gather_git_status(),
             iteration=iteration,
-            git_status=git_status,
-            git_diff=git_diff,
-            state=state,
+            patch_error=state.last_error,
+            attempt=state.patch_attempt,
         )
+        diff_text = _obtain_patch_diff(options=options, prompt=prompt)
         validation_error = _validate_patch_candidate(
             diff_text,
             seen_patches=seen_patches,
@@ -146,8 +134,9 @@ def request_and_apply_patches(
             else:
                 print("[info] Working tree is clean after applying patch.")
             return
-        git_status = gather_git_status()
-        git_diff = gather_git_diff(staged=False)
+        # Defensive check: ensure state was updated when apply fails
+        if not state.last_error:
+            state.record_failure("Patch application failed", retryable=False)


 __all__ = ["request_and_apply_patches"]
diff --git a/ci_tools/ci_runtime/patching.py b/ci_tools/ci_runtime/patching.py
index d7c49bf..18d3d36 100644
--- a/ci_tools/ci_runtime/patching.py
+++ b/ci_tools/ci_runtime/patching.py
@@ -12,6 +12,7 @@ from .models import PatchApplyError


 def _extract_diff_paths(patch_text: str) -> set[str]:
+    """Return protected file paths touched by the diff headers."""
     protected_paths: set[str] = set()
     for line in patch_text.splitlines():
         if not line.startswith("diff --git"):
@@ -27,6 +28,7 @@ def _extract_diff_paths(patch_text: str) -> set[str]:


 def patch_looks_risky(patch_text: str, *, max_lines: int) -> tuple[bool, Optional[str]]:
+    """Evaluate a Codex patch suggestion for risky patterns or size limits."""
     if not patch_text:
         msg = "Patch content was empty."
         return True, msg
@@ -48,16 +50,19 @@ def patch_looks_risky(patch_text: str, *, max_lines: int) -> tuple[bool, Optiona


 def _ensure_trailing_newline(patch_text: str) -> str:
+    """Guarantee the diff ends with a newline for `patch` compatibility."""
     return patch_text if patch_text.endswith("\n") else f"{patch_text}\n"


 def _apply_patch_with_git(patch_text: str) -> tuple[bool, str]:
+    """Attempt to apply the patch using git; return (applied?, diagnostics)."""
     git_check = subprocess.run(
         ["git", "apply", "--check", "--whitespace=nowarn"],
         input=patch_text,
         text=True,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
+        check=False,
     )
     check_output = (git_check.stdout or "") + (git_check.stderr or "")
     if git_check.returncode != 0:
@@ -68,6 +73,7 @@ def _apply_patch_with_git(patch_text: str) -> tuple[bool, str]:
         text=True,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
+        check=False,
     )
     if git_apply.returncode != 0:
         output = (git_apply.stdout or "") + (git_apply.stderr or "")
@@ -78,12 +84,14 @@ def _apply_patch_with_git(patch_text: str) -> tuple[bool, str]:


 def _patch_already_applied(patch_text: str) -> bool:
+    """Return True when the diff has already been applied to the worktree."""
     git_reverse_check = subprocess.run(
         ["git", "apply", "--check", "--reverse", "--whitespace=nowarn"],
         input=patch_text,
         text=True,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
+        check=False,
     )
     if git_reverse_check.returncode == 0:
         print("[info] Patch already applied according to `git apply`; skipping.")
@@ -96,8 +104,10 @@ def _apply_patch_with_patch_tool(
     *,
     check_output: str,
 ) -> None:
+    """Fallback to the `patch` utility when git cannot apply the diff."""
     env = dict(os.environ)
-    env.setdefault("PATCH_CREATE_BACKUP", "no")
+    if "PATCH_CREATE_BACKUP" not in env:
+        env["PATCH_CREATE_BACKUP"] = "no"
     dry_run_cmd = [
         "patch",
         "--batch",
@@ -113,6 +123,7 @@ def _apply_patch_with_patch_tool(
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
         env=env,
+        check=False,
     )
     if dry_run.returncode != 0:
         dry_output = (dry_run.stdout or "") + (dry_run.stderr or "")
@@ -129,6 +140,7 @@ def _apply_patch_with_patch_tool(
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
         env=env,
+        check=False,
     )
     if actual.returncode != 0:
         output = (actual.stdout or "") + (actual.stderr or "")
@@ -138,6 +150,7 @@ def _apply_patch_with_patch_tool(


 def apply_patch(patch_text: str) -> None:
+    """Apply a diff using git when possible, then fall back to POSIX patch."""
     normalized = _ensure_trailing_newline(patch_text)
     applied, check_output = _apply_patch_with_git(normalized)
     if applied:
diff --git a/ci_tools/ci_runtime/process.py b/ci_tools/ci_runtime/process.py
index 26509d3..ea48664 100644
--- a/ci_tools/ci_runtime/process.py
+++ b/ci_tools/ci_runtime/process.py
@@ -18,12 +18,14 @@ def _run_command_buffered(
     check: bool,
     env: dict[str, str],
 ) -> CommandResult:
+    """Run a subprocess and capture its output without streaming."""
     process = subprocess.run(
         args,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
         text=True,
         env=env,
+        check=False,
     )
     if check and process.returncode != 0:
         raise subprocess.CalledProcessError(
@@ -40,6 +42,7 @@ def _run_command_buffered(


 def _stream_pipe(pipe, collector: list[str], target) -> None:
+    """Collect text from a pipe while forwarding it to the provided stream."""
     try:
         for line in iter(pipe.readline, ""):
             collector.append(line)
@@ -55,42 +58,42 @@ def _run_command_streaming(
     check: bool,
     env: dict[str, str],
 ) -> CommandResult:
-    process = subprocess.Popen(
+    """Stream stdout/stderr live while accumulating the full text."""
+    stdout_lines: list[str] = []
+    stderr_lines: list[str] = []
+    with subprocess.Popen(
         args,
         stdout=subprocess.PIPE,
         stderr=subprocess.PIPE,
         text=True,
         bufsize=1,
         env=env,
-    )
-
-    stdout_lines: list[str] = []
-    stderr_lines: list[str] = []
-    threads: list[threading.Thread] = []
-
-    if process.stdout:
-        threads.append(
-            threading.Thread(
-                target=_stream_pipe,
-                args=(process.stdout, stdout_lines, sys.stdout),
-                daemon=True,
+    ) as process:
+        threads: list[threading.Thread] = []
+
+        if process.stdout:
+            threads.append(
+                threading.Thread(
+                    target=_stream_pipe,
+                    args=(process.stdout, stdout_lines, sys.stdout),
+                    daemon=True,
+                )
             )
-        )
-    if process.stderr:
-        threads.append(
-            threading.Thread(
-                target=_stream_pipe,
-                args=(process.stderr, stderr_lines, sys.stderr),
-                daemon=True,
+        if process.stderr:
+            threads.append(
+                threading.Thread(
+                    target=_stream_pipe,
+                    args=(process.stderr, stderr_lines, sys.stderr),
+                    daemon=True,
+                )
             )
-        )

-    for thread in threads:
-        thread.start()
-    for thread in threads:
-        thread.join()
+        for thread in threads:
+            thread.start()
+        for thread in threads:
+            thread.join()

-    returncode = process.wait()
+        returncode = process.wait()
     stdout_text = "".join(stdout_lines)
     stderr_text = "".join(stderr_lines)

@@ -127,26 +130,31 @@ def run_command(


 def tail_text(text: str, lines: int) -> str:
+    """Return the last *lines* lines from the provided multiline string."""
     return "\n".join(text.splitlines()[-lines:])


 def gather_git_diff(*, staged: bool = False) -> str:
+    """Return the git diff for staged or unstaged changes."""
     args = ["git", "diff", "--cached"] if staged else ["git", "diff"]
     result = run_command(args)
     return result.stdout


 def gather_git_status() -> str:
+    """Return a short git status suitable for prompt summaries."""
     result = run_command(["git", "status", "--short"])
     return result.stdout.strip()


 def gather_file_diff(path: str) -> str:
+    """Return the diff for a single path relative to HEAD."""
     result = run_command(["git", "diff", path])
     return result.stdout


 def log_codex_interaction(kind: str, prompt: str, response: str) -> None:
+    """Append the interaction to logs/codex_ci.log for later auditing."""
     log_dir = Path("logs")
     log_dir.mkdir(parents=True, exist_ok=True)
     log_path = log_dir / "codex_ci.log"
diff --git a/ci_tools/ci_runtime/workflow.py b/ci_tools/ci_runtime/workflow.py
index d41a9fb..b1a5687 100644
--- a/ci_tools/ci_runtime/workflow.py
+++ b/ci_tools/ci_runtime/workflow.py
@@ -26,10 +26,11 @@ from .models import (
     RuntimeOptions,
 )
 from .patch_cycle import request_and_apply_patches
-from .process import gather_git_diff, gather_git_status, run_command
+from .process import gather_git_diff, run_command


 def _resolve_model_choice(model_arg: Optional[str]) -> str:
+    """Validate the requested Codex model against the one we support."""
     candidate = model_arg or os.environ.get("OPENAI_MODEL") or REQUIRED_MODEL
     if candidate != REQUIRED_MODEL:
         raise ModelSelectionAbort.unsupported_model(
@@ -40,6 +41,7 @@ def _resolve_model_choice(model_arg: Optional[str]) -> str:


 def _resolve_reasoning_choice(reasoning_arg: Optional[str]) -> str:
+    """Determine the reasoning effort flag to send to Codex."""
     env_reasoning = os.environ.get("OPENAI_REASONING_EFFORT")
     candidate = (
         reasoning_arg
@@ -57,6 +59,7 @@ def _resolve_reasoning_choice(reasoning_arg: Optional[str]) -> str:
 def _derive_runtime_flags(
     args: argparse.Namespace, command_tokens: list[str]
 ) -> tuple[bool, dict[str, str], bool, bool, bool]:
+    """Derive automation flags based on CLI args and the requested command."""
     command_basename = Path(command_tokens[0]).name if command_tokens else ""
     automation_mode = command_basename == "ci.sh"
     command_env = {"CI_AUTOMATION": "1"} if automation_mode else {}
@@ -73,6 +76,7 @@ def _derive_runtime_flags(


 def configure_runtime(args: argparse.Namespace) -> RuntimeOptions:
+    """Convert parsed CLI arguments into the runtime options dataclass."""
     load_env_settings(args.env_file)
     command_tokens = shlex.split(args.command)
     model_name = _resolve_model_choice(args.model)
@@ -99,6 +103,7 @@ def configure_runtime(args: argparse.Namespace) -> RuntimeOptions:


 def perform_dry_run(args: argparse.Namespace, options: RuntimeOptions) -> Optional[int]:
+    """Run the CI command once when --dry-run is supplied."""
     if not args.dry_run:
         return None
     print("[info] Dry run: executing CI command once without invoking Codex.")
@@ -107,14 +112,17 @@ def perform_dry_run(args: argparse.Namespace, options: RuntimeOptions) -> Option


 def _collect_worktree_diffs() -> tuple[str, str]:
+    """Return both unstaged and staged git diffs."""
     return gather_git_diff(staged=False), gather_git_diff(staged=True)


 def _worktree_is_clean(unstaged_diff: str, staged_diff: str) -> bool:
+    """Return True when there are no staged or unstaged changes."""
     return not unstaged_diff and not staged_diff


 def _stage_if_needed(options: RuntimeOptions, staged_diff: str) -> str:
+    """Stage all changes when auto-stage is enabled and return the staged diff."""
     if not options.auto_stage_enabled:
         return staged_diff
     print("[info] Staging all changes (`git add -A`).")
@@ -123,6 +131,7 @@ def _stage_if_needed(options: RuntimeOptions, staged_diff: str) -> str:


 def _warn_missing_staged_changes() -> None:
+    """Warn when a commit message was requested without staged changes."""
     print(
         "[warn] No staged changes detected. Stage files before requesting a commit message.",
         file=sys.stderr,
@@ -134,6 +143,7 @@ def _maybe_request_commit_message(
     staged_diff: str,
     extra_context: str,
 ) -> tuple[Optional[str], list[str]]:
+    """Request a commit message from Codex when the mode is enabled."""
     if not options.commit_message_enabled:
         return None, []
     summary, body_lines = request_commit_message(
@@ -158,6 +168,7 @@ def _maybe_push_or_notify(
     summary: Optional[str],
     body_lines: list[str],
 ) -> None:
+    """Push automatically or prompt the user to commit manually."""
     if options.auto_push_enabled:
         commit_summary = summary or "Automated commit"
         commit_body = body_lines if summary is not None else []
@@ -168,6 +179,7 @@ def _maybe_push_or_notify(


 def finalize_worktree(args: argparse.Namespace, options: RuntimeOptions) -> int:
+    """Stage, commit, and optionally push once CI passes."""
     unstaged_diff, staged_diff = _collect_worktree_diffs()
     if _worktree_is_clean(unstaged_diff, staged_diff):
         print("[info] Working tree clean. Nothing to stage or commit.")
@@ -186,6 +198,7 @@ def finalize_worktree(args: argparse.Namespace, options: RuntimeOptions) -> int:


 def run_repair_iterations(args: argparse.Namespace, options: RuntimeOptions) -> None:
+    """Loop CI execution and Codex interactions until the command succeeds."""
     seen_patches: Set[str] = set()
     for iteration in range(1, args.max_iterations + 1):
         print(f"[loop] Iteration {iteration} — running `{args.command}`")
@@ -197,64 +210,51 @@ def run_repair_iterations(args: argparse.Namespace, options: RuntimeOptions) ->
             print(f"[loop] CI command succeeded on iteration {iteration}.")
             return
         failure_ctx = build_failure_context(args, result, coverage_report)
-        git_status = gather_git_status()
-        git_diff = gather_git_diff(staged=False)
         request_and_apply_patches(
             args=args,
             options=options,
             failure_ctx=failure_ctx,
             iteration=iteration,
-            git_status=git_status,
-            git_diff=git_diff,
             seen_patches=seen_patches,
         )
     raise PatchLifecycleAbort.attempts_exhausted()


 def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
+    """Parse command-line arguments for the workflow CLI."""
     parser = argparse.ArgumentParser(description="Automate CI fixes via Codex.")
     parser.add_argument(
         "--command",
-        default="./scripts/ci.sh",
-        help="Command to run for CI (default: %(default)s)",
+        help="Command to run for CI (initial: ./scripts/ci.sh)",
     )
     parser.add_argument(
         "--max-iterations",
         type=int,
-        default=5,
-        help="Maximum Codex-assisted fix attempts (default: %(default)s)",
+        help="Maximum Codex-assisted fix attempts (initial: 5)",
     )
     parser.add_argument(
         "--log-tail",
         type=int,
-        default=200,
-        help="Number of log lines from the failure to send to Codex (default: %(default)s)",
+        help="Number of log lines from the failure to send to Codex (initial: 200)",
     )
     parser.add_argument(
         "--model",
-        help=f"Codex model name (default/required: {REQUIRED_MODEL})",
+        help=f"Codex model name (required: {REQUIRED_MODEL})",
     )
     parser.add_argument(
         "--reasoning-effort",
         choices=REASONING_EFFORT_CHOICES,
-        help=(
-            "Reasoning effort hint for Codex " f"(default: {DEFAULT_REASONING_EFFORT})"
-        ),
+        help=f"Reasoning effort hint for Codex (initial: {DEFAULT_REASONING_EFFORT})",
     )
     parser.add_argument(
         "--max-patch-lines",
         type=int,
-        default=1500,
-        help="Abort if Codex suggests touching more than this many lines (default: %(default)s)",
+        help="Abort if Codex suggests touching more than this many lines (initial: 1500)",
     )
     parser.add_argument(
         "--patch-approval-mode",
         choices=("prompt", "auto"),
-        default="prompt",
-        help=(
-            "Control whether patch application requires approval "
-            "(default: %(default)s)"
-        ),
+        help="Control whether patch application requires approval (initial: prompt)",
     )
     parser.add_argument(
         "--auto-stage",
@@ -268,7 +268,6 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     )
     parser.add_argument(
         "--commit-extra-context",
-        default="",
         help="Additional instructions for the commit message prompt.",
     )
     parser.add_argument(
@@ -278,23 +277,32 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     )
     parser.add_argument(
         "--env-file",
-        default="~/.env",
-        help="Path to dotenv file for Codex CLI environment defaults (default: %(default)s)",
+        help="Path to dotenv file for Codex CLI environment initial values (initial: ~/.env)",
     )
     parser.add_argument(
         "--patch-retries",
         type=int,
-        default=1,
-        help="Number of additional patch attempts when apply fails (default: %(default)s)",
+        help="Number of additional patch attempts when apply fails (initial: 1)",
+    )
+    parser.set_defaults(
+        command="./scripts/ci.sh",
+        max_iterations=5,
+        log_tail=200,
+        max_patch_lines=1500,
+        patch_approval_mode="prompt",
+        commit_extra_context="",
+        env_file="~/.env",
+        patch_retries=1,
     )
     parsed_args = list(argv) if argv is not None else None
     return parser.parse_args(parsed_args)


 def main(argv: Optional[Iterable[str]] = None) -> int:
+    """Entry point for running the CI automation workflow."""
     args = parse_args(argv)
-    options = configure_runtime(args)
     try:
+        options = configure_runtime(args)
         dry_run_exit = perform_dry_run(args, options)
         if dry_run_exit is not None:
             return dry_run_exit
diff --git a/ci_tools/scripts/coverage_guard.py b/ci_tools/scripts/coverage_guard.py
index c4bcfc4..631e2ad 100644
--- a/ci_tools/scripts/coverage_guard.py
+++ b/ci_tools/scripts/coverage_guard.py
@@ -49,19 +49,21 @@ def parse_args(argv: Sequence[str]) -> argparse.Namespace:
     parser.add_argument(
         "--threshold",
         type=float,
-        default=float(os.environ.get("ZEUS_COVERAGE_THRESHOLD", "80")),
-        help="Required per-file coverage percentage (default: 80).",
+        help="Required per-file coverage percentage (initial: 80).",
     )
     parser.add_argument(
         "--data-file",
-        default=None,
-        help="Coverage data file (defaults to COVERAGE_FILE or .coverage).",
+        help="Coverage data file (initial: COVERAGE_FILE or .coverage).",
     )
     parser.add_argument(
         "--include",
         action="append",
-        default=[],
-        help="Relative path prefixes to check (repeatable). Defaults to repo root.",
+        help="Relative path prefixes to check (repeatable). Initial: repo root.",
+    )
+    parser.set_defaults(
+        threshold=float(os.environ.get("ZEUS_COVERAGE_THRESHOLD") or "80"),
+        data_file=None,
+        include=[],
     )
     return parser.parse_args(argv)

@@ -89,7 +91,10 @@ def should_include(path: Path, prefixes: Sequence[Path]) -> bool:
         return False
     if not prefixes:
         return True
-    return any(path == prefix or str(path).startswith(str(prefix) + os.sep) for prefix in prefixes)
+    return any(
+        path == prefix or str(path).startswith(str(prefix) + os.sep)
+        for prefix in prefixes
+    )


 def collect_results(cov: Coverage, prefixes: Sequence[Path]) -> List[CoverageResult]:
@@ -123,7 +128,10 @@ def main(argv: Sequence[str] | None = None) -> int:
     args = parse_args(argv or sys.argv[1:])
     data_file = resolve_data_file(args.data_file)
     if not data_file.exists():
-        print(f"coverage_guard: coverage data file not found: {data_file}", file=sys.stderr)
+        print(
+            f"coverage_guard: coverage data file not found: {data_file}",
+            file=sys.stderr,
+        )
         return 1
     cov = Coverage(data_file=str(data_file))
     prefixes = normalize_prefixes(args.include)
@@ -140,8 +148,7 @@ def main(argv: Sequence[str] | None = None) -> int:
     ]
     if failures:
         print(
-            "coverage_guard: per-file coverage below threshold "
-            f"({threshold:.2f}%):",
+            "coverage_guard: per-file coverage below threshold " f"({threshold:.2f}%):",
             file=sys.stderr,
         )
         for result in failures:
diff --git a/ci_tools/scripts/dependency_guard.py b/ci_tools/scripts/dependency_guard.py
index ee64325..dd6efe5 100644
--- a/ci_tools/scripts/dependency_guard.py
+++ b/ci_tools/scripts/dependency_guard.py
@@ -21,26 +21,26 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python modules (defaults to ./src).",
+        help="Directory to scan for Python modules (initial: ./src).",
     )
     parser.add_argument(
         "--max-instantiations",
         type=int,
-        default=8,
-        help="Maximum allowed object instantiations in __init__/__post_init__ (default: 8).",
+        help="Maximum allowed object instantiations in __init__/__post_init__ (initial: 8).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan (may be passed multiple times).",
     )
+    parser.set_defaults(root=Path("src"), max_instantiations=8, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


 def iter_python_files(root: Path) -> Iterable[Path]:
+    if not root.exists():
+        raise OSError(f"Path does not exist: {root}")
     if root.is_file():
         if root.suffix == ".py":
             yield root
@@ -99,7 +99,9 @@ def count_instantiations(func_node: ast.FunctionDef) -> Tuple[int, List[str]]:
     return count, instantiated_classes


-def scan_file(path: Path, max_instantiations: int) -> List[Tuple[Path, str, int, int, List[str]]]:
+def scan_file(
+    path: Path, max_instantiations: int
+) -> List[Tuple[Path, str, int, int, List[str]]]:
     """Return list of (path, class_name, line, count, classes) for violations."""
     source = path.read_text()
     try:
@@ -122,9 +124,7 @@ def scan_file(path: Path, max_instantiations: int) -> List[Tuple[Path, str, int,

             count, instantiated = count_instantiations(item)
             if count > max_instantiations:
-                violations.append(
-                    (path, node.name, node.lineno, count, instantiated)
-                )
+                violations.append((path, node.name, node.lineno, count, instantiated))

     return violations

diff --git a/ci_tools/scripts/directory_depth_guard.py b/ci_tools/scripts/directory_depth_guard.py
index b63d122..233633d 100644
--- a/ci_tools/scripts/directory_depth_guard.py
+++ b/ci_tools/scripts/directory_depth_guard.py
@@ -15,22 +15,20 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan (defaults to ./src).",
+        help="Directory to scan (initial: ./src).",
     )
     parser.add_argument(
         "--max-depth",
         type=int,
-        default=5,
-        help="Maximum allowed directory nesting depth (defaults to 5).",
+        help="Maximum allowed directory nesting depth (initial: 5).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=str,
-        default=[],
         help="Directory name patterns to exclude (e.g., '__pycache__').",
     )
+    parser.set_defaults(root=Path("src"), max_depth=5, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


@@ -83,7 +81,9 @@ def main(argv: Optional[Iterable[str]] = None) -> int:
     root = args.root.resolve()

     if not root.exists():
-        print(f"directory_depth_guard: root path does not exist: {root}", file=sys.stderr)
+        print(
+            f"directory_depth_guard: root path does not exist: {root}", file=sys.stderr
+        )
         return 1

     exclusions = args.exclude + ["__pycache__", ".pytest_cache", ".mypy_cache"]
diff --git a/ci_tools/scripts/documentation_guard.py b/ci_tools/scripts/documentation_guard.py
index c02f260..fb8055a 100755
--- a/ci_tools/scripts/documentation_guard.py
+++ b/ci_tools/scripts/documentation_guard.py
@@ -25,9 +25,9 @@ def parse_args() -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("."),
-        help="Repository root directory (defaults to current directory).",
+        help="Repository root directory (initial: current directory).",
     )
+    parser.set_defaults(root=Path("."))
     return parser.parse_args()


diff --git a/ci_tools/scripts/function_size_guard.py b/ci_tools/scripts/function_size_guard.py
index 9964ef1..988537d 100644
--- a/ci_tools/scripts/function_size_guard.py
+++ b/ci_tools/scripts/function_size_guard.py
@@ -16,26 +16,26 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python files (defaults to ./src).",
+        help="Directory to scan for Python files (initial: ./src).",
     )
     parser.add_argument(
         "--max-function-lines",
         type=int,
-        default=80,
-        help="Maximum allowed lines per function (defaults to 80).",
+        help="Maximum allowed lines per function (initial: 80).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan.",
     )
+    parser.set_defaults(root=Path("src"), max_function_lines=80, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


 def iter_python_files(root: Path) -> Iterable[Path]:
+    if not root.exists():
+        raise OSError(f"Path does not exist: {root}")
     if root.is_file():
         if root.suffix == ".py":
             yield root
diff --git a/ci_tools/scripts/generate_commit_message.py b/ci_tools/scripts/generate_commit_message.py
index ebb443d..151a7a4 100755
--- a/ci_tools/scripts/generate_commit_message.py
+++ b/ci_tools/scripts/generate_commit_message.py
@@ -13,10 +13,9 @@ from ci_tools.ci import gather_git_diff, request_commit_message

 def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
     parser = argparse.ArgumentParser(description="Generate a commit message via Codex")
-    parser.add_argument("--model", default=None, help="Model name to pass to Codex")
+    parser.add_argument("--model", help="Model name to pass to Codex")
     parser.add_argument(
         "--reasoning",
-        default=None,
         help="Reasoning effort to request (low/medium/high)",
     )
     parser.add_argument(
@@ -27,9 +26,9 @@ def parse_args(argv: list[str] | None = None) -> argparse.Namespace:
     parser.add_argument(
         "--output",
         type=Path,
-        default=None,
         help="Optional file to write the commit summary/body (suppresses stdout).",
     )
+    parser.set_defaults(model=None, reasoning=None, output=None)
     return parser.parse_args(argv)


diff --git a/ci_tools/scripts/inheritance_guard.py b/ci_tools/scripts/inheritance_guard.py
index 97cc5ec..f010b10 100644
--- a/ci_tools/scripts/inheritance_guard.py
+++ b/ci_tools/scripts/inheritance_guard.py
@@ -21,26 +21,26 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python modules (defaults to ./src).",
+        help="Directory to scan for Python modules (initial: ./src).",
     )
     parser.add_argument(
         "--max-depth",
         type=int,
-        default=2,
-        help="Maximum allowed inheritance depth (default: 2, meaning class → parent → grandparent).",
+        help="Maximum allowed inheritance depth (initial: 2, meaning class → parent → grandparent).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan (may be passed multiple times).",
     )
+    parser.set_defaults(root=Path("src"), max_depth=2, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


 def iter_python_files(root: Path) -> Iterable[Path]:
+    if not root.exists():
+        raise OSError(f"Path does not exist: {root}")
     if root.is_file():
         if root.suffix == ".py":
             yield root
@@ -114,14 +114,17 @@ def calculate_depth(

     # Recursively calculate depth for each base
     max_base_depth = 0
+    has_real_bases = False
     for base in bases:
         # Skip common base classes that don't count as "real" inheritance
         if base in ("object", "Protocol", "ABC"):
             continue
+        has_real_bases = True
         base_depth = calculate_depth(base, hierarchy, visited.copy())
         max_base_depth = max(max_base_depth, base_depth)

-    return max_base_depth + 1
+    # Only count depth if there were non-skipped bases
+    return max_base_depth + 1 if has_real_bases else 0


 def scan_file(
diff --git a/ci_tools/scripts/method_count_guard.py b/ci_tools/scripts/method_count_guard.py
index 6bafc38..845e090 100644
--- a/ci_tools/scripts/method_count_guard.py
+++ b/ci_tools/scripts/method_count_guard.py
@@ -21,32 +21,33 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python modules (defaults to ./src).",
+        help="Directory to scan for Python modules (initial: ./src).",
     )
     parser.add_argument(
         "--max-public-methods",
         type=int,
-        default=15,
-        help="Maximum allowed public methods per class (default: 15).",
+        help="Maximum allowed public methods per class (initial: 15).",
     )
     parser.add_argument(
         "--max-total-methods",
         type=int,
-        default=25,
-        help="Maximum allowed total methods (public + private) per class (default: 25).",
+        help="Maximum allowed total methods (public + private) per class (initial: 25).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan (may be passed multiple times).",
     )
+    parser.set_defaults(
+        root=Path("src"), max_public_methods=15, max_total_methods=25, exclude=[]
+    )
     return parser.parse_args(list(argv) if argv is not None else None)


 def iter_python_files(root: Path) -> Iterable[Path]:
+    if not root.exists():
+        raise OSError(f"path does not exist: {root}")
     if root.is_file():
         if root.suffix == ".py":
             yield root
@@ -82,8 +83,8 @@ def count_methods(node: ast.ClassDef) -> Tuple[int, int]:
         if not isinstance(item, ast.FunctionDef):
             continue

-        # Skip dunder methods
-        if item.name.startswith("__") and item.name.endswith("__"):
+        # Skip dunder methods and name-mangled methods
+        if item.name.startswith("__"):
             continue

         # Skip properties (they're data access, not behavior)
diff --git a/ci_tools/scripts/module_guard.py b/ci_tools/scripts/module_guard.py
index b2dbf24..24b3771 100644
--- a/ci_tools/scripts/module_guard.py
+++ b/ci_tools/scripts/module_guard.py
@@ -15,22 +15,20 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python modules (defaults to ./src).",
+        help="Directory to scan for Python modules (initial: ./src).",
     )
     parser.add_argument(
         "--max-module-lines",
         type=int,
-        default=600,
         help="Maximum allowed number of lines per module (file).",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan (may be passed multiple times).",
     )
+    parser.set_defaults(root=Path("src"), max_module_lines=600, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


diff --git a/ci_tools/scripts/policy_collectors_ast.py b/ci_tools/scripts/policy_collectors_ast.py
index 7f58941..cd991c9 100644
--- a/ci_tools/scripts/policy_collectors_ast.py
+++ b/ci_tools/scripts/policy_collectors_ast.py
@@ -167,26 +167,43 @@ def collect_literal_fallbacks() -> List[Tuple[str, int, str]]:
             self.rel_path = rel_path

         def visit_Call(self, node: ast.Call) -> None:
+            self._check_get_method(node)
+            self._check_getattr(node)
+            self._check_os_getenv(node)
+            self._check_setdefault(node)
+            self.generic_visit(node)
+
+        def _check_get_method(self, node: ast.Call) -> None:
             qualname = get_call_qualname(node.func) or ""
-            if qualname.endswith(".get"):
-                default_arg = _resolve_default_argument(
-                    node,
-                    positional_index=1,
-                    keyword_names={"default", "fallback"},
-                )
-                self._maybe_record(node, default_arg, f"{qualname} literal fallback")
-            elif qualname == "getattr" and len(node.args) >= 3:
+            if not qualname.endswith(".get"):
+                return
+            default_arg = _resolve_default_argument(
+                node,
+                positional_index=1,
+                keyword_names={"default", "fallback"},
+            )
+            self._maybe_record(node, default_arg, f"{qualname} literal fallback")
+
+        def _check_getattr(self, node: ast.Call) -> None:
+            qualname = get_call_qualname(node.func) or ""
+            if qualname == "getattr" and len(node.args) >= 3:
                 self._maybe_record(node, node.args[2], "getattr literal fallback")
-            elif qualname in {"os.getenv", "os.environ.get"}:
-                default_arg = _resolve_default_argument(
-                    node,
-                    positional_index=1,
-                    keyword_names={"default"},
-                )
-                self._maybe_record(node, default_arg, f"{qualname} literal fallback")
-            elif qualname.endswith(".setdefault") and len(node.args) >= 2:
+
+        def _check_os_getenv(self, node: ast.Call) -> None:
+            qualname = get_call_qualname(node.func) or ""
+            if qualname not in {"os.getenv", "os.environ.get"}:
+                return
+            default_arg = _resolve_default_argument(
+                node,
+                positional_index=1,
+                keyword_names={"default"},
+            )
+            self._maybe_record(node, default_arg, f"{qualname} literal fallback")
+
+        def _check_setdefault(self, node: ast.Call) -> None:
+            qualname = get_call_qualname(node.func) or ""
+            if qualname.endswith(".setdefault") and len(node.args) >= 2:
                 self._maybe_record(node, node.args[1], f"{qualname} literal fallback")
-            self.generic_visit(node)

         def _maybe_record(
             self,
@@ -198,6 +215,8 @@ def collect_literal_fallbacks() -> List[Tuple[str, int, str]]:
                 records.append((self.rel_path, node.lineno, message))

     for ctx in iter_module_contexts():
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         LiteralFallbackVisitor(ctx.rel_path).visit(ctx.tree)
     return records

@@ -221,6 +240,8 @@ def collect_bool_fallbacks() -> List[Tuple[str, int]]:
             self.generic_visit(node)

     for ctx in iter_module_contexts():
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         BoolFallbackVisitor(ctx.rel_path).visit(ctx.tree)
     return records

@@ -240,6 +261,8 @@ def collect_conditional_literal_returns() -> List[Tuple[str, int]]:
             self.generic_visit(node)

     for ctx in iter_module_contexts():
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         ConditionalLiteralVisitor(ctx.rel_path).visit(ctx.tree)
     return records

@@ -282,6 +305,8 @@ def collect_backward_compat_blocks() -> List[Tuple[str, int, str]]:
                 )

     for ctx in iter_module_contexts(include_source=True):
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         LegacyVisitor(ctx).visit(ctx.tree)
     return records

@@ -341,6 +366,8 @@ def _function_entries_from_context(
 def _build_duplicate_mapping(min_length: int) -> Dict[str, List[FunctionEntry]]:
     mapping: Dict[str, List[FunctionEntry]] = defaultdict(list)
     for ctx in iter_module_contexts():
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         if ctx.path.name == "__init__.py":
             continue
         for key, entry in _function_entries_from_context(ctx, min_length=min_length):
@@ -385,19 +412,16 @@ def purge_bytecode_artifacts() -> None:


 __all__ = [
-    name
-    for name in [
-        "collect_long_functions",
-        "collect_broad_excepts",
-        "collect_silent_handlers",
-        "collect_generic_raises",
-        "collect_literal_fallbacks",
-        "collect_bool_fallbacks",
-        "collect_conditional_literal_returns",
-        "collect_backward_compat_blocks",
-        "collect_forbidden_sync_calls",
-        "collect_duplicate_functions",
-        "collect_bytecode_artifacts",
-        "purge_bytecode_artifacts",
-    ]
+    "collect_long_functions",
+    "collect_broad_excepts",
+    "collect_silent_handlers",
+    "collect_generic_raises",
+    "collect_literal_fallbacks",
+    "collect_bool_fallbacks",
+    "collect_conditional_literal_returns",
+    "collect_backward_compat_blocks",
+    "collect_forbidden_sync_calls",
+    "collect_duplicate_functions",
+    "collect_bytecode_artifacts",
+    "purge_bytecode_artifacts",
 ]
diff --git a/ci_tools/scripts/policy_collectors_text.py b/ci_tools/scripts/policy_collectors_text.py
index d195552..43bbefc 100644
--- a/ci_tools/scripts/policy_collectors_text.py
+++ b/ci_tools/scripts/policy_collectors_text.py
@@ -42,6 +42,8 @@ def scan_keywords() -> Dict[str, Dict[str, List[int]]]:
     keyword_lookup = {kw.lower(): kw for kw in BANNED_KEYWORDS}

     for ctx in iter_module_contexts(include_source=True):
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         source = ctx.source or ""
         keyword_hits = _keyword_token_lines(source, keyword_lookup)
         for keyword, lines in keyword_hits.items():
@@ -55,6 +57,8 @@ def collect_flagged_tokens() -> List[Tuple[str, int, str]]:
     for ctx in iter_module_contexts(include_source=True):
         if ctx.source is None:
             continue
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         for lineno, line in enumerate(ctx.source.splitlines(), start=1):
             for token in FLAGGED_TOKENS:
                 if token in line:
@@ -67,6 +71,8 @@ def collect_suppressions() -> List[Tuple[str, int, str]]:
     for ctx in iter_module_contexts(include_source=True):
         if ctx.source is None:
             continue
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         for lineno, line in enumerate(ctx.source.splitlines(), start=1):
             for token in SUPPRESSION_PATTERNS:
                 if token in line:
@@ -74,18 +80,44 @@ def collect_suppressions() -> List[Tuple[str, int, str]]:
     return records


-def collect_legacy_modules() -> List[Tuple[str, int, str]]:
-    records: List[Tuple[str, int, str]] = []
+def _build_legacy_patterns() -> (
+    Tuple[Tuple[str, ...], Tuple[str, ...], Tuple[str, ...]]
+):
+    """Build forbidden patterns for legacy module detection."""
     forbidden_suffixes = tuple(f"{suffix}.py" for suffix in LEGACY_SUFFIXES)
     dir_tokens = tuple(token.strip("_") for token in LEGACY_SUFFIXES)
     forbidden_parts = tuple(f"/{token}/" for token in dir_tokens) + tuple(
         f"\\{token}\\" for token in dir_tokens
     )
+    forbidden_prefixes = tuple(f"{token}/" for token in dir_tokens) + tuple(
+        f"{token}\\" for token in dir_tokens
+    )
+    return forbidden_suffixes, forbidden_parts, forbidden_prefixes
+
+
+def _has_legacy_pattern(
+    lowered_path: str,
+    patterns: Tuple[Tuple[str, ...], Tuple[str, ...], Tuple[str, ...]],
+) -> bool:
+    """Check if a lowercased path contains any legacy patterns."""
+    forbidden_suffixes, forbidden_parts, forbidden_prefixes = patterns
+    if any(suffix in lowered_path for suffix in forbidden_suffixes):
+        return True
+    if any(part in lowered_path for part in forbidden_parts):
+        return True
+    if any(lowered_path.startswith(prefix) for prefix in forbidden_prefixes):
+        return True
+    return False
+
+
+def collect_legacy_modules() -> List[Tuple[str, int, str]]:
+    records: List[Tuple[str, int, str]] = []
+    patterns = _build_legacy_patterns()
     for ctx in iter_module_contexts():
+        if ctx.rel_path.startswith(("scripts/", "ci_runtime/", "vendor/")):
+            continue
         lowered = ctx.rel_path.lower()
-        if any(suffix in lowered for suffix in forbidden_suffixes) or any(
-            part in lowered for part in forbidden_parts
-        ):
+        if _has_legacy_pattern(lowered, patterns):
             records.append((ctx.rel_path, 1, "legacy module path"))
     return records

diff --git a/ci_tools/scripts/policy_context.py b/ci_tools/scripts/policy_context.py
index 66f2798..4fca879 100644
--- a/ci_tools/scripts/policy_context.py
+++ b/ci_tools/scripts/policy_context.py
@@ -105,7 +105,10 @@ def normalize_function(node: ast.FunctionDef | ast.AsyncFunctionDef) -> str:


 def normalize_path(path: Path) -> str:
-    return str(path.relative_to(ROOT)).replace("\\", "/")
+    try:
+        return str(path.relative_to(ROOT)).replace("\\", "/")
+    except ValueError:
+        return str(path).replace("\\", "/")


 def iter_python_files(bases: Sequence[Path]) -> Iterator[Path]:
@@ -123,11 +126,18 @@ def parse_ast(path: Path) -> ast.AST | None:


 def iter_module_contexts(
-    bases: Sequence[Path] = SCAN_DIRECTORIES,
+    bases: Sequence[Path] | None = None,
     *,
     include_source: bool = False,
     include_lines: bool = False,
 ) -> Iterator[ModuleContext]:
+    if bases is None:
+        src_dir = ROOT / "src"
+        tests_dir = ROOT / "tests"
+        if src_dir.exists() or tests_dir.exists():
+            bases = (src_dir, tests_dir)
+        else:
+            bases = (ROOT,)
     for path in iter_python_files(bases):
         try:
             text = path.read_text()
@@ -245,4 +255,35 @@ def handler_contains_suppression(
     return False


-__all__ = [name for name in globals() if not name.startswith("_")]
+__all__ = [
+    "ROOT",
+    "SCAN_DIRECTORIES",
+    "BANNED_KEYWORDS",
+    "FLAGGED_TOKENS",
+    "FUNCTION_LENGTH_THRESHOLD",
+    "BROAD_EXCEPT_SUPPRESSION",
+    "SILENT_HANDLER_SUPPRESSION",
+    "SUPPRESSION_PATTERNS",
+    "FORBIDDEN_SYNC_CALLS",
+    "LEGACY_GUARD_TOKENS",
+    "LEGACY_SUFFIXES",
+    "LEGACY_CONFIG_TOKENS",
+    "CONFIG_EXTENSIONS",
+    "BROAD_EXCEPTION_NAMES",
+    "FunctionEntry",
+    "ModuleContext",
+    "FunctionNormalizer",
+    "normalize_function",
+    "normalize_path",
+    "iter_python_files",
+    "parse_ast",
+    "iter_module_contexts",
+    "_resolve_default_argument",
+    "get_call_qualname",
+    "is_non_none_literal",
+    "is_logging_call",
+    "handler_has_raise",
+    "classify_handler",
+    "is_literal_none_guard",
+    "handler_contains_suppression",
+]
diff --git a/ci_tools/scripts/policy_guard.py b/ci_tools/scripts/policy_guard.py
index 1bc60b2..5c487a6 100644
--- a/ci_tools/scripts/policy_guard.py
+++ b/ci_tools/scripts/policy_guard.py
@@ -5,19 +5,20 @@ from __future__ import annotations

 import sys

-from . import policy_checks as _policy_checks
-from .policy_checks import *  # noqa: F401,F403
+from .policy_checks import PolicyViolation
+from .policy_checks import main as _run_policy_checks
+from .policy_checks import purge_bytecode_artifacts

-__all__ = getattr(_policy_checks, "__all__", [])
+__all__ = ["PolicyViolation", "purge_bytecode_artifacts", "main"]


 def main() -> int:  # pragma: no cover - thin wrapper
-    return _policy_checks.main()
+    return _run_policy_checks()


-if __name__ == "__main__":
+if __name__ == "__main__":  # pragma: no cover
     try:
         sys.exit(main())
-    except _policy_checks.PolicyViolation as err:
+    except PolicyViolation as err:
         print(err, file=sys.stderr)
         sys.exit(1)
diff --git a/ci_tools/scripts/structure_guard.py b/ci_tools/scripts/structure_guard.py
index 5c7a295..f072924 100644
--- a/ci_tools/scripts/structure_guard.py
+++ b/ci_tools/scripts/structure_guard.py
@@ -16,22 +16,20 @@ def parse_args(argv: Optional[Iterable[str]] = None) -> argparse.Namespace:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Directory to scan for Python modules (defaults to ./src).",
+        help="Directory to scan for Python modules (initial: ./src).",
     )
     parser.add_argument(
         "--max-class-lines",
         type=int,
-        default=100,
         help="Maximum allowed number of lines per class definition.",
     )
     parser.add_argument(
         "--exclude",
         action="append",
         type=Path,
-        default=[],
         help="Path prefix to exclude from the scan (may be passed multiple times).",
     )
+    parser.set_defaults(root=Path("src"), max_class_lines=100, exclude=[])
     return parser.parse_args(list(argv) if argv is not None else None)


diff --git a/ci_tools/scripts/unused_module_guard.py b/ci_tools/scripts/unused_module_guard.py
index d1f8038..a057252 100755
--- a/ci_tools/scripts/unused_module_guard.py
+++ b/ci_tools/scripts/unused_module_guard.py
@@ -19,14 +19,16 @@ import argparse
 import ast
 import sys
 from pathlib import Path
-from typing import Dict, List, Set, Tuple
+from typing import Iterable, List, Optional, Set, Tuple


 class ImportCollector(ast.NodeVisitor):
     """Collects all import statements from a Python file."""

-    def __init__(self):
+    def __init__(self, file_path: Optional[Path] = None, root: Optional[Path] = None):
         self.imports: Set[str] = set()
+        self.file_path = file_path
+        self.root = root

     def visit_Import(self, node: ast.Import) -> None:
         """Visit 'import foo' statements."""
@@ -41,17 +43,54 @@ class ImportCollector(ast.NodeVisitor):
                 self.imports.add(".".join(parts[: i + 1]))
         self.generic_visit(node)

+    def _resolve_relative_import(self, node: ast.ImportFrom) -> None:
+        """Handle relative imports like 'from . import X'."""
+        if not (self.file_path and self.root):
+            return
+
+        file_module = get_module_name(self.file_path, self.root)
+        if not file_module:
+            return
+
+        parts = file_module.split(".")
+        if node.level > len(parts):
+            return
+
+        # Calculate base module path based on relative level
+        base_parts = parts[: -node.level + 1] if node.level > 1 else parts[:-1]
+        base_module = ".".join(base_parts) if base_parts else ""
+
+        # Register imported names
+        for alias in node.names:
+            if alias.name != "*":
+                if base_module:
+                    self.imports.add(f"{base_module}.{alias.name}")
+                else:
+                    self.imports.add(alias.name)
+
+    def _resolve_absolute_import(self, module: str, names: list) -> None:
+        """Handle absolute imports like 'from foo import bar'."""
+        # Strip src. prefix if present
+        if module.startswith("src."):
+            module = module[4:]
+
+        # Add module and all parent modules
+        parts = module.split(".")
+        for i in range(len(parts)):
+            self.imports.add(".".join(parts[: i + 1]))
+
+        # Add imported names as submodules
+        for alias in names:
+            if alias.name != "*":
+                self.imports.add(f"{module}.{alias.name}")
+
     def visit_ImportFrom(self, node: ast.ImportFrom) -> None:
         """Visit 'from foo import bar' statements."""
-        if node.module:
-            # Add full module path and all parent modules
-            module = node.module
-            # Strip src. prefix if present
-            if module.startswith("src."):
-                module = module[4:]
-            parts = module.split(".")
-            for i in range(len(parts)):
-                self.imports.add(".".join(parts[: i + 1]))
+        if node.module is None and node.level > 0:
+            self._resolve_relative_import(node)
+        elif node.module:
+            self._resolve_absolute_import(node.module, node.names)
+
         self.generic_visit(node)


@@ -74,7 +113,7 @@ def collect_all_imports(root: Path) -> Set[str]:
         try:
             with open(py_file, "r", encoding="utf-8") as f:
                 tree = ast.parse(f.read(), filename=str(py_file))
-            collector = ImportCollector()
+            collector = ImportCollector(file_path=py_file, root=root)
             collector.visit(tree)
             all_imports.update(collector.imports)
         except (SyntaxError, UnicodeDecodeError):
@@ -109,6 +148,55 @@ def get_module_name(file_path: Path, root: Path) -> str:
     return ".".join(parts) if parts else ""


+SUSPICIOUS_PATTERNS: Tuple[str, ...] = (
+    "_refactored",
+    "_slim",
+    "_optimized",
+    "_old",
+    "_backup",
+    "_copy",
+    "_new",
+    "_temp",
+    "_v2",
+    "_2",
+)
+
+FALSE_POSITIVE_RULES: Tuple[Tuple[str, Iterable[str]], ...] = (
+    ("_temp", ("temperature", "max_temp")),
+    ("_2", ("phase_2", "_v2")),
+)
+
+
+def _is_false_positive(stem: str) -> bool:
+    """Check if a stem matches any false positive exclusion markers."""
+    for fp_pattern, exclusions in FALSE_POSITIVE_RULES:
+        for marker in exclusions:
+            if marker in stem:
+                return True
+    return False
+
+
+def _is_false_positive_for_pattern(stem: str, pattern: str) -> bool:
+    """Check if a stem matches false positive rules for a specific pattern."""
+    for fp_pattern, exclusions in FALSE_POSITIVE_RULES:
+        if fp_pattern == pattern:
+            # Check if any exclusion marker is in the stem
+            for marker in exclusions:
+                if marker in stem:
+                    return True
+    return False
+
+
+def _duplicate_reason(stem: str) -> Optional[str]:
+    """Check if a stem contains suspicious patterns, accounting for false positives."""
+    for pattern in SUSPICIOUS_PATTERNS:
+        if pattern in stem:
+            # Check if this is a false positive for this specific pattern
+            if not _is_false_positive_for_pattern(stem, pattern):
+                return f"Suspicious duplicate pattern '{pattern}' in filename"
+    return None
+
+
 def find_suspicious_duplicates(root: Path) -> List[Tuple[Path, str]]:
     """
     Find files with suspicious naming patterns that suggest duplicates.
@@ -119,45 +207,128 @@ def find_suspicious_duplicates(root: Path) -> List[Tuple[Path, str]]:
     Returns:
         List of (file_path, reason) tuples
     """
-    suspicious_patterns = [
-        "_refactored",
-        "_slim",
-        "_optimized",
-        "_old",
-        "_backup",
-        "_copy",
-        "_new",
-        "_temp",
-        "_v2",
-        "_2",
-    ]
-
     duplicates: List[Tuple[Path, str]] = []

     for py_file in root.rglob("*.py"):
         if "__pycache__" in str(py_file):
             continue

-        stem = py_file.stem
+        reason = _duplicate_reason(py_file.stem)
+        if reason:
+            duplicates.append((py_file, reason))

-        # Skip false positives
-        if "_temp" in stem and ("temperature" in stem or "max_temp" in stem):
-            continue
-        if "_2" in stem and ("phase_2" in stem or "_v2" in stem):
-            continue
+    return duplicates

-        for pattern in suspicious_patterns:
-            if pattern in stem:
-                duplicates.append(
-                    (py_file, f"Suspicious duplicate pattern '{pattern}' in filename")
-                )
-                break

-    return duplicates
+def _collect_all_imports_with_parent(root: Path) -> Set[str]:
+    imports = collect_all_imports(root)
+    parent = root.parent
+    if parent.exists():
+        imports.update(collect_all_imports(parent))
+    return imports
+
+
+def _is_cli_entry_point(py_file: Path) -> bool:
+    """
+    Check if a file is a CLI entry point (has main() and if __name__ == "__main__").
+
+    CLI entry points are meant to be executed directly (e.g., python -m module)
+    rather than imported, so they don't need to appear in import statements.
+    """
+    try:
+        with open(py_file, "r", encoding="utf-8") as f:
+            content = f.read()
+            tree = ast.parse(content, filename=str(py_file))
+
+        # Check for if __name__ == "__main__": pattern
+        has_main_guard = 'if __name__ == "__main__"' in content
+
+        # Check for main() function definition
+        has_main_function = any(
+            isinstance(node, ast.FunctionDef) and node.name == "main"
+            for node in ast.walk(tree)
+        )
+
+        return has_main_guard and has_main_function
+    except (SyntaxError, UnicodeDecodeError, FileNotFoundError, OSError):
+        return False
+
+
+def _should_skip_file(py_file: Path, exclude_patterns: List[str]) -> bool:
+    if "__pycache__" in str(py_file):
+        return True
+    if any(pattern in str(py_file) for pattern in exclude_patterns):
+        return True
+    # Skip __main__.py and main.py as they are entry points
+    if py_file.name in ("__main__.py", "main.py"):
+        return True
+    # Skip CLI entry points (files with main() and if __name__ == "__main__")
+    return _is_cli_entry_point(py_file)
+
+
+def _check_exact_match(module_name: str, file_stem: str, all_imports: Set[str]) -> bool:
+    """Check for exact module name matches."""
+    if module_name in all_imports:
+        return True
+    if f"src.{module_name}" in all_imports:
+        return True
+    if file_stem in all_imports:
+        return True
+    return False
+
+
+def _check_child_imported(module_name: str, all_imports: Set[str]) -> bool:
+    """Check if any child module is imported."""
+    for imported in all_imports:
+        if imported.startswith(module_name + "."):
+            return True
+        if imported.startswith(f"src.{module_name}."):
+            return True
+    return False
+
+
+def _has_specific_child_imports(
+    parent: str, module_name: str, all_imports: Set[str]
+) -> bool:
+    """Check if parent has specific child imports that exclude this module."""
+    return any(
+        imp.startswith(parent + ".") and imp != module_name for imp in all_imports
+    )
+
+
+def _check_parent_imported(module_name: str, all_imports: Set[str]) -> bool:
+    """Check if a parent module is imported wholesale."""
+    module_parts = module_name.split(".")
+    for i in range(len(module_parts) - 1):
+        parent = ".".join(module_parts[: i + 1])
+        if parent in all_imports or f"src.{parent}" in all_imports:
+            if not _has_specific_child_imports(parent, module_name, all_imports):
+                return True
+    return False
+
+
+def _module_is_imported(
+    module_name: str,
+    file_stem: str,
+    all_imports: Set[str],
+) -> bool:
+    if not module_name:
+        return True
+
+    if _check_exact_match(module_name, file_stem, all_imports):
+        return True
+
+    if _check_child_imported(module_name, all_imports):
+        return True
+
+    if _check_parent_imported(module_name, all_imports):
+        return True
+
+    return False


 def find_unused_modules(
-    root: Path, exclude_patterns: List[str] = None
+    root: Path, exclude_patterns: Optional[List[str]] = None
 ) -> List[Tuple[Path, str]]:
     """
     Find Python modules that are never imported.
@@ -169,52 +340,19 @@ def find_unused_modules(
     Returns:
         List of (file_path, reason) tuples for unused modules
     """
-    if exclude_patterns is None:
-        exclude_patterns = []
-
-    # Collect all imports from all files
-    all_imports = collect_all_imports(root)
-
-    # Also check parent directory for imports of this package
-    if root.parent.exists():
-        parent_imports = collect_all_imports(root.parent)
-        all_imports.update(parent_imports)
-
+    exclude_patterns = list(exclude_patterns or [])
+    all_imports = _collect_all_imports_with_parent(root)
     unused: List[Tuple[Path, str]] = []

     for py_file in root.rglob("*.py"):
-        if "__pycache__" in str(py_file):
-            continue
-
-        # Skip excluded patterns
-        if any(pattern in str(py_file) for pattern in exclude_patterns):
+        if _should_skip_file(py_file, exclude_patterns):
             continue

-        # Skip __main__.py (entry points)
-        if py_file.name == "__main__.py":
-            continue
-
-        # Get module name
         module_name = get_module_name(py_file, root)
-        if not module_name:
+        if _module_is_imported(module_name, py_file.stem, all_imports):
             continue

-        # Check if this module or any parent module is imported
-        module_parts = module_name.split(".")
-        is_imported = False
-
-        for i in range(len(module_parts)):
-            partial_module = ".".join(module_parts[: i + 1])
-            if partial_module in all_imports:
-                is_imported = True
-                break
-
-        # Also check just the filename without path
-        if py_file.stem in all_imports:
-            is_imported = True
-
-        if not is_imported:
-            unused.append((py_file, f"Never imported (module: {module_name})"))
+        unused.append((py_file, f"Never imported (module: {module_name})"))

     return unused

@@ -228,8 +366,7 @@ def main() -> int:
     parser.add_argument(
         "--root",
         type=Path,
-        default=Path("src"),
-        help="Root directory to check (default: src)",
+        help="Root directory to check (initial: src)",
     )
     parser.add_argument(
         "--strict",
@@ -239,9 +376,9 @@ def main() -> int:
     parser.add_argument(
         "--exclude",
         nargs="+",
-        default=["__init__.py", "conftest.py"],
         help="Patterns to exclude from unused checks",
     )
+    parser.set_defaults(root=Path("src"), exclude=["__init__.py", "conftest.py"])

     args = parser.parse_args()

diff --git a/ci_tools/vendor/__init__.py b/ci_tools/vendor/__init__.py
index 30e7d41..bbbf42d 100644
--- a/ci_tools/vendor/__init__.py
+++ b/ci_tools/vendor/__init__.py
@@ -1,2 +1 @@
 """Compatibility shims used by shared CI tooling."""
-
diff --git a/ci_tools/vendor/packaging/__init__.py b/ci_tools/vendor/packaging/__init__.py
index 8f7d33e..a75a356 100644
--- a/ci_tools/vendor/packaging/__init__.py
+++ b/ci_tools/vendor/packaging/__init__.py
@@ -17,4 +17,3 @@ __all__ = [
     "InvalidVersion",
     "Version",
 ]
-
diff --git a/ci_tools/vendor/packaging/specifiers.py b/ci_tools/vendor/packaging/specifiers.py
index 5e7428f..7843e43 100644
--- a/ci_tools/vendor/packaging/specifiers.py
+++ b/ci_tools/vendor/packaging/specifiers.py
@@ -4,8 +4,10 @@ from __future__ import annotations

 import re
 from dataclasses import dataclass
-from typing import Iterable, Iterator, List, Optional, Sequence, Tuple, TypeVar
+from operator import eq, ge, gt, le, lt, ne
+from typing import Iterable, Iterator, List, TypeVar

+from ..._messages import format_default_message
 from .version import InvalidVersion, Version

 _SPEC_PATTERN = re.compile(r"\s*(===|==|!=|~=|>=|<=|>|<)\s*(.+)\s*$")
@@ -18,19 +20,23 @@ class InvalidSpecifier(ValueError):
     default_message = "Invalid version specifier"

     def __init__(self, *, detail: str | None = None) -> None:
-        message = self.default_message if detail is None else f"{self.default_message}: {detail}"
+        """Initialise the error with an optional detail string."""
+        message = format_default_message(self.default_message, detail)
         super().__init__(message)

     @classmethod
     def for_value(cls, spec: str) -> "InvalidSpecifier":
+        """Return an error describing the invalid spec string."""
         return cls(detail=f"unable to parse {spec!r}")

     @classmethod
     def unsupported_wildcard_operator(cls, operator: str) -> "InvalidSpecifier":
+        """Return an error describing an unsupported wildcard operator."""
         return cls(detail=f"unsupported wildcard operator {operator!r}")

     @classmethod
     def unsupported_operator(cls, operator: str) -> "InvalidSpecifier":
+        """Return an error describing an unsupported comparison operator."""
         return cls(detail=f"unsupported operator {operator!r}")


@@ -45,6 +51,7 @@ class Specifier:
     version: str

     def __init__(self, spec: str) -> None:
+        """Parse the specifier string into operator and version components."""
         match = _SPEC_PATTERN.fullmatch(spec)
         if not match:
             raise InvalidSpecifier.for_value(spec)
@@ -55,10 +62,12 @@ class Specifier:
         return f"{self.operator}{self.version}"

     def _matches_wildcard(self, candidate: str) -> bool:
+        """Return True when the wildcard operator matches the candidate."""
         prefix = _STAR_PATTERN.split(self.version, 1)[0]
         return candidate.startswith(prefix)

     def _handle_wildcard(self, candidate: str) -> bool:
+        """Evaluate wildcard comparisons for equality and inequality."""
         op = self.operator
         if op == "==":
             return self._matches_wildcard(candidate)
@@ -73,25 +82,26 @@ class Specifier:
         spec_version: Version,
         raw_candidate: str,
     ) -> bool:
-        if op == "==":
-            return candidate_version == spec_version
-        if op == "!=":
-            return candidate_version != spec_version
-        if op == ">":
-            return candidate_version > spec_version
-        if op == ">=":
-            return candidate_version >= spec_version
-        if op == "<":
-            return candidate_version < spec_version
-        if op == "<=":
-            return candidate_version <= spec_version
-        if op == "===":
-            return raw_candidate == self.version
+        standard_ops = {
+            "==": eq,
+            "!=": ne,
+            ">": gt,
+            ">=": ge,
+            "<": lt,
+            "<=": le,
+        }
+
         if op == "~=":
             lower = spec_version
             upper = _compatible_upper_bound(spec_version)
             return lower <= candidate_version < upper
-        raise InvalidSpecifier.unsupported_operator(op)
+        if op == "===":
+            return raw_candidate == self.version
+        try:
+            comparator = standard_ops[op]
+        except KeyError as exc:  # pragma: no cover - defensive
+            raise InvalidSpecifier.unsupported_operator(op) from exc
+        return comparator(candidate_version, spec_version)

     def contains(self, candidate: str) -> bool:
         """Return True when *candidate* satisfies this specifier."""
@@ -107,6 +117,7 @@ class Specifier:


 def _compatible_upper_bound(version: Version) -> Version:
+    """Return the upper bound for compatible release comparisons."""
     components = list(version.release)
     if len(components) >= 2:
         components[1] += 1
@@ -136,7 +147,9 @@ class SpecifierSet:
     def __str__(self) -> str:
         return ",".join(str(spec) for spec in self._specs)

-    def filter(self, iterable: Iterable[_T], prereleases: bool | None = None) -> Iterator[_T]:
+    def filter(
+        self, iterable: Iterable[_T], prereleases: bool | None = None
+    ) -> Iterator[_T]:
         """Yield items from *iterable* that satisfy every specifier."""

         del prereleases  # pragma: no cover - compatibility argument
@@ -150,6 +163,7 @@ class SpecifierSet:


 def _coerce_candidate(item: object) -> str:
+    """Return the string representation used for comparisons."""
     if isinstance(item, str):
         return item
     if hasattr(item, "value"):
diff --git a/ci_tools/vendor/packaging/version.py b/ci_tools/vendor/packaging/version.py
index 0255cde..e6f9456 100644
--- a/ci_tools/vendor/packaging/version.py
+++ b/ci_tools/vendor/packaging/version.py
@@ -6,6 +6,7 @@ import re
 from functools import total_ordering
 from typing import Tuple

+from ..._messages import format_default_message

 _VERSION_PATTERN = re.compile(r"^\s*(\d+)(?:\.(\d+))?(?:\.(\d+))?")

@@ -16,11 +17,13 @@ class InvalidVersion(ValueError):
     default_message = "Invalid version string"

     def __init__(self, *, detail: str | None = None) -> None:
-        message = self.default_message if detail is None else f"{self.default_message}: {detail}"
+        """Initialise the exception with optional extra detail."""
+        message = format_default_message(self.default_message, detail)
         super().__init__(message)

     @classmethod
     def for_value(cls, version: str) -> "InvalidVersion":
+        """Return an error describing the invalid version string."""
         return cls(detail=f"unable to parse {version!r}")


@@ -48,17 +51,21 @@ class Version:

     @property
     def release(self) -> Tuple[int, ...]:
+        """Return the release tuple (major, minor, patch)."""
         return self._release

     @property
     def major(self) -> int:
+        """Return the major version number."""
         return self._release[0]

     @property
     def minor(self) -> int:
+        """Return the minor version number."""
         return self._release[1] if len(self._release) > 1 else 0

     def _normalized_release(self) -> Tuple[int, int, int]:
+        """Return a three-component tuple padded with zeros."""
         first = self._release[0] if len(self._release) > 0 else 0
         second = self._release[1] if len(self._release) > 1 else 0
         third = self._release[2] if len(self._release) > 2 else 0
diff --git a/docs/README.md b/docs/README.md
new file mode 100644
index 0000000..af9a386
--- /dev/null
+++ b/docs/README.md
@@ -0,0 +1,10 @@
+# Codex CI Tools Documentation
+
+This directory hosts in-depth references for contributors working with the shared CI automation suite. Use these guides to navigate the guard pipeline and automation workflow.
+
+- [`getting-started.md`](getting-started.md) — Install the toolkit and integrate the shared Makefile.
+- [`automation.md`](automation.md) — Run the Codex repair loop and interpret its phases.
+- [`guard-suite.md`](guard-suite.md) — Explore every guard, their CLI flags, and enforcement policies.
+- [`development.md`](development.md) — Local development workflow, release steps, and vendoring tips.
+
+Add new documents alongside these references so the automation guard can surface them.
diff --git a/pyproject.toml b/pyproject.toml
index a24de01..95602bf 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -19,7 +19,23 @@ classifiers = [
 ]

 [project.optional-dependencies]
-dev = ["jinja2>=3.1.0"]
+dev = [
+    "jinja2>=3.1.0",
+    "coverage>=7.5.0",
+    "radon>=6.0.1",
+]
+
+[tool.deptry]
+root = ["ci_tools", "scripts"]
+exclude = ["tests"]
+known_first_party = ["ci_tools"]
+
+[tool.deptry.per_rule_ignores]
+DEP002 = ["jinja2"]
+DEP003 = [
+    "ci_tools/scripts/coverage_guard.py:coverage",
+    "scripts/complexity_guard.py:radon",
+]

 [tool.setuptools]
 packages = ["ci_tools"]
@@ -29,3 +45,11 @@ script-files = ["ci_tools/scripts/xci.sh"]
 [tool.setuptools.package-data]
 ci_tools = ["scripts/*.sh", "scripts/*.py", "vendor/**/*"]

+[tool.pytest.ini_options]
+pythonpath = ["."]
+
+[tool.coverage.run]
+omit = [
+    "*/vendor/*",
+    "*/__main__.py",
+]
diff --git a/scripts/ci.sh b/scripts/ci.sh
new file mode 100755
index 0000000..7b98462
--- /dev/null
+++ b/scripts/ci.sh
@@ -0,0 +1,16 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# Delegate to the shared CI script that we provide to consuming repositories.
+# This ensures ci_shared uses the same CI flow it provides to Zeus and Kalshi.
+ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
+cd "$ROOT_DIR"
+
+SHARED_CI_SCRIPT="${ROOT_DIR}/ci_tools/scripts/ci.sh"
+
+if [[ ! -x "${SHARED_CI_SCRIPT}" ]]; then
+  echo "[ci.sh] Shared CI script not found at ${SHARED_CI_SCRIPT}" >&2
+  exit 1
+fi
+
+exec "${SHARED_CI_SCRIPT}" "$@"
diff --git a/tests/test_ci.py b/tests/test_ci.py
new file mode 100644
index 0000000..d945875
--- /dev/null
+++ b/tests/test_ci.py
@@ -0,0 +1,233 @@
+"""Unit tests for ci_tools.ci module (compatibility layer)."""
+
+from __future__ import annotations
+
+
+# Import all exports from ci.py to verify they're accessible
+from ci_tools.ci import (
+    PatchPrompt,
+    apply_patch,
+    build_codex_command,
+    build_failure_context,
+    commit_and_push,
+    configure_runtime,
+    extract_coverage_deficits,
+    extract_unified_diff,
+    finalize_worktree,
+    gather_file_diff,
+    gather_git_diff,
+    gather_git_status,
+    has_unified_diff_header,
+    invoke_codex,
+    log_codex_interaction,
+    main,
+    patch_looks_risky,
+    perform_dry_run,
+    request_and_apply_patches,
+    request_codex_patch,
+    request_commit_message,
+    run_command,
+    run_repair_iterations,
+    tail_text,
+    truncate_diff_summary,
+    truncate_error,
+)
+
+
+class TestCiCompatibilityLayer:
+    """Tests verifying the ci.py compatibility layer exports."""
+
+    def test_all_exports_accessible(self):
+        """Test that all exported functions are accessible from ci.py."""
+        # Verify main entry point is callable
+        assert callable(main)
+
+        # Verify runtime configuration functions
+        assert callable(configure_runtime)
+        assert callable(perform_dry_run)
+        assert callable(run_repair_iterations)
+        assert callable(finalize_worktree)
+
+        # Verify command execution functions
+        assert callable(run_command)
+        assert callable(tail_text)
+
+        # Verify git helpers
+        assert callable(gather_git_diff)
+        assert callable(gather_git_status)
+        assert callable(gather_file_diff)
+
+        # Verify Codex interaction functions
+        assert callable(invoke_codex)
+        assert callable(build_codex_command)
+        assert callable(request_codex_patch)
+        assert callable(request_commit_message)
+        assert callable(log_codex_interaction)
+
+        # Verify patch functions
+        assert callable(apply_patch)
+        assert callable(patch_looks_risky)
+        assert callable(request_and_apply_patches)
+        assert callable(extract_unified_diff)
+        assert callable(has_unified_diff_header)
+
+        # Verify failure handling
+        assert callable(build_failure_context)
+        assert callable(extract_coverage_deficits)
+        assert callable(truncate_error)
+        assert callable(truncate_diff_summary)
+
+        # Verify commit/push functions
+        assert callable(commit_and_push)
+
+        # Verify data models are accessible
+        assert PatchPrompt is not None
+
+    def test_main_is_reexported_from_workflow(self):
+        """Test that main function is correctly re-exported."""
+        from ci_tools.ci_runtime.workflow import main as workflow_main
+        assert main is workflow_main
+
+    def test_configure_runtime_is_reexported(self):
+        """Test that configure_runtime is correctly re-exported."""
+        from ci_tools.ci_runtime.workflow import configure_runtime as workflow_cfg
+        assert configure_runtime is workflow_cfg
+
+    def test_run_command_is_reexported(self):
+        """Test that run_command is correctly re-exported."""
+        from ci_tools.ci_runtime.process import run_command as process_run
+        assert run_command is process_run
+
+    def test_apply_patch_is_reexported(self):
+        """Test that apply_patch is correctly re-exported."""
+        from ci_tools.ci_runtime.patching import apply_patch as patching_apply
+        assert apply_patch is patching_apply
+
+    def test_request_commit_message_is_reexported(self):
+        """Test that request_commit_message is correctly re-exported."""
+        from ci_tools.ci_runtime.messaging import (
+            request_commit_message as msg_request,
+        )
+        assert request_commit_message is msg_request
+
+    def test_patch_prompt_model_is_reexported(self):
+        """Test that PatchPrompt model is correctly re-exported."""
+        from ci_tools.ci_runtime.models import PatchPrompt as models_prompt
+        assert PatchPrompt is models_prompt
+
+
+class TestCiModuleStructure:
+    """Tests for ci.py module structure and organization."""
+
+    def test_module_has_all_attribute(self):
+        """Test that ci.py defines __all__ for explicit exports."""
+        import ci_tools.ci as ci_module
+        assert hasattr(ci_module, "__all__")
+        assert isinstance(ci_module.__all__, list)
+        assert len(ci_module.__all__) > 0
+
+    def test_all_items_in_all_are_exported(self):
+        """Test that every item in __all__ is actually exported."""
+        import ci_tools.ci as ci_module
+        for name in ci_module.__all__:
+            assert hasattr(ci_module, name), f"{name} in __all__ but not exported"
+
+    def test_main_entry_point_in_all(self):
+        """Test that main entry point is in __all__."""
+        import ci_tools.ci as ci_module
+        assert "main" in ci_module.__all__
+
+    def test_core_workflow_functions_in_all(self):
+        """Test that core workflow functions are in __all__."""
+        import ci_tools.ci as ci_module
+        core_functions = [
+            "configure_runtime",
+            "perform_dry_run",
+            "run_repair_iterations",
+            "finalize_worktree",
+        ]
+        for func in core_functions:
+            assert func in ci_module.__all__
+
+    def test_command_execution_functions_in_all(self):
+        """Test that command execution functions are in __all__."""
+        import ci_tools.ci as ci_module
+        command_functions = ["run_command", "tail_text"]
+        for func in command_functions:
+            assert func in ci_module.__all__
+
+    def test_git_functions_in_all(self):
+        """Test that git helper functions are in __all__."""
+        import ci_tools.ci as ci_module
+        git_functions = ["gather_git_diff", "gather_git_status", "gather_file_diff"]
+        for func in git_functions:
+            assert func in ci_module.__all__
+
+    def test_codex_functions_in_all(self):
+        """Test that Codex interaction functions are in __all__."""
+        import ci_tools.ci as ci_module
+        codex_functions = [
+            "invoke_codex",
+            "build_codex_command",
+            "request_codex_patch",
+            "request_commit_message",
+            "log_codex_interaction",
+        ]
+        for func in codex_functions:
+            assert func in ci_module.__all__
+
+    def test_patch_functions_in_all(self):
+        """Test that patch handling functions are in __all__."""
+        import ci_tools.ci as ci_module
+        patch_functions = [
+            "apply_patch",
+            "patch_looks_risky",
+            "request_and_apply_patches",
+            "extract_unified_diff",
+            "has_unified_diff_header",
+        ]
+        for func in patch_functions:
+            assert func in ci_module.__all__
+
+    def test_models_in_all(self):
+        """Test that data models are in __all__."""
+        import ci_tools.ci as ci_module
+        assert "PatchPrompt" in ci_module.__all__
+
+
+class TestCiIntegration:
+    """Integration tests for the ci.py compatibility layer."""
+
+    def test_can_import_from_ci_module_directly(self):
+        """Test that functions can be imported directly from ci_tools.ci."""
+        # This should not raise ImportError
+        from ci_tools.ci import main, configure_runtime, run_command
+        assert main is not None
+        assert configure_runtime is not None
+        assert run_command is not None
+
+    def test_ci_module_provides_same_interface_as_runtime(self):
+        """Test that ci.py provides the same interface as ci_runtime modules."""
+        import ci_tools.ci as ci_module
+        from ci_tools.ci_runtime import workflow, process, patching, messaging
+
+        # Verify workflow functions
+        assert ci_module.main is workflow.main
+        assert ci_module.configure_runtime is workflow.configure_runtime
+
+        # Verify process functions
+        assert ci_module.run_command is process.run_command
+        assert ci_module.tail_text is process.tail_text
+
+        # Verify patching functions
+        assert ci_module.apply_patch is patching.apply_patch
+        assert ci_module.patch_looks_risky is patching.patch_looks_risky
+
+        # Verify messaging functions
+        assert ci_module.request_commit_message is messaging.request_commit_message
+
+    def test_ci_module_docstring_exists(self):
+        """Test that ci.py has a module docstring."""
+        import ci_tools.ci as ci_module
+        assert ci_module.__doc__ is not None
+        assert len(ci_module.__doc__.strip()) > 0
diff --git a/tests/test_codex.py b/tests/test_codex.py
new file mode 100644
index 0000000..0c268f8
--- /dev/null
+++ b/tests/test_codex.py
@@ -0,0 +1,629 @@
+"""Unit tests for ci_tools.ci_runtime.codex module."""
+
+from __future__ import annotations
+
+from unittest.mock import MagicMock, Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.codex import (
+    build_codex_command,
+    invoke_codex,
+    truncate_error,
+    extract_unified_diff,
+    has_unified_diff_header,
+    request_codex_patch,
+    truncate_diff_summary,
+    risky_pattern_in_diff,
+    _feed_prompt,
+    _stream_output,
+)
+from ci_tools.ci_runtime.models import (
+    CodexCliError,
+    PatchPrompt,
+    FailureContext,
+)
+
+
+class TestBuildCodexCommand:
+    """Tests for build_codex_command function."""
+
+    def test_basic_command_without_reasoning_effort(self):
+        """Test command building without reasoning effort."""
+        result = build_codex_command("gpt-5-codex", None)
+        assert result == ["codex", "exec", "--model", "gpt-5-codex", "-"]
+
+    def test_command_with_reasoning_effort(self):
+        """Test command building with reasoning effort."""
+        result = build_codex_command("gpt-5-codex", "high")
+        assert result == [
+            "codex",
+            "exec",
+            "--model",
+            "gpt-5-codex",
+            "-c",
+            "model_reasoning_effort=high",
+            "-",
+        ]
+
+    def test_command_with_low_reasoning_effort(self):
+        """Test command building with low reasoning effort."""
+        result = build_codex_command("gpt-5-codex", "low")
+        assert result == [
+            "codex",
+            "exec",
+            "--model",
+            "gpt-5-codex",
+            "-c",
+            "model_reasoning_effort=low",
+            "-",
+        ]
+
+    def test_command_with_medium_reasoning_effort(self):
+        """Test command building with medium reasoning effort."""
+        result = build_codex_command("gpt-5-codex", "medium")
+        assert result == [
+            "codex",
+            "exec",
+            "--model",
+            "gpt-5-codex",
+            "-c",
+            "model_reasoning_effort=medium",
+            "-",
+        ]
+
+
+class TestFeedPrompt:
+    """Tests for _feed_prompt helper function."""
+
+    def test_writes_prompt_and_closes_stdin(self):
+        """Test that prompt is written to stdin and stream is closed."""
+        mock_process = Mock()
+        mock_process.stdin = Mock()
+
+        _feed_prompt(mock_process, "test prompt")
+
+        mock_process.stdin.write.assert_called_once_with("test prompt")
+        mock_process.stdin.close.assert_called_once()
+
+    def test_handles_broken_pipe_error(self):
+        """Test that BrokenPipeError is handled gracefully."""
+        mock_process = Mock()
+        mock_process.stdin = Mock()
+        mock_process.stdin.write.side_effect = BrokenPipeError()
+
+        # Should not raise
+        _feed_prompt(mock_process, "test prompt")
+
+    def test_handles_none_stdin(self):
+        """Test handling when stdin is None."""
+        mock_process = Mock()
+        mock_process.stdin = None
+
+        # Should not raise
+        _feed_prompt(mock_process, "test prompt")
+
+
+class TestStreamOutput:
+    """Tests for _stream_output helper function."""
+
+    def test_streams_stdout_and_stderr(self):
+        """Test that stdout and stderr are streamed correctly."""
+        mock_process = Mock()
+        mock_stdout = Mock()
+        mock_stderr = Mock()
+        mock_stdout.readline.side_effect = ["line1\n", "line2\n", ""]
+        mock_stderr.readline.side_effect = ["error1\n", ""]
+        mock_stdout.close = Mock()
+        mock_stderr.close = Mock()
+        mock_process.stdout = mock_stdout
+        mock_process.stderr = mock_stderr
+
+        stdout_lines, stderr_lines = _stream_output(mock_process)
+
+        assert stdout_lines == ["line1\n", "line2\n"]
+        assert stderr_lines == ["error1\n"]
+
+    def test_handles_stdout_only(self):
+        """Test streaming when only stdout exists."""
+        mock_process = Mock()
+        mock_stdout = Mock()
+        mock_stdout.readline.side_effect = ["output\n", ""]
+        mock_stdout.close = Mock()
+        mock_process.stdout = mock_stdout
+        mock_process.stderr = None
+
+        stdout_lines, stderr_lines = _stream_output(mock_process)
+
+        assert stdout_lines == ["output\n"]
+        assert stderr_lines == []
+
+    def test_handles_stderr_only(self):
+        """Test streaming when only stderr exists."""
+        mock_process = Mock()
+        mock_stderr = Mock()
+        mock_stderr.readline.side_effect = ["error\n", ""]
+        mock_stderr.close = Mock()
+        mock_process.stdout = None
+        mock_process.stderr = mock_stderr
+
+        stdout_lines, stderr_lines = _stream_output(mock_process)
+
+        assert stdout_lines == []
+        assert stderr_lines == ["error\n"]
+
+    def test_handles_no_streams(self):
+        """Test when neither stdout nor stderr exist."""
+        mock_process = Mock()
+        mock_process.stdout = None
+        mock_process.stderr = None
+
+        stdout_lines, stderr_lines = _stream_output(mock_process)
+
+        assert stdout_lines == []
+        assert stderr_lines == []
+
+
+class TestInvokeCodex:
+    """Tests for invoke_codex function."""
+
+    @patch("ci_tools.ci_runtime.codex.log_codex_interaction")
+    @patch("subprocess.Popen")
+    def test_successful_invocation(self, mock_popen, mock_log):
+        """Test successful Codex CLI invocation."""
+        mock_process = MagicMock()
+        mock_process.wait.return_value = 0
+        mock_process.stdout = Mock()
+        mock_process.stderr = Mock()
+        mock_process.stdout.readline.side_effect = ["assistant:\n", "response text\n", ""]
+        mock_process.stderr.readline.return_value = ""
+        mock_process.stdout.close = Mock()
+        mock_process.stderr.close = Mock()
+        mock_popen.return_value.__enter__ = Mock(return_value=mock_process)
+        mock_popen.return_value.__exit__ = Mock(return_value=False)
+
+        result = invoke_codex(
+            "test prompt",
+            model="gpt-5-codex",
+            description="test",
+            reasoning_effort="high",
+        )
+
+        assert result == "response text"
+        mock_log.assert_called_once()
+
+    @patch("ci_tools.ci_runtime.codex.log_codex_interaction")
+    @patch("subprocess.Popen")
+    def test_invocation_without_assistant_prefix(self, mock_popen, mock_log):
+        """Test invocation when response doesn't have assistant prefix."""
+        mock_process = MagicMock()
+        mock_process.wait.return_value = 0
+        mock_process.stdout = Mock()
+        mock_process.stderr = Mock()
+        mock_process.stdout.readline.side_effect = ["direct response\n", ""]
+        mock_process.stderr.readline.return_value = ""
+        mock_process.stdout.close = Mock()
+        mock_process.stderr.close = Mock()
+        mock_popen.return_value.__enter__ = Mock(return_value=mock_process)
+        mock_popen.return_value.__exit__ = Mock(return_value=False)
+
+        result = invoke_codex(
+            "test prompt",
+            model="gpt-5-codex",
+            description="test",
+            reasoning_effort=None,
+        )
+
+        assert result == "direct response"
+
+    @patch("ci_tools.ci_runtime.codex.log_codex_interaction")
+    @patch("subprocess.Popen")
+    def test_invocation_with_error(self, mock_popen, mock_log):
+        """Test invocation when Codex CLI returns error."""
+        mock_process = MagicMock()
+        mock_process.wait.return_value = 1
+        mock_process.stdout = Mock()
+        mock_process.stderr = Mock()
+        mock_process.stdout.readline.return_value = ""
+        mock_process.stderr.readline.side_effect = ["error occurred\n", ""]
+        mock_process.stdout.close = Mock()
+        mock_process.stderr.close = Mock()
+        mock_popen.return_value.__enter__ = Mock(return_value=mock_process)
+        mock_popen.return_value.__exit__ = Mock(return_value=False)
+
+        with pytest.raises(CodexCliError) as exc_info:
+            invoke_codex(
+                "test prompt",
+                model="gpt-5-codex",
+                description="test",
+                reasoning_effort="low",
+            )
+
+        assert "exit status 1" in str(exc_info.value)
+
+    @patch("ci_tools.ci_runtime.codex.log_codex_interaction")
+    @patch("subprocess.Popen")
+    def test_invocation_returns_stderr_when_no_stdout(self, mock_popen, mock_log):
+        """Test that stderr is returned when stdout is empty."""
+        mock_process = MagicMock()
+        mock_process.wait.return_value = 0
+        mock_process.stdout = Mock()
+        mock_process.stderr = Mock()
+        mock_process.stdout.readline.return_value = ""
+        mock_process.stderr.readline.side_effect = ["stderr output\n", ""]
+        mock_process.stdout.close = Mock()
+        mock_process.stderr.close = Mock()
+        mock_popen.return_value.__enter__ = Mock(return_value=mock_process)
+        mock_popen.return_value.__exit__ = Mock(return_value=False)
+
+        result = invoke_codex(
+            "test prompt",
+            model="gpt-5-codex",
+            description="test",
+            reasoning_effort="medium",
+        )
+
+        assert result == "stderr output"
+
+
+class TestTruncateError:
+    """Tests for truncate_error function."""
+
+    def test_returns_placeholder_for_none(self):
+        """Test that None error returns placeholder."""
+        assert truncate_error(None) == "(none)"
+
+    def test_returns_placeholder_for_empty_string(self):
+        """Test that empty string returns placeholder."""
+        assert truncate_error("") == "(none)"
+        # Whitespace-only strings get stripped to empty string
+        assert truncate_error("   ") == ""
+
+    def test_returns_short_error_unchanged(self):
+        """Test that short errors are returned as-is."""
+        error = "This is a short error message"
+        assert truncate_error(error) == error
+
+    def test_truncates_long_error(self):
+        """Test that long errors are truncated."""
+        error = "x" * 3000
+        result = truncate_error(error, limit=2000)
+        assert len(result) == 2000 + len("...(truncated)")
+        assert result.endswith("...(truncated)")
+
+    def test_custom_limit(self):
+        """Test truncation with custom limit."""
+        error = "x" * 200
+        result = truncate_error(error, limit=100)
+        assert result == "x" * 100 + "...(truncated)"
+
+    def test_strips_whitespace(self):
+        """Test that whitespace is stripped before checking length."""
+        error = "  error message  "
+        assert truncate_error(error) == "error message"
+
+
+class TestExtractUnifiedDiff:
+    """Tests for extract_unified_diff function."""
+
+    def test_returns_none_for_empty_response(self):
+        """Test that empty response returns None."""
+        assert extract_unified_diff("") is None
+        assert extract_unified_diff(None) is None
+
+    def test_returns_none_for_noop(self):
+        """Test that NOOP response returns None."""
+        assert extract_unified_diff("NOOP") is None
+        assert extract_unified_diff("noop") is None
+        assert extract_unified_diff("  NOOP  ") is None
+
+    def test_extracts_diff_block_with_marker(self):
+        """Test extraction of diff block with ```diff marker."""
+        response = """
+Here's the fix:
+```diff
+diff --git a/file.py b/file.py
+--- a/file.py
++++ b/file.py
+@@ -1,3 +1,3 @@
+-old line
++new line
+```
+"""
+        result = extract_unified_diff(response)
+        assert result is not None
+        assert result.startswith("diff --git")
+
+    def test_extracts_diff_block_without_marker(self):
+        """Test extraction of diff block without language marker."""
+        response = """
+```
+diff --git a/file.py b/file.py
+--- a/file.py
++++ b/file.py
+```
+"""
+        result = extract_unified_diff(response)
+        assert result is not None
+        assert "diff --git" in result
+
+    def test_extracts_first_diff_from_multiple_blocks(self):
+        """Test that first diff block is extracted when multiple exist."""
+        response = """
+```
+Some text
+```
+```diff
+--- a/file.py
++++ b/file.py
+@@ -1 +1 @@
+-old
++new
+```
+"""
+        result = extract_unified_diff(response)
+        assert "--- a/file.py" in result
+
+    def test_extracts_diff_starting_with_index(self):
+        """Test extraction of diff starting with Index:."""
+        response = """
+```
+Index: file.py
+===================================================================
+--- file.py
++++ file.py
+```
+"""
+        result = extract_unified_diff(response)
+        assert result.startswith("Index:")
+
+    def test_returns_first_code_block_if_no_diff_markers(self):
+        """Test that first code block is returned if no diff markers found."""
+        response = """
+```
+some code
+without diff markers
+```
+"""
+        result = extract_unified_diff(response)
+        assert "some code" in result
+
+    def test_returns_response_text_if_no_code_blocks(self):
+        """Test that raw response is returned if no code blocks exist."""
+        response = "diff --git a/file.py b/file.py"
+        result = extract_unified_diff(response)
+        assert result == response
+
+
+class TestHasUnifiedDiffHeader:
+    """Tests for has_unified_diff_header function."""
+
+    def test_detects_diff_git_header(self):
+        """Test detection of diff --git header."""
+        diff = "diff --git a/file.py b/file.py"
+        assert has_unified_diff_header(diff) is True
+
+    def test_detects_minus_line_header(self):
+        """Test detection of --- header."""
+        diff = "--- a/file.py"
+        assert has_unified_diff_header(diff) is True
+
+    def test_detects_plus_line_header(self):
+        """Test detection of +++ header."""
+        diff = "+++ b/file.py"
+        assert has_unified_diff_header(diff) is True
+
+    def test_returns_false_for_no_headers(self):
+        """Test that False is returned when no headers present."""
+        diff = "just some text\nwithout headers"
+        assert has_unified_diff_header(diff) is False
+
+    def test_detects_header_in_middle_of_text(self):
+        """Test detection of header in middle of text."""
+        diff = "some text\n--- a/file.py\nmore text"
+        assert has_unified_diff_header(diff) is True
+
+
+class TestRequestCodexPatch:
+    """Tests for request_codex_patch function."""
+
+    @patch("ci_tools.ci_runtime.codex.invoke_codex")
+    def test_builds_prompt_with_all_context(self, mock_invoke):
+        """Test that prompt is built with all context fields."""
+        mock_invoke.return_value = "diff response"
+
+        failure_context = FailureContext(
+            log_excerpt="test failed",
+            summary="failure summary",
+            implicated_files=["file.py"],
+            focused_diff="--- a/file.py",
+            coverage_report=None,
+        )
+        prompt = PatchPrompt(
+            command="make test",
+            failure_context=failure_context,
+            git_diff="working tree diff",
+            git_status="M file.py",
+            iteration=1,
+            patch_error="previous error",
+            attempt=1,
+        )
+
+        result = request_codex_patch(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            prompt=prompt,
+        )
+
+        assert result == "diff response"
+        call_args = mock_invoke.call_args
+        prompt_text = call_args[0][0]
+        assert "make test" in prompt_text
+        assert "Iteration: 1" in prompt_text
+        assert "Patch attempt: 1" in prompt_text
+        assert "M file.py" in prompt_text
+        assert "failure summary" in prompt_text
+        assert "test failed" in prompt_text
+        assert "previous error" in prompt_text
+
+    @patch("ci_tools.ci_runtime.codex.invoke_codex")
+    def test_handles_none_git_status(self, mock_invoke):
+        """Test handling of None git_status."""
+        mock_invoke.return_value = "diff response"
+
+        failure_context = FailureContext(
+            log_excerpt="error",
+            summary="summary",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=None,
+        )
+        prompt = PatchPrompt(
+            command="test",
+            failure_context=failure_context,
+            git_diff="",
+            git_status=None,
+            iteration=1,
+            patch_error=None,
+            attempt=1,
+        )
+
+        request_codex_patch(
+            model="gpt-5-codex",
+            reasoning_effort="low",
+            prompt=prompt,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt_text = call_args[0][0]
+        assert "(clean)" in prompt_text
+
+    @patch("ci_tools.ci_runtime.codex.invoke_codex")
+    def test_truncates_patch_error(self, mock_invoke):
+        """Test that patch error is truncated in prompt."""
+        mock_invoke.return_value = "diff response"
+
+        long_error = "x" * 3000
+        failure_context = FailureContext(
+            log_excerpt="error",
+            summary="summary",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=None,
+        )
+        prompt = PatchPrompt(
+            command="test",
+            failure_context=failure_context,
+            git_diff="",
+            git_status="",
+            iteration=1,
+            patch_error=long_error,
+            attempt=1,
+        )
+
+        request_codex_patch(
+            model="gpt-5-codex",
+            reasoning_effort="medium",
+            prompt=prompt,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt_text = call_args[0][0]
+        # Prompt should contain truncated error
+        assert "...(truncated)" in prompt_text
+
+
+class TestTruncateDiffSummary:
+    """Tests for truncate_diff_summary function."""
+
+    def test_returns_false_for_small_diff(self):
+        """Test that small diffs are not flagged."""
+        diff = "+line1\n-line2\n line3"
+        exceeded, message = truncate_diff_summary(diff, line_limit=10)
+        assert exceeded is False
+        assert message is None
+
+    def test_returns_true_for_large_diff(self):
+        """Test that large diffs are flagged."""
+        diff = "\n".join(["+line" for _ in range(100)])
+        exceeded, message = truncate_diff_summary(diff, line_limit=50)
+        assert exceeded is True
+        assert "100" in message
+        assert "50" in message
+
+    def test_counts_only_changed_lines(self):
+        """Test that only +/- lines are counted."""
+        diff = """
+diff --git a/file.py b/file.py
+--- a/file.py
++++ b/file.py
+@@ -1,3 +1,3 @@
+ context line
+-removed line
++added line
+ more context
+"""
+        exceeded, message = truncate_diff_summary(diff, line_limit=10)
+        assert exceeded is False
+
+    def test_exact_limit_boundary(self):
+        """Test behavior at exact limit boundary."""
+        diff = "\n".join(["+line" for _ in range(10)])
+        exceeded, message = truncate_diff_summary(diff, line_limit=10)
+        assert exceeded is False
+
+        diff = "\n".join(["+line" for _ in range(11)])
+        exceeded, message = truncate_diff_summary(diff, line_limit=10)
+        assert exceeded is True
+
+
+class TestRiskyPatternInDiff:
+    """Tests for risky_pattern_in_diff function."""
+
+    def test_detects_drop_table(self):
+        """Test detection of DROP TABLE pattern."""
+        diff = "+  DROP TABLE users;"
+        result = risky_pattern_in_diff(diff)
+        assert result is not None
+        assert "DROP" in result
+
+    def test_detects_rm_rf(self):
+        """Test detection of rm -rf pattern."""
+        diff = "+  rm -rf /important/directory"
+        result = risky_pattern_in_diff(diff)
+        assert result is not None
+        assert "rm" in result
+
+    def test_detects_subprocess_rm(self):
+        """Test detection of subprocess.run with rm."""
+        diff = '+  subprocess.run(["rm", "-rf", path])'
+        result = risky_pattern_in_diff(diff)
+        assert result is not None
+        assert "subprocess" in result or "rm" in result
+
+    def test_returns_none_for_safe_diff(self):
+        """Test that safe diffs return None."""
+        diff = """
++def safe_function():
++    return "safe"
+-old_line = value
++new_line = value
+"""
+        result = risky_pattern_in_diff(diff)
+        assert result is None
+
+    def test_case_insensitive_detection(self):
+        """Test that detection is case insensitive."""
+        diff = "+  drop table Users;"
+        result = risky_pattern_in_diff(diff)
+        assert result is not None
+
+    def test_returns_first_match(self):
+        """Test that first matched pattern is returned."""
+        diff = """
++  DROP TABLE users;
++  rm -rf /
+"""
+        result = risky_pattern_in_diff(diff)
+        # Should return the first pattern matched
+        assert result is not None
diff --git a/tests/test_complexity_guard.py b/tests/test_complexity_guard.py
index 639cdfd..dd26644 100644
--- a/tests/test_complexity_guard.py
+++ b/tests/test_complexity_guard.py
@@ -1,12 +1,13 @@
 from __future__ import annotations

-import textwrap
 import sys
+import textwrap
 from pathlib import Path

 import pytest

-from scripts import complexity_guard
+sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))
+import complexity_guard


 def write_module(path: Path, source: str) -> None:
diff --git a/tests/test_config.py b/tests/test_config.py
new file mode 100644
index 0000000..c7ccc22
--- /dev/null
+++ b/tests/test_config.py
@@ -0,0 +1,394 @@
+"""Unit tests for ci_tools.ci_runtime.config module."""
+
+from __future__ import annotations
+
+import json
+import subprocess
+from pathlib import Path
+from unittest.mock import Mock, patch
+
+
+from ci_tools.ci_runtime.config import (
+    CONFIG_CANDIDATES,
+    COVERAGE_THRESHOLD,
+    DEFAULT_PROTECTED_PATH_PREFIXES,
+    DEFAULT_REASONING_EFFORT,
+    DEFAULT_REPO_CONTEXT,
+    PROTECTED_PATH_PREFIXES,
+    REASONING_EFFORT_CHOICES,
+    REPO_CONFIG,
+    REPO_CONTEXT,
+    REPO_ROOT,
+    REQUIRED_MODEL,
+    RISKY_PATTERNS,
+    detect_repo_root,
+    load_repo_config,
+)
+
+
+class TestConstants:
+    """Tests for module-level constants."""
+
+    def test_config_candidates(self):
+        """Test CONFIG_CANDIDATES contains expected filenames."""
+        assert "ci_shared.config.json" in CONFIG_CANDIDATES
+        assert ".ci_shared.config.json" in CONFIG_CANDIDATES
+
+    def test_default_protected_path_prefixes(self):
+        """Test DEFAULT_PROTECTED_PATH_PREFIXES contains expected paths."""
+        assert "ci.py" in DEFAULT_PROTECTED_PATH_PREFIXES
+        assert "ci_tools/" in DEFAULT_PROTECTED_PATH_PREFIXES
+        assert "scripts/ci.sh" in DEFAULT_PROTECTED_PATH_PREFIXES
+        assert "Makefile" in DEFAULT_PROTECTED_PATH_PREFIXES
+
+    def test_risky_patterns_is_tuple_of_regex(self):
+        """Test RISKY_PATTERNS contains compiled regex patterns."""
+        assert isinstance(RISKY_PATTERNS, tuple)
+        assert len(RISKY_PATTERNS) > 0
+        # Test that patterns can match expected dangerous strings
+        dangerous_sql = "DROP TABLE users"
+        dangerous_rm = "rm -rf /"
+        dangerous_subprocess = "subprocess.run(['rm', '-rf'])"
+
+        matched = False
+        for pattern in RISKY_PATTERNS:
+            if pattern.search(dangerous_sql) or pattern.search(
+                dangerous_rm
+            ) or pattern.search(dangerous_subprocess):
+                matched = True
+                break
+        assert matched, "RISKY_PATTERNS should match dangerous patterns"
+
+    def test_required_model(self):
+        """Test REQUIRED_MODEL is set."""
+        assert REQUIRED_MODEL == "gpt-5-codex"
+
+    def test_reasoning_effort_choices(self):
+        """Test REASONING_EFFORT_CHOICES contains expected values."""
+        assert "low" in REASONING_EFFORT_CHOICES
+        assert "medium" in REASONING_EFFORT_CHOICES
+        assert "high" in REASONING_EFFORT_CHOICES
+
+    def test_default_reasoning_effort(self):
+        """Test DEFAULT_REASONING_EFFORT is a valid choice."""
+        assert DEFAULT_REASONING_EFFORT in REASONING_EFFORT_CHOICES
+
+
+class TestDetectRepoRoot:
+    """Tests for detect_repo_root function."""
+
+    def test_detect_repo_root_git_success(self):
+        """Test detect_repo_root uses git when available."""
+        with patch("subprocess.run") as mock_run:
+            mock_result = Mock()
+            mock_result.stdout = "/path/to/repo\n"
+            mock_run.return_value = mock_result
+
+            with patch("pathlib.Path.exists", return_value=True):
+                result = detect_repo_root()
+                assert result == Path("/path/to/repo")
+
+            mock_run.assert_called_once()
+            args = mock_run.call_args[0][0]
+            assert args == ["git", "rev-parse", "--show-toplevel"]
+
+    def test_detect_repo_root_git_failure_returns_cwd(self):
+        """Test detect_repo_root falls back to cwd when git fails."""
+        with patch(
+            "subprocess.run", side_effect=subprocess.CalledProcessError(1, "git")
+        ):
+            result = detect_repo_root()
+            assert result == Path.cwd()
+
+    def test_detect_repo_root_git_not_found_returns_cwd(self):
+        """Test detect_repo_root falls back to cwd when git not found."""
+        with patch("subprocess.run", side_effect=FileNotFoundError):
+            result = detect_repo_root()
+            assert result == Path.cwd()
+
+    def test_detect_repo_root_path_does_not_exist(self):
+        """Test detect_repo_root falls back to cwd when path doesn't exist."""
+        with patch("subprocess.run") as mock_run:
+            mock_result = Mock()
+            mock_result.stdout = "/nonexistent/path\n"
+            mock_run.return_value = mock_result
+
+            with patch("pathlib.Path.exists", return_value=False):
+                result = detect_repo_root()
+                assert result == Path.cwd()
+
+
+class TestLoadRepoConfig:
+    """Tests for load_repo_config function."""
+
+    def test_load_repo_config_first_candidate(self, tmp_path):
+        """Test loading config from first candidate file."""
+        config_file = tmp_path / "ci_shared.config.json"
+        config_data = {"repo_context": "test context", "coverage_threshold": 85.0}
+        config_file.write_text(json.dumps(config_data))
+
+        result = load_repo_config(tmp_path)
+        assert result == config_data
+
+    def test_load_repo_config_second_candidate(self, tmp_path):
+        """Test loading config from second candidate file."""
+        config_file = tmp_path / ".ci_shared.config.json"
+        config_data = {"protected_path_prefixes": ["custom/"]}
+        config_file.write_text(json.dumps(config_data))
+
+        result = load_repo_config(tmp_path)
+        assert result == config_data
+
+    def test_load_repo_config_prefers_first_candidate(self, tmp_path):
+        """Test that first candidate is preferred over second."""
+        config1 = tmp_path / "ci_shared.config.json"
+        config1.write_text(json.dumps({"key": "from_first"}))
+
+        config2 = tmp_path / ".ci_shared.config.json"
+        config2.write_text(json.dumps({"key": "from_second"}))
+
+        result = load_repo_config(tmp_path)
+        assert result["key"] == "from_first"
+
+    def test_load_repo_config_no_file_returns_empty(self, tmp_path):
+        """Test loading returns empty dict when no config file exists."""
+        result = load_repo_config(tmp_path)
+        assert result == {}
+
+    def test_load_repo_config_invalid_json(self, tmp_path, capsys):
+        """Test loading handles invalid JSON gracefully."""
+        config_file = tmp_path / "ci_shared.config.json"
+        config_file.write_text("{ invalid json }")
+
+        result = load_repo_config(tmp_path)
+        assert result == {}
+
+        captured = capsys.readouterr()
+        assert "warning" in captured.err.lower()
+        assert "Failed to parse" in captured.err
+
+    def test_load_repo_config_non_dict_returns_empty(self, tmp_path):
+        """Test loading returns empty dict when JSON is not a dict."""
+        config_file = tmp_path / "ci_shared.config.json"
+        config_file.write_text(json.dumps(["not", "a", "dict"]))
+
+        result = load_repo_config(tmp_path)
+        assert result == {}
+
+    def test_load_repo_config_empty_json(self, tmp_path):
+        """Test loading empty JSON object."""
+        config_file = tmp_path / "ci_shared.config.json"
+        config_file.write_text(json.dumps({}))
+
+        result = load_repo_config(tmp_path)
+        assert result == {}
+
+    def test_load_repo_config_complex_structure(self, tmp_path):
+        """Test loading config with nested structure."""
+        config_file = tmp_path / "ci_shared.config.json"
+        config_data = {
+            "repo_context": "Complex context",
+            "coverage_threshold": 90.0,
+            "protected_path_prefixes": ["ci/", "scripts/"],
+            "nested": {"key": "value"},
+        }
+        config_file.write_text(json.dumps(config_data))
+
+        result = load_repo_config(tmp_path)
+        assert result == config_data
+
+
+class TestCoercionFunctions:
+    """Tests for internal coercion functions."""
+
+    def test_coerce_repo_context_with_string(self):
+        """Test _coerce_repo_context with string value."""
+        from ci_tools.ci_runtime.config import _coerce_repo_context
+
+        config = {"repo_context": "Custom context"}
+        result = _coerce_repo_context(config, "default")
+        assert result == "Custom context"
+
+    def test_coerce_repo_context_missing_uses_default(self):
+        """Test _coerce_repo_context uses default when key missing."""
+        from ci_tools.ci_runtime.config import _coerce_repo_context
+
+        config = {}
+        result = _coerce_repo_context(config, "default context")
+        assert result == "default context"
+
+    def test_coerce_repo_context_wrong_type_uses_default(self):
+        """Test _coerce_repo_context uses default for non-string value."""
+        from ci_tools.ci_runtime.config import _coerce_repo_context
+
+        config = {"repo_context": 123}
+        result = _coerce_repo_context(config, "default")
+        assert result == "default"
+
+    def test_coerce_protected_prefixes_with_list(self):
+        """Test _coerce_protected_prefixes with list value."""
+        from ci_tools.ci_runtime.config import _coerce_protected_prefixes
+
+        config = {"protected_path_prefixes": ["path1/", "path2/"]}
+        result = _coerce_protected_prefixes(config, ["default/"])
+        assert result == ("path1/", "path2/")
+
+    def test_coerce_protected_prefixes_with_tuple(self):
+        """Test _coerce_protected_prefixes with tuple value."""
+        from ci_tools.ci_runtime.config import _coerce_protected_prefixes
+
+        config = {"protected_path_prefixes": ("path1/", "path2/")}
+        result = _coerce_protected_prefixes(config, ["default/"])
+        assert result == ("path1/", "path2/")
+
+    def test_coerce_protected_prefixes_with_set(self):
+        """Test _coerce_protected_prefixes with set value."""
+        from ci_tools.ci_runtime.config import _coerce_protected_prefixes
+
+        config = {"protected_path_prefixes": {"path1/", "path2/"}}
+        result = _coerce_protected_prefixes(config, ["default/"])
+        assert isinstance(result, tuple)
+        assert set(result) == {"path1/", "path2/"}
+
+    def test_coerce_protected_prefixes_missing_uses_default(self):
+        """Test _coerce_protected_prefixes uses default when key missing."""
+        from ci_tools.ci_runtime.config import _coerce_protected_prefixes
+
+        config = {}
+        result = _coerce_protected_prefixes(config, ["default/"])
+        assert result == ("default/",)
+
+    def test_coerce_protected_prefixes_wrong_type_uses_default(self):
+        """Test _coerce_protected_prefixes uses default for wrong type."""
+        from ci_tools.ci_runtime.config import _coerce_protected_prefixes
+
+        config = {"protected_path_prefixes": "not a list"}
+        result = _coerce_protected_prefixes(config, ["default/"])
+        assert result == ("default/",)
+
+    def test_coerce_coverage_threshold_with_float(self):
+        """Test _coerce_coverage_threshold with float value."""
+        from ci_tools.ci_runtime.config import _coerce_coverage_threshold
+
+        config = {"coverage_threshold": 85.5}
+        result = _coerce_coverage_threshold(config, 80.0)
+        assert result == 85.5
+
+    def test_coerce_coverage_threshold_with_int(self):
+        """Test _coerce_coverage_threshold with int value."""
+        from ci_tools.ci_runtime.config import _coerce_coverage_threshold
+
+        config = {"coverage_threshold": 90}
+        result = _coerce_coverage_threshold(config, 80.0)
+        assert result == 90.0
+
+    def test_coerce_coverage_threshold_with_string(self):
+        """Test _coerce_coverage_threshold with string value."""
+        from ci_tools.ci_runtime.config import _coerce_coverage_threshold
+
+        config = {"coverage_threshold": "75.5"}
+        result = _coerce_coverage_threshold(config, 80.0)
+        assert result == 75.5
+
+    def test_coerce_coverage_threshold_missing_uses_default(self):
+        """Test _coerce_coverage_threshold uses default when key missing."""
+        from ci_tools.ci_runtime.config import _coerce_coverage_threshold
+
+        config = {}
+        result = _coerce_coverage_threshold(config, 80.0)
+        assert result == 80.0
+
+    def test_coerce_coverage_threshold_invalid_string_uses_default(self):
+        """Test _coerce_coverage_threshold uses default for invalid string."""
+        from ci_tools.ci_runtime.config import _coerce_coverage_threshold
+
+        config = {"coverage_threshold": "not-a-number"}
+        result = _coerce_coverage_threshold(config, 80.0)
+        assert result == 80.0
+
+
+class TestModuleLevelInitialization:
+    """Tests for module-level variables initialized at import."""
+
+    def test_repo_root_is_path(self):
+        """Test REPO_ROOT is a Path object."""
+        assert isinstance(REPO_ROOT, Path)
+
+    def test_repo_config_is_dict(self):
+        """Test REPO_CONFIG is a dict."""
+        assert isinstance(REPO_CONFIG, dict)
+
+    def test_repo_context_is_string(self):
+        """Test REPO_CONTEXT is a string."""
+        assert isinstance(REPO_CONTEXT, str)
+        # Should be either the default or loaded from config
+        assert len(REPO_CONTEXT) > 0
+
+    def test_protected_path_prefixes_is_tuple(self):
+        """Test PROTECTED_PATH_PREFIXES is a tuple."""
+        assert isinstance(PROTECTED_PATH_PREFIXES, tuple)
+        # Should contain at least the default paths
+        assert len(PROTECTED_PATH_PREFIXES) > 0
+
+    def test_coverage_threshold_is_float(self):
+        """Test COVERAGE_THRESHOLD is a float."""
+        assert isinstance(COVERAGE_THRESHOLD, float)
+        assert COVERAGE_THRESHOLD > 0.0
+        assert COVERAGE_THRESHOLD <= 100.0
+
+    def test_default_repo_context_contains_key_info(self):
+        """Test DEFAULT_REPO_CONTEXT contains expected guidance."""
+        assert "Python 3.10+" in DEFAULT_REPO_CONTEXT
+        assert "src/" in DEFAULT_REPO_CONTEXT
+        assert "tests/" in DEFAULT_REPO_CONTEXT
+        assert "unified diff" in DEFAULT_REPO_CONTEXT
+
+
+class TestRiskyPatterns:
+    """Tests for RISKY_PATTERNS regex patterns."""
+
+    def test_risky_pattern_drop_table(self):
+        """Test RISKY_PATTERNS detects DROP TABLE statements."""
+        test_cases = [
+            "DROP TABLE users",
+            "drop table accounts",
+            "DROP  TABLE  products",
+        ]
+        for test_case in test_cases:
+            matched = any(pattern.search(test_case) for pattern in RISKY_PATTERNS)
+            assert matched, f"Should match: {test_case}"
+
+    def test_risky_pattern_rm_rf(self):
+        """Test RISKY_PATTERNS detects rm -rf commands."""
+        test_cases = [
+            "rm -rf /",
+            "rm -rf ./directory",
+            "rm -rf *",
+        ]
+        for test_case in test_cases:
+            matched = any(pattern.search(test_case) for pattern in RISKY_PATTERNS)
+            assert matched, f"Should match: {test_case}"
+
+    def test_risky_pattern_subprocess_rm(self):
+        """Test RISKY_PATTERNS detects subprocess.run with rm."""
+        test_cases = [
+            "subprocess.run(['rm', '-rf'])",
+            'subprocess.run(["rm", "file"])',
+            "subprocess.run('rm -rf', shell=True)",
+        ]
+        for test_case in test_cases:
+            matched = any(pattern.search(test_case) for pattern in RISKY_PATTERNS)
+            assert matched, f"Should match: {test_case}"
+
+    def test_risky_pattern_safe_commands(self):
+        """Test RISKY_PATTERNS doesn't match safe commands."""
+        safe_cases = [
+            "git rm file.txt",
+            "remove_file('test.txt')",
+            "SELECT * FROM users",
+            "subprocess.run(['ls', '-l'])",
+        ]
+        for safe_case in safe_cases:
+            matched = any(pattern.search(safe_case) for pattern in RISKY_PATTERNS)
+            # These should not match the risky patterns
+            assert not matched, f"Should not match: {safe_case}"
diff --git a/tests/test_coverage.py b/tests/test_coverage.py
new file mode 100644
index 0000000..1c0f850
--- /dev/null
+++ b/tests/test_coverage.py
@@ -0,0 +1,417 @@
+"""Unit tests for ci_tools.ci_runtime.coverage module."""
+
+from __future__ import annotations
+
+
+from ci_tools.ci_runtime.coverage import (
+    _find_coverage_table,
+    _parse_coverage_entries,
+    extract_coverage_deficits,
+)
+from ci_tools.ci_runtime.models import CoverageCheckResult, CoverageDeficit
+
+
+class TestFindCoverageTable:
+    """Tests for _find_coverage_table helper function."""
+
+    def test_finds_coverage_table_with_header(self):
+        """Test finds coverage table when header is present."""
+        lines = [
+            "Running tests...",
+            "Name                 Stmts   Miss  Cover",
+            "-----------------------------------------",
+            "src/module.py          100     20    80%",
+            "",
+            "TOTAL                  100     20    80%",
+        ]
+        table = _find_coverage_table(lines)
+        assert table is not None
+        assert len(table) > 1
+        assert "Name" in table[0]
+        assert "Cover" in table[0]
+
+    def test_returns_none_when_no_header_found(self):
+        """Test returns None when no coverage header found."""
+        lines = ["Just some output", "No coverage table here"]
+        table = _find_coverage_table(lines)
+        assert table is None
+
+    def test_includes_all_rows_until_blank_line(self):
+        """Test includes all table rows until blank line."""
+        lines = [
+            "Name                 Stmts   Cover",
+            "-----------------------------------",
+            "file1.py                10    50%",
+            "file2.py                20    75%",
+            "",
+            "Other output",
+        ]
+        table = _find_coverage_table(lines)
+        assert len(table) == 5  # header + separator + 2 files + blank
+        assert "file1.py" in table[2]
+        assert "file2.py" in table[3]
+
+    def test_handles_table_at_end_of_output(self):
+        """Test handles coverage table at end of output."""
+        lines = [
+            "Previous output",
+            "Name       Cover",
+            "file.py      80%",
+        ]
+        table = _find_coverage_table(lines)
+        assert table is not None
+        assert len(table) == 2
+
+    def test_returns_none_for_header_only(self):
+        """Test returns None when only header exists."""
+        lines = ["Name                 Stmts   Cover"]
+        table = _find_coverage_table(lines)
+        assert table is None  # len(table) must be > 1
+
+    def test_handles_whitespace_in_header(self):
+        """Test handles extra whitespace in header line."""
+        lines = [
+            "  Name                 Stmts   Cover  ",
+            "file.py                  10    50%",
+            "",
+        ]
+        table = _find_coverage_table(lines)
+        assert table is not None
+        assert "Name" in table[0]
+
+    def test_finds_first_matching_header(self):
+        """Test finds first matching coverage header."""
+        lines = [
+            "First Name section",
+            "Name                 Cover",
+            "file1.py               50%",
+            "",
+            "Second Name section",
+            "Name                 Cover",
+            "file2.py               60%",
+        ]
+        table = _find_coverage_table(lines)
+        assert table is not None
+        assert "file1.py" in "".join(table)
+        # Should stop at first blank line after first header
+
+
+class TestParseCoverageEntries:
+    """Tests for _parse_coverage_entries helper function."""
+
+    def test_parses_entries_below_threshold(self):
+        """Test parses coverage entries below threshold."""
+        rows = [
+            "Name                 Stmts   Miss  Cover",
+            "-----------------------------------------",
+            "src/module.py          100     40    60%",
+            "src/other.py           200     10    95%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+        assert deficits[0].path == "src/module.py"
+        assert deficits[0].coverage == 60.0
+
+    def test_ignores_entries_above_threshold(self):
+        """Test ignores entries at or above threshold."""
+        rows = [
+            "module1.py    100     10    90%",
+            "module2.py    100     20    80%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 0
+
+    def test_skips_separator_lines(self):
+        """Test skips separator lines with dashes."""
+        rows = [
+            "Name                 Stmts   Miss  Cover",
+            "-----------------------------------------",
+            "file.py                100     50    50%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+        assert deficits[0].path == "file.py"
+
+    def test_skips_total_row(self):
+        """Test skips TOTAL summary row."""
+        rows = [
+            "module.py          100     50    50%",
+            "TOTAL              100     50    50%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+        assert deficits[0].path == "module.py"
+
+    def test_handles_paths_with_spaces(self):
+        """Test handles file paths that contain spaces."""
+        rows = [
+            "src/my module.py       100     50    50%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+        assert "my module.py" in deficits[0].path
+
+    def test_parses_percentage_correctly(self):
+        """Test parses coverage percentage correctly."""
+        rows = [
+            "file.py    10    5    45%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=50.0)
+        assert deficits[0].coverage == 45.0
+
+    def test_skips_malformed_rows(self):
+        """Test skips rows that don't have expected format."""
+        rows = [
+            "incomplete",
+            "only two tokens",
+            "file.py    100     50    50%",  # valid
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+        assert deficits[0].path == "file.py"
+
+    def test_skips_rows_without_percentage(self):
+        """Test skips rows where coverage doesn't end with %."""
+        rows = [
+            "file.py    100     50    noPercent",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 0
+
+    def test_handles_empty_row_list(self):
+        """Test handles empty row list."""
+        deficits = _parse_coverage_entries([], threshold=80.0)
+        assert len(deficits) == 0
+
+    def test_handles_blank_lines(self):
+        """Test handles blank lines in rows."""
+        rows = [
+            "",
+            "   ",
+            "file.py    100    50    50%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 1
+
+    def test_parses_float_percentages(self):
+        """Test parses float percentages correctly."""
+        rows = [
+            "file.py    100    27    72.5%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=75.0)
+        assert len(deficits) == 1
+        assert deficits[0].coverage == 72.5
+
+    def test_handles_value_error_in_parsing(self):
+        """Test handles ValueError when parsing invalid percentage."""
+        rows = [
+            "file.py    100    50    abc%",
+        ]
+        deficits = _parse_coverage_entries(rows, threshold=80.0)
+        assert len(deficits) == 0
+
+
+class TestExtractCoverageDeficits:
+    """Tests for extract_coverage_deficits public API."""
+
+    def test_extracts_deficits_from_pytest_output(self):
+        """Test extracts coverage deficits from pytest output."""
+        output = """
+========== test session starts ===========
+platform linux -- Python 3.10.0
+
+----------- coverage: platform linux, python 3.10.0 -----------
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+src/module1.py         100     40    60%
+src/module2.py         200     10    95%
+-----------------------------------------
+TOTAL                  300     50    83%
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert len(result.deficits) == 1
+        assert result.deficits[0].path == "src/module1.py"
+        assert result.deficits[0].coverage == 60.0
+        assert result.threshold == 80.0
+
+    def test_returns_none_when_no_table_found(self):
+        """Test returns None when no coverage table found."""
+        output = "No coverage information in this output"
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is None
+
+    def test_returns_none_for_empty_output(self):
+        """Test returns None for empty output string."""
+        result = extract_coverage_deficits("", threshold=80.0)
+        assert result is None
+
+    def test_returns_none_when_no_deficits(self):
+        """Test returns None when all modules meet threshold."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+src/module.py          100      5    95%
+TOTAL                  100      5    95%
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is None
+
+    def test_uses_explicit_threshold(self):
+        """Test uses explicit threshold parameter."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+module.py              100     25    75%
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert result.threshold == 80.0
+
+    def test_includes_table_text_in_result(self):
+        """Test includes original table text in result."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+module.py              100     50    50%
+
+TOTAL                  100     50    50%
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert "Name" in result.table_text
+        assert "module.py" in result.table_text
+
+    def test_handles_multiple_deficits(self):
+        """Test handles multiple modules with coverage deficits."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+src/low1.py            100     70    30%
+src/low2.py            100     60    40%
+src/high.py            100     10    90%
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert len(result.deficits) == 2
+        paths = [d.path for d in result.deficits]
+        assert "src/low1.py" in paths
+        assert "src/low2.py" in paths
+        assert "src/high.py" not in paths
+
+    def test_preserves_coverage_values(self):
+        """Test preserves exact coverage values."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+module.py              100     32    67.8%
+"""
+        result = extract_coverage_deficits(output, threshold=70.0)
+        assert result is not None
+        assert result.deficits[0].coverage == 67.8
+
+    def test_handles_complex_pytest_output(self):
+        """Test handles complex pytest output with test results."""
+        output = """
+collected 50 items
+
+tests/test_module.py::test_function PASSED      [ 10%]
+tests/test_other.py::test_other FAILED          [ 20%]
+
+---------- coverage: platform darwin -----------
+Name                      Stmts   Miss  Cover
+----------------------------------------------
+ci_tools/module.py          150     75    50%
+ci_tools/other.py           100     10    90%
+----------------------------------------------
+TOTAL                       250     85    66%
+
+========== 49 passed, 1 failed in 5.23s ==========
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert len(result.deficits) == 1
+        assert result.deficits[0].path == "ci_tools/module.py"
+
+    def test_uses_custom_threshold(self):
+        """Test uses custom threshold value."""
+        output = """
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+file.py                100     15    85%
+"""
+        result_high = extract_coverage_deficits(output, threshold=90.0)
+        assert result_high is not None
+        assert len(result_high.deficits) == 1
+
+        result_low = extract_coverage_deficits(output, threshold=80.0)
+        assert result_low is None
+
+    def test_strips_table_text(self):
+        """Test strips whitespace from table text."""
+        output = """
+
+Name                 Stmts   Miss  Cover
+-----------------------------------------
+file.py                100     50    50%
+
+
+"""
+        result = extract_coverage_deficits(output, threshold=80.0)
+        assert result is not None
+        assert result.table_text.startswith("Name")
+        assert not result.table_text.startswith("\n")
+
+
+class TestCoverageCheckResult:
+    """Tests for CoverageCheckResult dataclass."""
+
+    def test_dataclass_initialization(self):
+        """Test CoverageCheckResult can be initialized."""
+        result = CoverageCheckResult(
+            table_text="table",
+            deficits=[CoverageDeficit("file.py", 60.0)],
+            threshold=80.0,
+        )
+        assert result.table_text == "table"
+        assert len(result.deficits) == 1
+        assert result.threshold == 80.0
+
+    def test_stores_multiple_deficits(self):
+        """Test can store multiple deficits."""
+        deficits = [
+            CoverageDeficit("file1.py", 50.0),
+            CoverageDeficit("file2.py", 60.0),
+            CoverageDeficit("file3.py", 70.0),
+        ]
+        result = CoverageCheckResult(
+            table_text="table", deficits=deficits, threshold=80.0
+        )
+        assert len(result.deficits) == 3
+        assert result.deficits[0].coverage == 50.0
+        assert result.deficits[2].coverage == 70.0
+
+    def test_empty_deficits_list(self):
+        """Test can have empty deficits list."""
+        result = CoverageCheckResult(table_text="table", deficits=[], threshold=80.0)
+        assert len(result.deficits) == 0
+
+
+class TestCoverageDeficit:
+    """Tests for CoverageDeficit dataclass."""
+
+    def test_dataclass_initialization(self):
+        """Test CoverageDeficit can be initialized."""
+        deficit = CoverageDeficit(path="module.py", coverage=55.5)
+        assert deficit.path == "module.py"
+        assert deficit.coverage == 55.5
+
+    def test_stores_integer_coverage(self):
+        """Test can store integer coverage values."""
+        deficit = CoverageDeficit(path="file.py", coverage=75.0)
+        assert deficit.coverage == 75.0
+
+    def test_stores_float_coverage(self):
+        """Test can store float coverage values."""
+        deficit = CoverageDeficit(path="file.py", coverage=72.35)
+        assert deficit.coverage == 72.35
diff --git a/tests/test_coverage_guard.py b/tests/test_coverage_guard.py
new file mode 100644
index 0000000..0e1ceca
--- /dev/null
+++ b/tests/test_coverage_guard.py
@@ -0,0 +1,738 @@
+from __future__ import annotations
+
+import sys
+from pathlib import Path
+from unittest.mock import Mock, patch
+
+import pytest
+from coverage import Coverage
+from coverage.exceptions import CoverageException, NoDataError, NoSource
+
+from ci_tools.scripts import coverage_guard
+
+
+class TestFindRepoRoot:
+    """Test repository root finding."""
+
+    def test_find_repo_root_with_git(self, tmp_path: Path) -> None:
+        """Test finding root with .git directory."""
+        repo = tmp_path / "project"
+        repo.mkdir()
+        (repo / ".git").mkdir()
+
+        nested = repo / "src" / "module"
+        nested.mkdir(parents=True)
+
+        with patch("pathlib.Path.cwd", return_value=nested):
+            root = coverage_guard.find_repo_root()
+            assert root == repo
+
+    def test_find_repo_root_no_git(self, tmp_path: Path) -> None:
+        """Test finding root without .git directory."""
+        with patch("pathlib.Path.cwd", return_value=tmp_path):
+            root = coverage_guard.find_repo_root()
+            assert root == tmp_path
+
+    def test_find_repo_root_at_filesystem_root(self) -> None:
+        """Test finding root at filesystem root."""
+        # Mock being at filesystem root
+        with patch("pathlib.Path.cwd", return_value=Path("/")):
+            root = coverage_guard.find_repo_root()
+            assert root == Path("/")
+
+
+class TestCoverageResult:
+    """Test CoverageResult dataclass."""
+
+    def test_coverage_result_percent_normal(self) -> None:
+        """Test coverage percentage calculation."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=100,
+            missing=20
+        )
+        assert result.percent == 80.0
+
+    def test_coverage_result_percent_zero_statements(self) -> None:
+        """Test coverage with zero statements."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=0,
+            missing=0
+        )
+        assert result.percent == 100.0
+
+    def test_coverage_result_percent_full_coverage(self) -> None:
+        """Test 100% coverage."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=50,
+            missing=0
+        )
+        assert result.percent == 100.0
+
+    def test_coverage_result_percent_no_coverage(self) -> None:
+        """Test 0% coverage."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=50,
+            missing=50
+        )
+        assert result.percent == 0.0
+
+    def test_coverage_result_frozen(self) -> None:
+        """Test that CoverageResult is frozen."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=100,
+            missing=20
+        )
+
+        with pytest.raises(Exception):  # FrozenInstanceError
+            result.statements = 50
+
+
+class TestParseArgs:
+    """Test argument parsing."""
+
+    def test_parse_args_defaults(self) -> None:
+        """Test default argument values."""
+        args = coverage_guard.parse_args([])
+        assert args.threshold == 80.0
+        assert args.data_file is None
+        assert args.include == []
+
+    def test_parse_args_custom_threshold(self) -> None:
+        """Test custom threshold argument."""
+        args = coverage_guard.parse_args(["--threshold", "90"])
+        assert args.threshold == 90.0
+
+    def test_parse_args_custom_data_file(self) -> None:
+        """Test custom data file argument."""
+        args = coverage_guard.parse_args(["--data-file", "/path/to/.coverage"])
+        assert args.data_file == "/path/to/.coverage"
+
+    def test_parse_args_single_include(self) -> None:
+        """Test single include path."""
+        args = coverage_guard.parse_args(["--include", "src"])
+        assert "src" in args.include
+
+    def test_parse_args_multiple_includes(self) -> None:
+        """Test multiple include paths."""
+        args = coverage_guard.parse_args([
+            "--include", "src",
+            "--include", "lib",
+            "--include", "tests"
+        ])
+        assert "src" in args.include
+        assert "lib" in args.include
+        assert "tests" in args.include
+
+    def test_parse_args_threshold_from_env(self, monkeypatch: pytest.MonkeyPatch) -> None:
+        """Test reading threshold from environment variable."""
+        monkeypatch.setenv("ZEUS_COVERAGE_THRESHOLD", "75")
+        args = coverage_guard.parse_args([])
+        assert args.threshold == 75.0
+
+    def test_parse_args_combined_options(self) -> None:
+        """Test combining multiple options."""
+        args = coverage_guard.parse_args([
+            "--threshold", "85",
+            "--data-file", ".coverage.test",
+            "--include", "src",
+            "--include", "lib"
+        ])
+        assert args.threshold == 85.0
+        assert args.data_file == ".coverage.test"
+        assert "src" in args.include
+        assert "lib" in args.include
+
+
+class TestResolveDataFile:
+    """Test coverage data file resolution."""
+
+    def test_resolve_data_file_explicit_path(self, tmp_path: Path) -> None:
+        """Test with explicit absolute path."""
+        explicit = tmp_path / ".coverage.custom"
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.resolve_data_file(str(explicit))
+            assert result == explicit
+
+    def test_resolve_data_file_relative_path(self, tmp_path: Path) -> None:
+        """Test with relative path."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.resolve_data_file(".coverage.test")
+            assert result == (tmp_path / ".coverage.test").resolve()
+
+    def test_resolve_data_file_none_defaults(self, tmp_path: Path) -> None:
+        """Test with None defaults to .coverage."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.resolve_data_file(None)
+            assert result == (tmp_path / ".coverage").resolve()
+
+    def test_resolve_data_file_from_env(self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
+        """Test reading from COVERAGE_FILE environment variable."""
+        monkeypatch.setenv("COVERAGE_FILE", ".coverage.env")
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.resolve_data_file(None)
+            assert result == (tmp_path / ".coverage.env").resolve()
+
+    def test_resolve_data_file_candidate_overrides_env(self, monkeypatch: pytest.MonkeyPatch, tmp_path: Path) -> None:
+        """Test that explicit candidate overrides environment."""
+        monkeypatch.setenv("COVERAGE_FILE", ".coverage.env")
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.resolve_data_file(".coverage.explicit")
+            assert result == (tmp_path / ".coverage.explicit").resolve()
+
+
+class TestNormalizePrefixes:
+    """Test prefix normalization."""
+
+    def test_normalize_prefixes_single(self, tmp_path: Path) -> None:
+        """Test normalizing single prefix."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.normalize_prefixes(["src"])
+            assert len(result) == 1
+            assert result[0] == (tmp_path / "src").resolve()
+
+    def test_normalize_prefixes_multiple(self, tmp_path: Path) -> None:
+        """Test normalizing multiple prefixes."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.normalize_prefixes(["src", "lib", "tests"])
+            assert len(result) == 3
+            assert result[0] == (tmp_path / "src").resolve()
+            assert result[1] == (tmp_path / "lib").resolve()
+            assert result[2] == (tmp_path / "tests").resolve()
+
+    def test_normalize_prefixes_empty(self, tmp_path: Path) -> None:
+        """Test normalizing empty list."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.normalize_prefixes([])
+            assert result == []
+
+    def test_normalize_prefixes_absolute_paths(self, tmp_path: Path) -> None:
+        """Test normalizing absolute paths."""
+        abs_path = tmp_path / "project" / "src"
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.normalize_prefixes([str(abs_path)])
+            assert len(result) == 1
+
+
+class TestShouldInclude:
+    """Test file inclusion logic."""
+
+    def test_should_include_no_prefixes(self, tmp_path: Path) -> None:
+        """Test that all files are included when no prefixes specified."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            file_path = tmp_path / "src" / "module.py"
+            assert coverage_guard.should_include(file_path, [])
+
+    def test_should_include_matching_prefix(self, tmp_path: Path) -> None:
+        """Test including file matching prefix."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            file_path = tmp_path / "src" / "module.py"
+            prefixes = [(tmp_path / "src").resolve()]
+            assert coverage_guard.should_include(file_path, prefixes)
+
+    def test_should_include_not_matching_prefix(self, tmp_path: Path) -> None:
+        """Test excluding file not matching prefix."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            file_path = tmp_path / "tests" / "test_module.py"
+            prefixes = [(tmp_path / "src").resolve()]
+            assert not coverage_guard.should_include(file_path, prefixes)
+
+    def test_should_include_outside_root(self, tmp_path: Path) -> None:
+        """Test excluding file outside root."""
+        root = tmp_path / "project"
+        outside = tmp_path / "other" / "file.py"
+
+        with patch.object(coverage_guard, "ROOT", root):
+            assert not coverage_guard.should_include(outside, [])
+
+    def test_should_include_exact_prefix_match(self, tmp_path: Path) -> None:
+        """Test including file that is exactly the prefix."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            file_path = tmp_path / "src"
+            prefixes = [(tmp_path / "src").resolve()]
+            assert coverage_guard.should_include(file_path, prefixes)
+
+    def test_should_include_multiple_prefixes(self, tmp_path: Path) -> None:
+        """Test including with multiple prefixes."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            file1 = tmp_path / "src" / "module.py"
+            file2 = tmp_path / "lib" / "util.py"
+            file3 = tmp_path / "other" / "file.py"
+
+            prefixes = [
+                (tmp_path / "src").resolve(),
+                (tmp_path / "lib").resolve()
+            ]
+
+            assert coverage_guard.should_include(file1, prefixes)
+            assert coverage_guard.should_include(file2, prefixes)
+            assert not coverage_guard.should_include(file3, prefixes)
+
+
+class TestCollectResults:
+    """Test collecting coverage results."""
+
+    def test_collect_results_no_data(self, tmp_path: Path) -> None:
+        """Test when no coverage data exists."""
+        mock_cov = Mock(spec=Coverage)
+        mock_cov.load.side_effect = NoDataError("No data")
+
+        with pytest.raises(SystemExit) as exc_info:
+            coverage_guard.collect_results(mock_cov, [])
+
+        assert "no data found" in str(exc_info.value)
+
+    def test_collect_results_empty_data(self, tmp_path: Path) -> None:
+        """Test with empty coverage data."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+        mock_data.measured_files.return_value = []
+
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        results = coverage_guard.collect_results(mock_cov, [])
+        assert results == []
+
+    def test_collect_results_single_file(self, tmp_path: Path) -> None:
+        """Test collecting results for single file."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file_path = tmp_path / "src" / "module.py"
+        mock_data.measured_files.return_value = [str(file_path)]
+
+        # Mock analysis2 to return (?, statements, ?, missing, ?)
+        mock_cov.analysis2.return_value = (
+            None,
+            [1, 2, 3, 4, 5],  # statements (line numbers)
+            None,
+            [3, 5],  # missing (line numbers)
+            None
+        )
+
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            results = coverage_guard.collect_results(mock_cov, [])
+
+        assert len(results) == 1
+        assert results[0].path == file_path.resolve()
+        assert results[0].statements == 5
+        assert results[0].missing == 2
+
+    def test_collect_results_multiple_files(self, tmp_path: Path) -> None:
+        """Test collecting results for multiple files."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "src" / "module1.py"
+        file2 = tmp_path / "src" / "module2.py"
+        mock_data.measured_files.return_value = [str(file1), str(file2)]
+
+        # Different coverage for each file
+        def analysis_side_effect(filename):
+            if "module1" in filename:
+                return (None, [1, 2, 3], None, [1], None)
+            else:
+                return (None, [1, 2, 3, 4], None, [2, 4], None)
+
+        mock_cov.analysis2.side_effect = analysis_side_effect
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            results = coverage_guard.collect_results(mock_cov, [])
+
+        assert len(results) == 2
+
+    def test_collect_results_with_prefixes(self, tmp_path: Path) -> None:
+        """Test collecting results filtered by prefixes."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "src" / "module.py"
+        file2 = tmp_path / "tests" / "test_module.py"
+        mock_data.measured_files.return_value = [str(file1), str(file2)]
+
+        mock_cov.analysis2.return_value = (None, [1, 2], None, [1], None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        prefixes = [(tmp_path / "src").resolve()]
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            results = coverage_guard.collect_results(mock_cov, prefixes)
+
+        # Only src/ file should be included
+        assert len(results) == 1
+        assert "src" in str(results[0].path)
+
+    def test_collect_results_skip_no_source(self, tmp_path: Path) -> None:
+        """Test that files with no source are skipped."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "exists.py"
+        file2 = tmp_path / "deleted.py"
+        mock_data.measured_files.return_value = [str(file1), str(file2)]
+
+        def analysis_side_effect(filename):
+            if "deleted" in filename:
+                raise NoSource("File not found")
+            return (None, [1, 2], None, [1], None)
+
+        mock_cov.analysis2.side_effect = analysis_side_effect
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            results = coverage_guard.collect_results(mock_cov, [])
+
+        # Only the existing file should be in results
+        assert len(results) == 1
+
+    def test_collect_results_sorted_output(self, tmp_path: Path) -> None:
+        """Test that results are sorted by filename."""
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        files = [
+            str(tmp_path / "zebra.py"),
+            str(tmp_path / "alpha.py"),
+            str(tmp_path / "beta.py")
+        ]
+        mock_data.measured_files.return_value = files
+
+        mock_cov.analysis2.return_value = (None, [1, 2], None, [1], None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            results = coverage_guard.collect_results(mock_cov, [])
+
+        # Files are sorted by measured_files
+        assert len(results) == 3
+
+
+class TestMainFunction:
+    """Test main function and CLI behavior."""
+
+    def test_main_data_file_not_found(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when coverage data file doesn't exist."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            result = coverage_guard.main(["--data-file", str(tmp_path / ".coverage.missing")])
+
+        assert result == 1
+        captured = capsys.readouterr()
+        assert "coverage data file not found" in captured.err
+
+    def test_main_all_files_pass(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when all files meet threshold."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")  # Create empty file
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "module.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # 90% coverage
+        mock_cov.analysis2.return_value = (None, list(range(10)), None, [9], None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        assert result == 0
+
+    def test_main_some_files_fail(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when some files fail threshold."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "low_coverage.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # 50% coverage
+        mock_cov.analysis2.return_value = (None, list(range(10)), None, list(range(5)), None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        assert result == 1
+        captured = capsys.readouterr()
+        assert "coverage_guard" in captured.err
+        assert "below threshold" in captured.err
+        assert "low_coverage.py" in captured.err
+        assert "50.00%" in captured.err
+
+    def test_main_coverage_exception(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when coverage raises exception."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_cov.load.side_effect = CoverageException("Error loading data")
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--data-file", str(data_file)])
+
+        assert result == 1
+        captured = capsys.readouterr()
+        assert "failed to load coverage data" in captured.err
+
+    def test_main_threshold_boundary(self, tmp_path: Path) -> None:
+        """Test coverage exactly at threshold."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "module.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # Exactly 80% coverage
+        mock_cov.analysis2.return_value = (None, list(range(10)), None, [8, 9], None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        # Should pass (80.0 >= 80.0)
+        assert result == 0
+
+    def test_main_zero_statements_file(self, tmp_path: Path) -> None:
+        """Test file with zero statements."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "empty.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # Zero statements
+        mock_cov.analysis2.return_value = (None, [], None, [], None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        # Should pass (empty files don't count against coverage)
+        assert result == 0
+
+    def test_main_with_include_filters(self, tmp_path: Path) -> None:
+        """Test main with include filters."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "src" / "module.py"
+        file2 = tmp_path / "tests" / "test.py"
+        mock_data.measured_files.return_value = [str(file1), str(file2)]
+
+        # Good coverage in src, bad in tests
+        def analysis_side_effect(filename):
+            if "src" in filename:
+                return (None, list(range(10)), None, [9], None)  # 90%
+            else:
+                return (None, list(range(10)), None, list(range(5)), None)  # 50%
+
+        mock_cov.analysis2.side_effect = analysis_side_effect
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                # Only check src
+                result = coverage_guard.main([
+                    "--threshold", "80",
+                    "--data-file", str(data_file),
+                    "--include", "src"
+                ])
+
+        # Should pass because we're only checking src
+        assert result == 0
+
+    def test_main_script_entry_point(self, tmp_path: Path) -> None:
+        """Test __main__ entry point."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+        mock_data.measured_files.return_value = []
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        # We can't easily test the __main__ entry point due to module-level imports
+        # Instead, test that main() can be called successfully
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--data-file", str(data_file)])
+                assert result == 0
+
+    def test_main_float_precision_edge_case(self, tmp_path: Path) -> None:
+        """Test handling of floating point precision issues."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "module.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # Coverage that's very close to threshold
+        # 79.99999% should fail against 80% threshold
+        mock_cov.analysis2.return_value = (None, list(range(100000)), None, list(range(20001)), None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        # The 1e-9 tolerance should allow near-misses to pass
+        # With 20001 missing out of 100000, we have 79.999% which should pass
+
+    def test_main_none_argv_uses_sys_argv(self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
+        """Test that argv=None uses sys.argv."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+        mock_data.measured_files.return_value = []
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        monkeypatch.setattr(sys, "argv", ["coverage_guard.py", "--data-file", str(data_file)])
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(None)
+                assert result == 0
+
+
+class TestEdgeCases:
+    """Test edge cases and unusual scenarios."""
+
+    def test_coverage_result_with_rounding(self) -> None:
+        """Test coverage percentage with rounding."""
+        result = coverage_guard.CoverageResult(
+            path=Path("test.py"),
+            statements=3,
+            missing=1
+        )
+        # 2/3 = 66.666...%
+        assert abs(result.percent - 66.666666) < 0.001
+
+    def test_should_include_nested_paths(self, tmp_path: Path) -> None:
+        """Test inclusion with deeply nested paths."""
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            deep_file = tmp_path / "src" / "pkg" / "subpkg" / "module.py"
+            prefixes = [(tmp_path / "src").resolve()]
+            assert coverage_guard.should_include(deep_file, prefixes)
+
+    def test_multiple_failures_reported(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test that all failures are reported."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "module1.py"
+        file2 = tmp_path / "module2.py"
+        file3 = tmp_path / "module3.py"
+        mock_data.measured_files.return_value = [str(file1), str(file2), str(file3)]
+
+        # All have low coverage
+        mock_cov.analysis2.return_value = (None, list(range(10)), None, list(range(5)), None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                result = coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        assert result == 1
+        captured = capsys.readouterr()
+        assert "module1.py" in captured.err
+        assert "module2.py" in captured.err
+        assert "module3.py" in captured.err
+
+    def test_relative_path_display(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test that paths are displayed relative to root."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "src" / "deeply" / "nested" / "module.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # Low coverage
+        mock_cov.analysis2.return_value = (None, list(range(10)), None, list(range(5)), None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                coverage_guard.main(["--threshold", "80", "--data-file", str(data_file)])
+
+        captured = capsys.readouterr()
+        # Should show relative path with forward slashes
+        assert "src/deeply/nested/module.py" in captured.err
+
+    def test_high_threshold(self, tmp_path: Path) -> None:
+        """Test with very high threshold requirement."""
+        data_file = tmp_path / ".coverage"
+        data_file.write_text("")
+
+        mock_cov = Mock(spec=Coverage)
+        mock_data = Mock()
+
+        file1 = tmp_path / "module.py"
+        mock_data.measured_files.return_value = [str(file1)]
+
+        # 95% coverage
+        mock_cov.analysis2.return_value = (None, list(range(100)), None, list(range(5)), None)
+        mock_cov.load.return_value = None
+        mock_cov.get_data.return_value = mock_data
+
+        with patch.object(coverage_guard, "ROOT", tmp_path):
+            with patch("ci_tools.scripts.coverage_guard.Coverage", return_value=mock_cov):
+                # Should pass 95% threshold
+                result1 = coverage_guard.main(["--threshold", "95", "--data-file", str(data_file)])
+                assert result1 == 0
+
+                # Should fail 99% threshold
+                result2 = coverage_guard.main(["--threshold", "99", "--data-file", str(data_file)])
+                assert result2 == 1
diff --git a/tests/test_data_guard.py b/tests/test_data_guard.py
new file mode 100644
index 0000000..2c34aa7
--- /dev/null
+++ b/tests/test_data_guard.py
@@ -0,0 +1,552 @@
+from __future__ import annotations
+
+import ast
+import json
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import data_guard
+
+
+def write_module(path: Path, source: str) -> None:
+    """Helper to write a Python module with proper formatting."""
+    path.write_text(textwrap.dedent(source).strip() + "\n", encoding="utf-8")
+
+
+def write_allowlist(path: Path, content: dict) -> None:
+    """Helper to write a JSON allowlist file."""
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(json.dumps(content, indent=2))
+
+
+class TestAllowlistLoading:
+    """Test allowlist loading functionality."""
+
+    def test_load_allowlist_missing_file(self, tmp_path: Path) -> None:
+        """Test loading allowlist when file doesn't exist."""
+        with patch.object(data_guard, "ALLOWLIST_PATH", tmp_path / "missing.json"):
+            result = data_guard._load_allowlist()
+            assert result == {"assignments": set(), "comparisons": set(), "dataframe": set()}
+
+    def test_load_allowlist_valid_file(self, tmp_path: Path) -> None:
+        """Test loading valid allowlist file."""
+        allowlist_path = tmp_path / "allowlist.json"
+        content = {
+            "assignments": ["threshold_value", "max_retries"],
+            "comparisons": ["timeout"],
+            "dataframe": ["pd.DataFrame"]
+        }
+        write_allowlist(allowlist_path, content)
+
+        with patch.object(data_guard, "ALLOWLIST_PATH", allowlist_path):
+            result = data_guard._load_allowlist()
+            assert result["assignments"] == {"threshold_value", "max_retries"}
+            assert result["comparisons"] == {"timeout"}
+            assert result["dataframe"] == {"pd.DataFrame"}
+
+    def test_load_allowlist_invalid_json(self, tmp_path: Path) -> None:
+        """Test loading allowlist with invalid JSON."""
+        allowlist_path = tmp_path / "invalid.json"
+        allowlist_path.write_text("{ invalid json")
+
+        with patch.object(data_guard, "ALLOWLIST_PATH", allowlist_path):
+            with pytest.raises(data_guard.DataGuardAllowlistError) as exc_info:
+                data_guard._load_allowlist()
+            assert "JSON parse error" in str(exc_info.value)
+
+    def test_load_allowlist_coerces_types(self, tmp_path: Path) -> None:
+        """Test that allowlist values are coerced to strings."""
+        allowlist_path = tmp_path / "allowlist.json"
+        content = {
+            "assignments": [123, "string_value", True],
+            "comparisons": []
+        }
+        write_allowlist(allowlist_path, content)
+
+        with patch.object(data_guard, "ALLOWLIST_PATH", allowlist_path):
+            result = data_guard._load_allowlist()
+            assert result["assignments"] == {"123", "string_value", "True"}
+
+    def test_allowlisted_checks_membership(self, tmp_path: Path) -> None:
+        """Test _allowlisted helper function."""
+        allowlist_path = tmp_path / "allowlist.json"
+        content = {"assignments": ["allowed_var"]}
+        write_allowlist(allowlist_path, content)
+
+        with patch.object(data_guard, "ALLOWLIST_PATH", allowlist_path):
+            with patch.object(data_guard, "ALLOWLIST", data_guard._load_allowlist()):
+                assert data_guard._allowlisted("allowed_var", "assignments")
+                assert not data_guard._allowlisted("other_var", "assignments")
+                assert not data_guard._allowlisted("allowed_var", "comparisons")
+
+
+class TestASTUtilities:
+    """Test AST utility functions."""
+
+    def test_parse_ast_valid_file(self, tmp_path: Path) -> None:
+        """Test parsing valid Python file."""
+        target = tmp_path / "valid.py"
+        write_module(target, "def foo(): pass")
+
+        tree = data_guard.parse_ast(target)
+        assert tree is not None
+        assert isinstance(tree, ast.Module)
+
+    def test_parse_ast_invalid_syntax(self, tmp_path: Path) -> None:
+        """Test parsing file with syntax errors."""
+        target = tmp_path / "invalid.py"
+        target.write_text("def foo(")
+
+        tree = data_guard.parse_ast(target)
+        assert tree is None
+
+    def test_extract_target_names_simple(self) -> None:
+        """Test extracting names from simple assignment."""
+        code = "x = 10"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        names = list(data_guard.extract_target_names(assign.targets[0]))
+        assert names == ["x"]
+
+    def test_extract_target_names_tuple(self) -> None:
+        """Test extracting names from tuple unpacking."""
+        code = "x, y = 1, 2"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        names = list(data_guard.extract_target_names(assign.targets[0]))
+        assert set(names) == {"x", "y"}
+
+    def test_extract_target_names_attribute(self) -> None:
+        """Test extracting names from attribute assignment."""
+        code = "obj.attr = 10"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        names = list(data_guard.extract_target_names(assign.targets[0]))
+        assert names == ["attr"]
+
+    def test_is_all_caps_identifier(self) -> None:
+        """Test constant identifier detection."""
+        assert data_guard._is_all_caps_identifier("MAX_RETRY")
+        assert data_guard._is_all_caps_identifier("TIMEOUT")
+        assert not data_guard._is_all_caps_identifier("max_retry")
+        assert not data_guard._is_all_caps_identifier("MaxRetry")
+        assert not data_guard._is_all_caps_identifier("")
+        assert not data_guard._is_all_caps_identifier("123")
+
+    def test_is_numeric_constant(self) -> None:
+        """Test numeric constant detection."""
+        code = "x = 42"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        assert data_guard.is_numeric_constant(assign.value)
+
+        code = "x = 'string'"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        assert not data_guard.is_numeric_constant(assign.value)
+
+    def test_literal_value_repr(self) -> None:
+        """Test literal value representation."""
+        code = "x = 42"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        assert data_guard.literal_value_repr(assign.value) == "42"
+
+        code = "x = 'string'"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        assert data_guard.literal_value_repr(assign.value) == "'string'"
+
+        assert "None" in data_guard.literal_value_repr(None)
+
+
+class TestAssignmentViolations:
+    """Test sensitive assignment detection."""
+
+    def test_should_flag_assignment_sensitive_name(self) -> None:
+        """Test flagging assignment with sensitive name."""
+        code = "threshold = 100"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        names = list(data_guard.extract_target_names(assign.targets[0]))
+
+        assert data_guard._should_flag_assignment(names, assign.value)
+
+    def test_should_flag_assignment_constant_ignored(self) -> None:
+        """Test that all-caps constants are not flagged."""
+        code = "MAX_THRESHOLD = 100"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+        names = list(data_guard.extract_target_names(assign.targets[0]))
+
+        assert not data_guard._should_flag_assignment(names, assign.value)
+
+    def test_should_flag_assignment_allowed_literals(self) -> None:
+        """Test that 0, 1, -1 are not flagged."""
+        for value in [0, 1, -1]:
+            code = f"threshold = {value}"
+            tree = ast.parse(code)
+            assign = tree.body[0]
+            names = list(data_guard.extract_target_names(assign.targets[0]))
+            assert not data_guard._should_flag_assignment(names, assign.value)
+
+    def test_contains_sensitive_token(self) -> None:
+        """Test sensitive token detection."""
+        assert data_guard._contains_sensitive_token(["threshold"])
+        assert data_guard._contains_sensitive_token(["max_value"])
+        assert data_guard._contains_sensitive_token(["retry_count"])
+        assert not data_guard._contains_sensitive_token(["regular_var"])
+
+    def test_assignment_violation_from_node_simple(self, tmp_path: Path) -> None:
+        """Test creating violation from simple assignment."""
+        code = "threshold = 100"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+
+        with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+            violation = data_guard._assignment_violation_from_node(tmp_path / "test.py", assign)
+            assert violation is not None
+            assert "literal assignment" in violation.message
+            assert "threshold" in violation.message
+
+    def test_assignment_violation_from_node_annotated(self, tmp_path: Path) -> None:
+        """Test creating violation from annotated assignment."""
+        code = "max_retries: int = 5"
+        tree = ast.parse(code)
+        assign = tree.body[0]
+
+        with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+            violation = data_guard._assignment_violation_from_node(tmp_path / "test.py", assign)
+            assert violation is not None
+            assert "annotated literal assignment" in violation.message
+            assert "max_retries" in violation.message
+
+    def test_collect_sensitive_assignments(self, tmp_path: Path) -> None:
+        """Test collecting all sensitive assignments."""
+        target = tmp_path / "src" / "test.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            threshold = 100
+            MAX_THRESHOLD = 200
+            timeout: int = 30
+            regular_var = 50
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    violations = data_guard.collect_sensitive_assignments()
+                    assert len(violations) >= 2  # threshold and timeout
+                    messages = [v.message for v in violations]
+                    assert any("threshold" in msg for msg in messages)
+                    assert any("timeout" in msg for msg in messages)
+
+
+class TestComparisonViolations:
+    """Test numeric comparison detection."""
+
+    def test_should_flag_comparison_sensitive_name(self) -> None:
+        """Test flagging comparison with sensitive name."""
+        with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+            assert data_guard._should_flag_comparison(["threshold"])
+            assert not data_guard._should_flag_comparison(["THRESHOLD"])
+            assert not data_guard._should_flag_comparison([])
+
+    def test_literal_comparators(self) -> None:
+        """Test extracting literal comparators."""
+        code = "if threshold > 100: pass"
+        tree = ast.parse(code)
+        if_node = tree.body[0]
+        compare = if_node.test
+
+        literals = data_guard._literal_comparators(compare)
+        assert len(literals) == 1
+        assert literals[0].value == 100
+
+    def test_literal_comparators_allowed_values(self) -> None:
+        """Test that 0, 1, -1 comparators are not flagged."""
+        for value in [0, 1, -1]:
+            code = f"if threshold > {value}: pass"
+            tree = ast.parse(code)
+            if_node = tree.body[0]
+            compare = if_node.test
+
+            literals = data_guard._literal_comparators(compare)
+            assert len(literals) == 0
+
+    def test_comparison_targets(self) -> None:
+        """Test extracting comparison targets."""
+        code = "if threshold > 100: pass"
+        tree = ast.parse(code)
+        if_node = tree.body[0]
+        compare = if_node.test
+
+        targets = data_guard._comparison_targets(compare)
+        assert targets == ["threshold"]
+
+    def test_format_comparison_message(self) -> None:
+        """Test formatting comparison violation message."""
+        code = "if threshold > 100: pass"
+        tree = ast.parse(code)
+        if_node = tree.body[0]
+        compare = if_node.test
+
+        literals = [compare.comparators[0]]
+        message = data_guard._format_comparison_message(literals, ["threshold"])
+        assert "comparison against literal" in message
+        assert "100" in message
+        assert "threshold" in message
+
+    def test_collect_numeric_comparisons(self, tmp_path: Path) -> None:
+        """Test collecting all numeric comparisons."""
+        target = tmp_path / "src" / "test.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            def check(threshold):
+                if threshold > 100:
+                    return True
+                if MAX_THRESHOLD < 200:
+                    return False
+                return threshold == 50
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    violations = data_guard.collect_numeric_comparisons()
+                    assert len(violations) >= 1
+                    assert any("threshold" in v.message for v in violations)
+
+
+class TestDataframeLiterals:
+    """Test DataFrame literal detection."""
+
+    def test_contains_literal_dataset_list(self) -> None:
+        """Test detecting literal datasets in lists."""
+        code = "[1, 2, 3]"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        assert data_guard.contains_literal_dataset(expr.value)
+
+    def test_contains_literal_dataset_dict(self) -> None:
+        """Test detecting literal datasets in dicts."""
+        code = "{'a': 1, 'b': 2}"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        assert data_guard.contains_literal_dataset(expr.value)
+
+    def test_contains_literal_dataset_nested(self) -> None:
+        """Test detecting literal datasets in nested structures."""
+        code = "[[1, 2], [3, 4]]"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        assert data_guard.contains_literal_dataset(expr.value)
+
+    def test_contains_literal_dataset_empty(self) -> None:
+        """Test that empty containers don't count as literal datasets."""
+        code = "[]"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        assert not data_guard.contains_literal_dataset(expr.value)
+
+    def test_get_call_qualname(self) -> None:
+        """Test extracting qualified names from calls."""
+        code = "pd.DataFrame()"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        call = expr.value
+
+        qualname = data_guard.get_call_qualname(call.func)
+        assert qualname == "pd.DataFrame"
+
+    def test_get_call_qualname_simple(self) -> None:
+        """Test extracting simple names from calls."""
+        code = "DataFrame()"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        call = expr.value
+
+        qualname = data_guard.get_call_qualname(call.func)
+        assert qualname == "DataFrame"
+
+    def test_call_contains_literal_arguments(self) -> None:
+        """Test detecting literal arguments in calls."""
+        code = "pd.DataFrame([1, 2, 3])"
+        tree = ast.parse(code)
+        expr = tree.body[0]
+        call = expr.value
+
+        assert data_guard._call_contains_literal_arguments(call)
+
+    def test_collect_dataframe_literals(self, tmp_path: Path) -> None:
+        """Test collecting DataFrame calls with literal data."""
+        target = tmp_path / "src" / "test.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            import pandas as pd
+
+            def create_df():
+                df1 = pd.DataFrame([1, 2, 3])
+                df2 = pd.DataFrame(data)
+                return df1, df2
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    violations = data_guard.collect_dataframe_literals()
+                    assert len(violations) >= 1
+                    assert any("pd.DataFrame" in v.message for v in violations)
+
+
+class TestIterators:
+    """Test file iteration utilities."""
+
+    def test_iter_python_files(self, tmp_path: Path) -> None:
+        """Test iterating Python files."""
+        src = tmp_path / "src"
+        src.mkdir()
+        (src / "test1.py").write_text("# test")
+        (src / "test2.py").write_text("# test")
+        (src / "data.txt").write_text("not python")
+
+        files = list(data_guard.iter_python_files([src]))
+        assert len(files) == 2
+        assert all(f.suffix == ".py" for f in files)
+
+    def test_iter_python_files_missing_directory(self, tmp_path: Path) -> None:
+        """Test iterating with non-existent directory."""
+        missing = tmp_path / "missing"
+        files = list(data_guard.iter_python_files([missing]))
+        assert len(files) == 0
+
+    def test_normalize_path(self, tmp_path: Path) -> None:
+        """Test path normalization."""
+        with patch.object(data_guard, "ROOT", tmp_path):
+            path = tmp_path / "src" / "module.py"
+            normalized = data_guard.normalize_path(path)
+            assert normalized == "src/module.py"
+
+
+class TestViolationDataclass:
+    """Test Violation dataclass."""
+
+    def test_violation_immutable(self, tmp_path: Path) -> None:
+        """Test that Violation is frozen."""
+        violation = data_guard.Violation(
+            path=tmp_path / "test.py",
+            lineno=42,
+            message="test violation"
+        )
+
+        with pytest.raises(Exception):  # FrozenInstanceError
+            violation.lineno = 43
+
+
+class TestMainFunction:
+    """Test main function and CLI behavior."""
+
+    def test_main_no_violations(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main function with no violations."""
+        target = tmp_path / "src" / "clean.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            def clean_function():
+                return 42
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    result = data_guard.main()
+                    assert result == 0
+
+    def test_main_with_violations(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main function with violations."""
+        target = tmp_path / "src" / "violations.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            threshold = 100
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    with pytest.raises(data_guard.DataGuardViolation) as exc_info:
+                        data_guard.main()
+                    assert "Data integrity violations detected" in str(exc_info.value)
+                    assert "threshold" in str(exc_info.value)
+
+    def test_main_script_entry_point(self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
+        """Test __main__ entry point."""
+        target = tmp_path / "src" / "test.py"
+        target.parent.mkdir(parents=True)
+        write_module(target, "x = 1")
+
+        # We can't easily test the __main__ entry point due to module-level code
+        # Instead, test that main() can be called successfully
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    result = data_guard.main()
+                    assert result == 0
+
+
+class TestCollectAllViolations:
+    """Test comprehensive violation collection."""
+
+    def test_collect_all_violations_comprehensive(self, tmp_path: Path) -> None:
+        """Test collecting all types of violations."""
+        target = tmp_path / "src" / "test.py"
+        target.parent.mkdir(parents=True)
+        write_module(
+            target,
+            """
+            import pandas as pd
+
+            # Assignment violation
+            threshold = 100
+
+            # Comparison violation
+            def check(timeout):
+                if timeout > 500:
+                    return True
+                return False
+
+            # DataFrame literal violation
+            def create_data():
+                return pd.DataFrame([1, 2, 3])
+            """
+        )
+
+        with patch.object(data_guard, "SCAN_DIRECTORIES", (tmp_path / "src",)):
+            with patch.object(data_guard, "ROOT", tmp_path):
+                with patch.object(data_guard, "ALLOWLIST", {"assignments": set(), "comparisons": set(), "dataframe": set()}):
+                    violations = data_guard.collect_all_violations()
+                    assert len(violations) >= 3
+
+                    # Check we got different types of violations
+                    messages = [v.message for v in violations]
+                    has_assignment = any("literal assignment" in msg for msg in messages)
+                    has_comparison = any("comparison" in msg for msg in messages)
+                    has_dataframe = any("DataFrame" in msg for msg in messages)
+
+                    assert has_assignment or has_comparison or has_dataframe
diff --git a/tests/test_dependency_guard.py b/tests/test_dependency_guard.py
new file mode 100644
index 0000000..e939037
--- /dev/null
+++ b/tests/test_dependency_guard.py
@@ -0,0 +1,564 @@
+from __future__ import annotations
+
+import ast
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import dependency_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = dependency_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_instantiations == 8
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = dependency_guard.parse_args(
+        ["--root", "custom", "--max-instantiations", "5", "--exclude", "tests"]
+    )
+    assert args.root == Path("custom")
+    assert args.max_instantiations == 5
+    assert args.exclude == [Path("tests")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(dependency_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(dependency_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert dependency_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert dependency_guard.is_excluded(path, exclusions) is False
+
+
+def test_callee_name_simple():
+    """Test extracting callee name from simple call."""
+    source = "Foo()"
+    tree = ast.parse(source)
+    call_node = tree.body[0].value
+
+    name = dependency_guard._callee_name(call_node)
+    assert name == "Foo"
+
+
+def test_callee_name_attribute():
+    """Test extracting callee name from attribute call."""
+    source = "module.Foo()"
+    tree = ast.parse(source)
+    call_node = tree.body[0].value
+
+    name = dependency_guard._callee_name(call_node)
+    assert name == "Foo"
+
+
+def test_callee_name_complex():
+    """Test extracting callee name from complex expression."""
+    source = "(foo if condition else bar)()"
+    tree = ast.parse(source)
+    call_node = tree.body[0].value
+
+    name = dependency_guard._callee_name(call_node)
+    assert name is None
+
+
+def test_is_constructor_name_valid():
+    """Test is_constructor_name with valid constructor names."""
+    assert dependency_guard._is_constructor_name("Foo") is True
+    assert dependency_guard._is_constructor_name("MyClass") is True
+    assert dependency_guard._is_constructor_name("HTTPClient") is True
+
+
+def test_is_constructor_name_invalid():
+    """Test is_constructor_name with invalid names."""
+    assert dependency_guard._is_constructor_name("foo") is False
+    assert dependency_guard._is_constructor_name("myFunc") is False
+    assert dependency_guard._is_constructor_name("") is False
+
+
+def test_is_constructor_name_skipped():
+    """Test is_constructor_name skips certain names."""
+    assert dependency_guard._is_constructor_name("Path") is False
+    assert dependency_guard._is_constructor_name("List") is False
+    assert dependency_guard._is_constructor_name("Dict") is False
+    assert dependency_guard._is_constructor_name("Optional") is False
+
+
+def test_count_instantiations_basic():
+    """Test counting instantiations in a basic method."""
+    source = textwrap.dedent(
+        """
+        def __init__(self):
+            self.foo = Foo()
+            self.bar = Bar()
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    assert count == 2
+    assert "Foo" in classes
+    assert "Bar" in classes
+
+
+def test_count_instantiations_ignores_lowercase():
+    """Test that lowercase function calls are not counted."""
+    source = textwrap.dedent(
+        """
+        def __init__(self):
+            self.x = int(1)
+            self.y = str("test")
+            self.z = Service()
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    assert count == 1
+    assert "Service" in classes
+
+
+def test_count_instantiations_ignores_skipped():
+    """Test that skipped constructor names are not counted."""
+    source = textwrap.dedent(
+        """
+        def __init__(self):
+            self.path = Path("/tmp")
+            self.items = List()
+            self.data = Dict()
+            self.service = Service()
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    assert count == 1
+    assert "Service" in classes
+
+
+def test_count_instantiations_no_instantiations():
+    """Test counting with no instantiations."""
+    source = textwrap.dedent(
+        """
+        def __init__(self, foo, bar):
+            self.foo = foo
+            self.bar = bar
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    assert count == 0
+    assert classes == []
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the instantiation limit."""
+    py_file = tmp_path / "simple.py"
+    write_module(
+        py_file,
+        """
+        class Simple:
+            def __init__(self):
+                self.service1 = Service1()
+                self.service2 = Service2()
+        """,
+    )
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=5)
+    assert len(violations) == 0
+
+
+def test_scan_file_exceeds_limit(tmp_path: Path):
+    """Test scanning a file that exceeds the instantiation limit."""
+    py_file = tmp_path / "complex.py"
+    instantiations = "\n".join(
+        [f"        self.service{i} = Service{i}()" for i in range(15)]
+    )
+    content = f"class Complex:\n    def __init__(self):\n{instantiations}"
+    py_file.write_text(content)
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=5)
+    assert len(violations) == 1
+    assert violations[0][1] == "Complex"
+    assert violations[0][3] == 15  # count
+
+
+def test_scan_file_post_init(tmp_path: Path):
+    """Test scanning __post_init__ method."""
+    py_file = tmp_path / "dataclass.py"
+    write_module(
+        py_file,
+        """
+        class MyDataclass:
+            def __post_init__(self):
+                self.service1 = Service1()
+                self.service2 = Service2()
+                self.service3 = Service3()
+                self.service4 = Service4()
+                self.service5 = Service5()
+                self.service6 = Service6()
+        """,
+    )
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=3)
+    assert len(violations) == 1
+    assert violations[0][1] == "MyDataclass"
+
+
+def test_scan_file_syntax_error(tmp_path: Path):
+    """Test scan_file with syntax error."""
+    py_file = tmp_path / "bad.py"
+    py_file.write_text("class Foo:\n    def __init__(self\n")
+
+    with pytest.raises(RuntimeError, match="failed to parse Python source"):
+        dependency_guard.scan_file(py_file, max_instantiations=5)
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "simple.py",
+        """
+        class Simple:
+            def __init__(self):
+                self.service1 = Service1()
+                self.service2 = Service2()
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(["--root", str(root), "--max-instantiations", "5"])
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    py_file = root / "complex.py"
+
+    instantiations = "\n".join(
+        [f"        self.service{i} = Service{i}()" for i in range(15)]
+    )
+    content = f"class Complex:\n    def __init__(self):\n{instantiations}"
+    py_file.write_text(content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(["--root", str(root), "--max-instantiations", "5"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "too many dependency instantiations" in captured.err
+    assert "Complex" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    many_deps = "class ManyDeps:\n    def __init__(self):\n" + "\n".join(
+        [f"        self.s{i} = Service{i}()" for i in range(15)]
+    )
+    (root / "included.py").write_text(many_deps)
+    (excluded / "excluded.py").write_text(many_deps)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(
+            ["--root", str(root), "--max-instantiations", "5", "--exclude", str(excluded)]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    many_deps = "class ManyDeps:\n    def __init__(self):\n" + "\n".join(
+        [f"        self.s{i} = Service{i}()" for i in range(15)]
+    )
+    (root / "zebra.py").write_text(many_deps)
+    (root / "alpha.py").write_text(many_deps)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(["--root", str(root), "--max-instantiations", "5"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [
+        line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line
+    ]
+    assert len(err_lines) >= 2
+
+
+def test_format_violation():
+    """Test formatting violation message."""
+    violation = dependency_guard._format_violation(
+        Path("/project/src/module.py"),
+        class_name="BigClass",
+        lineno=10,
+        count=15,
+        instantiated=["Service1", "Service2", "Service3", "Service4", "Service5", "Service6"],
+        limit=8,
+        repo_root=Path("/project"),
+    )
+
+    assert "module.py:10" in violation or "module.py" in violation
+    assert "BigClass" in violation
+    assert "15 dependencies" in violation
+    assert "limit 8" in violation
+    assert "Service1" in violation
+
+
+def test_format_violation_truncates_long_list():
+    """Test formatting violation with long list of instantiations."""
+    long_list = [f"Service{i}" for i in range(20)]
+    violation = dependency_guard._format_violation(
+        Path("/project/src/module.py"),
+        class_name="BigClass",
+        lineno=10,
+        count=20,
+        instantiated=long_list,
+        limit=8,
+        repo_root=Path("/project"),
+    )
+
+    assert "more)" in violation  # Should show truncation
+
+
+def test_collect_file_violations(tmp_path: Path):
+    """Test collecting file violations."""
+    py_file = tmp_path / "test.py"
+    instantiations = "\n".join(
+        [f"        self.service{i} = Service{i}()" for i in range(15)]
+    )
+    content = f"class BigClass:\n    def __init__(self):\n{instantiations}"
+    py_file.write_text(content)
+
+    violations = dependency_guard._collect_file_violations(
+        py_file, max_instantiations=5, repo_root=tmp_path
+    )
+
+    assert len(violations) == 1
+    assert "BigClass" in violations[0]
+
+
+def test_print_violation_report(capsys: pytest.CaptureFixture):
+    """Test printing violation report."""
+    violations = ["violation1", "violation2"]
+    dependency_guard._print_violation_report(violations, limit=8)
+
+    captured = capsys.readouterr()
+    assert "too many dependency instantiations" in captured.err
+    assert "dependency injection" in captured.err
+    assert "violation1" in captured.err
+    assert "violation2" in captured.err
+
+
+def test_main_traverse_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles traversal errors."""
+    missing = tmp_path / "missing"
+
+    result = dependency_guard.main(["--root", str(missing)])
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to traverse" in captured.err
+
+
+def test_main_scan_file_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles scan_file errors."""
+    root = tmp_path / "src"
+    root.mkdir()
+    (root / "bad.py").write_text("class Foo:\n    def __init__(self\n")
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(["--root", str(root)])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to parse" in captured.err
+
+
+def test_scan_file_no_classes(tmp_path: Path):
+    """Test scanning file with no classes."""
+    py_file = tmp_path / "no_classes.py"
+    write_module(
+        py_file,
+        """
+        def function():
+            pass
+        """,
+    )
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=5)
+    assert len(violations) == 0
+
+
+def test_scan_file_class_without_init(tmp_path: Path):
+    """Test scanning class without __init__."""
+    py_file = tmp_path / "no_init.py"
+    write_module(
+        py_file,
+        """
+        class NoInit:
+            def method(self):
+                pass
+        """,
+    )
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=5)
+    assert len(violations) == 0
+
+
+def test_count_instantiations_nested_calls():
+    """Test counting nested instantiation calls."""
+    source = textwrap.dedent(
+        """
+        def __init__(self):
+            self.foo = Foo(Bar(), Baz())
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    assert count == 3  # Foo, Bar, Baz
+    assert "Foo" in classes
+    assert "Bar" in classes
+    assert "Baz" in classes
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    many_deps = "class ManyDeps:\n    def __init__(self):\n" + "\n".join(
+        [f"        self.s{i} = Service{i}()" for i in range(15)]
+    )
+    (root / "module.py").write_text(many_deps)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = dependency_guard.main(["--root", str(root), "--max-instantiations", "5"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "module.py" in captured.err
+
+
+def test_scan_file_multiple_classes(tmp_path: Path):
+    """Test scanning file with multiple classes."""
+    py_file = tmp_path / "multi.py"
+    write_module(
+        py_file,
+        """
+        class Simple:
+            def __init__(self):
+                self.service = Service()
+
+        class Complex:
+            def __init__(self):
+                self.s1 = Service1()
+                self.s2 = Service2()
+                self.s3 = Service3()
+                self.s4 = Service4()
+                self.s5 = Service5()
+                self.s6 = Service6()
+        """,
+    )
+
+    violations = dependency_guard.scan_file(py_file, max_instantiations=3)
+    assert len(violations) == 1
+    assert violations[0][1] == "Complex"
+
+
+def test_iter_python_files_empty_directory(tmp_path: Path):
+    """Test iter_python_files with empty directory."""
+    files = list(dependency_guard.iter_python_files(tmp_path))
+    assert len(files) == 0
+
+
+def test_callee_name_subscript():
+    """Test callee name with subscript expression."""
+    source = "foo[0]()"
+    tree = ast.parse(source)
+    call_node = tree.body[0].value
+
+    name = dependency_guard._callee_name(call_node)
+    assert name is None
+
+
+def test_count_instantiations_in_nested_function():
+    """Test counting instantiations in nested function."""
+    source = textwrap.dedent(
+        """
+        def __init__(self):
+            def helper():
+                return Service()
+            self.service = helper()
+        """
+    ).strip()
+
+    tree = ast.parse(source)
+    func_node = tree.body[0]
+    count, classes = dependency_guard.count_instantiations(func_node)
+
+    # Should count Service even though it's in nested function
+    assert count == 1
+    assert "Service" in classes
diff --git a/tests/test_directory_depth_guard.py b/tests/test_directory_depth_guard.py
new file mode 100644
index 0000000..5416def
--- /dev/null
+++ b/tests/test_directory_depth_guard.py
@@ -0,0 +1,571 @@
+from __future__ import annotations
+
+import sys
+from pathlib import Path
+
+import pytest
+
+from ci_tools.scripts import directory_depth_guard
+
+
+class TestParseArgs:
+    """Test argument parsing."""
+
+    def test_parse_args_defaults(self) -> None:
+        """Test default argument values."""
+        args = directory_depth_guard.parse_args([])
+        assert args.root == Path("src")
+        assert args.max_depth == 5
+        assert args.exclude == []
+
+    def test_parse_args_custom_root(self) -> None:
+        """Test custom root argument."""
+        args = directory_depth_guard.parse_args(["--root", "/custom/path"])
+        assert args.root == Path("/custom/path")
+
+    def test_parse_args_custom_max_depth(self) -> None:
+        """Test custom max depth argument."""
+        args = directory_depth_guard.parse_args(["--max-depth", "3"])
+        assert args.max_depth == 3
+
+    def test_parse_args_single_exclude(self) -> None:
+        """Test single exclusion pattern."""
+        args = directory_depth_guard.parse_args(["--exclude", "node_modules"])
+        assert "node_modules" in args.exclude
+
+    def test_parse_args_multiple_excludes(self) -> None:
+        """Test multiple exclusion patterns."""
+        args = directory_depth_guard.parse_args([
+            "--exclude", "node_modules",
+            "--exclude", ".git",
+            "--exclude", "venv"
+        ])
+        assert "node_modules" in args.exclude
+        assert ".git" in args.exclude
+        assert "venv" in args.exclude
+
+    def test_parse_args_combined_options(self) -> None:
+        """Test combining multiple options."""
+        args = directory_depth_guard.parse_args([
+            "--root", "/my/path",
+            "--max-depth", "7",
+            "--exclude", "test",
+            "--exclude", "cache"
+        ])
+        assert args.root == Path("/my/path")
+        assert args.max_depth == 7
+        assert "test" in args.exclude
+        assert "cache" in args.exclude
+
+
+class TestCalculateDepth:
+    """Test depth calculation."""
+
+    def test_calculate_depth_immediate_child(self, tmp_path: Path) -> None:
+        """Test depth of immediate child directory."""
+        root = tmp_path
+        child = root / "child"
+
+        depth = directory_depth_guard.calculate_depth(child, root)
+        assert depth == 1
+
+    def test_calculate_depth_nested(self, tmp_path: Path) -> None:
+        """Test depth of nested directories."""
+        root = tmp_path
+        nested = root / "level1" / "level2" / "level3"
+
+        depth = directory_depth_guard.calculate_depth(nested, root)
+        assert depth == 3
+
+    def test_calculate_depth_root_itself(self, tmp_path: Path) -> None:
+        """Test depth of root directory."""
+        root = tmp_path
+        depth = directory_depth_guard.calculate_depth(root, root)
+        assert depth == 0
+
+    def test_calculate_depth_outside_root(self, tmp_path: Path) -> None:
+        """Test depth calculation for path outside root."""
+        root = tmp_path / "project"
+        outside = tmp_path / "other"
+
+        depth = directory_depth_guard.calculate_depth(outside, root)
+        assert depth == 0
+
+    def test_calculate_depth_deep_nesting(self, tmp_path: Path) -> None:
+        """Test depth of deeply nested structure."""
+        root = tmp_path
+        deep = root / "a" / "b" / "c" / "d" / "e" / "f" / "g" / "h" / "i" / "j"
+
+        depth = directory_depth_guard.calculate_depth(deep, root)
+        assert depth == 10
+
+
+class TestShouldExclude:
+    """Test exclusion logic."""
+
+    def test_should_exclude_matching_pattern(self, tmp_path: Path) -> None:
+        """Test excluding directory matching pattern."""
+        path = tmp_path / "__pycache__"
+        exclusions = ["__pycache__", "node_modules"]
+
+        assert directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_not_matching(self, tmp_path: Path) -> None:
+        """Test not excluding directory that doesn't match."""
+        path = tmp_path / "src"
+        exclusions = ["__pycache__", "node_modules"]
+
+        assert not directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_dot_prefix(self, tmp_path: Path) -> None:
+        """Test excluding directories starting with dot."""
+        path = tmp_path / ".git"
+        # The implementation only checks for dot prefix inside the loop
+        # So we need at least one exclusion for the check to run
+        exclusions = ["dummy"]
+
+        assert directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_dot_files_and_dirs(self, tmp_path: Path) -> None:
+        """Test various dot-prefixed items."""
+        # The implementation only checks for dot prefix inside the exclusions loop
+        # So we need at least one exclusion
+        exclusions = ["dummy"]
+        for name in [".git", ".vscode", ".idea", ".pytest_cache"]:
+            path = tmp_path / name
+            assert directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_substring_match(self, tmp_path: Path) -> None:
+        """Test exclusion by substring matching."""
+        path = tmp_path / "my_cache_dir"
+        exclusions = ["cache"]
+
+        assert directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_empty_exclusions(self, tmp_path: Path) -> None:
+        """Test with empty exclusions list."""
+        path = tmp_path / "regular_dir"
+        exclusions = []
+
+        assert not directory_depth_guard.should_exclude(path, exclusions)
+
+    def test_should_exclude_case_sensitive(self, tmp_path: Path) -> None:
+        """Test that exclusion is case-sensitive."""
+        path = tmp_path / "Cache"
+        exclusions = ["cache"]
+
+        # This depends on the implementation - currently it IS case-sensitive
+        assert not directory_depth_guard.should_exclude(path, exclusions)
+
+
+class TestScanDirectories:
+    """Test directory scanning."""
+
+    def test_scan_directories_no_violations(self, tmp_path: Path) -> None:
+        """Test scanning with no depth violations."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        level1 = root / "level1"
+        level2 = level1 / "level2"
+        level2.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 5, [])
+        assert len(violations) == 0
+
+    def test_scan_directories_single_violation(self, tmp_path: Path) -> None:
+        """Test scanning with single depth violation."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create a deep nested structure
+        deep = root / "l1" / "l2" / "l3" / "l4" / "l5" / "l6"
+        deep.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 3, [])
+        assert len(violations) > 0
+
+        # Check that deep directories are reported
+        depths = [v[1] for v in violations]
+
+        assert any(depth > 3 for depth in depths)
+
+    def test_scan_directories_multiple_violations(self, tmp_path: Path) -> None:
+        """Test scanning with multiple violations."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create multiple deep branches
+        branch1 = root / "a" / "b" / "c" / "d" / "e"
+        branch2 = root / "x" / "y" / "z" / "w"
+
+        branch1.mkdir(parents=True)
+        branch2.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 2, [])
+        assert len(violations) >= 2
+
+    def test_scan_directories_respect_exclusions(self, tmp_path: Path) -> None:
+        """Test that exclusions are respected."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create deep structure in excluded directory
+        excluded = root / "__pycache__" / "deep" / "nested" / "path"
+        excluded.mkdir(parents=True)
+
+        # Create shallow structure in non-excluded directory
+        normal = root / "module"
+        normal.mkdir()
+
+        violations = directory_depth_guard.scan_directories(root, 2, ["__pycache__"])
+        # Should not report violations in excluded directory
+        paths = [str(v[0]) for v in violations]
+        assert not any("__pycache__" in path for path in paths)
+
+    def test_scan_directories_exclude_dot_dirs(self, tmp_path: Path) -> None:
+        """Test that dot directories are excluded when any exclusion is present."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create deep structure in .git
+        git = root / ".git" / "objects" / "pack" / "deep"
+        git.mkdir(parents=True)
+
+        # The should_exclude function only checks for dot prefix inside the loop
+        # So we need to pass at least one exclusion pattern
+        violations = directory_depth_guard.scan_directories(root, 1, ["dummy"])
+        paths = [str(v[0]) for v in violations]
+        # .git should be excluded because its name starts with "." (when exclusions exist)
+        assert not any(".git" in path for path in paths)
+
+    def test_scan_directories_boundary_depth(self, tmp_path: Path) -> None:
+        """Test scanning at exactly max depth."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create structure exactly at max depth
+        exact = root / "l1" / "l2" / "l3"
+        exact.mkdir(parents=True)
+
+        # Just above max depth
+        over = root / "a" / "b" / "c" / "d"
+        over.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 3, [])
+
+        # Only the over-depth one should be reported
+        depths = [v[1] for v in violations]
+
+        assert all(depth > 3 for depth in depths)
+
+    def test_scan_directories_empty_root(self, tmp_path: Path) -> None:
+        """Test scanning empty root directory."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        violations = directory_depth_guard.scan_directories(root, 5, [])
+        assert len(violations) == 0
+
+    def test_scan_directories_only_files(self, tmp_path: Path) -> None:
+        """Test scanning directory with only files."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        (root / "file1.py").write_text("# code")
+        (root / "file2.py").write_text("# code")
+
+        violations = directory_depth_guard.scan_directories(root, 5, [])
+        assert len(violations) == 0
+
+    def test_scan_directories_permission_error(self, tmp_path: Path) -> None:
+        """Test handling permission errors gracefully."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        restricted = root / "restricted"
+        restricted.mkdir()
+
+        # Create a deep structure inside
+        deep = restricted / "deep" / "nested"
+        deep.mkdir(parents=True)
+
+        try:
+            # Remove read permissions
+            restricted.chmod(0o000)
+
+            # Should not crash, just skip the restricted directory
+            violations = directory_depth_guard.scan_directories(root, 1, [])
+            # May or may not have violations depending on when permission was checked
+            assert isinstance(violations, list)
+        finally:
+            # Restore permissions for cleanup
+            restricted.chmod(0o755)
+
+
+class TestMainFunction:
+    """Test main function and CLI behavior."""
+
+    def test_main_root_does_not_exist(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with non-existent root."""
+        missing = tmp_path / "missing"
+        result = directory_depth_guard.main(["--root", str(missing)])
+
+        assert result == 1
+        captured = capsys.readouterr()
+        assert "does not exist" in captured.err
+
+    def test_main_no_violations(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with no violations."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create shallow structure
+        level1 = root / "level1"
+        level1.mkdir()
+
+        result = directory_depth_guard.main(["--root", str(root), "--max-depth", "5"])
+        assert result == 0
+
+    def test_main_with_violations(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with violations."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create deep structure
+        deep = root / "a" / "b" / "c" / "d" / "e"
+        deep.mkdir(parents=True)
+
+        result = directory_depth_guard.main(["--root", str(root), "--max-depth", "2"])
+        assert result == 1
+
+        captured = capsys.readouterr()
+        assert "Directory nesting exceeds" in captured.err
+        assert "depth:" in captured.err
+
+    def test_main_violation_message_format(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test violation message formatting."""
+        root = tmp_path / "project" / "src"
+        root.mkdir(parents=True)
+
+        deep = root / "a" / "b" / "c" / "d"
+        deep.mkdir(parents=True)
+
+        result = directory_depth_guard.main(["--root", str(root), "--max-depth", "2"])
+        assert result == 1
+
+        captured = capsys.readouterr()
+        assert "2 levels" in captured.err
+        assert "Consider flattening" in captured.err
+
+    def test_main_sorted_by_depth(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test that violations are sorted by depth."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create structures with different depths
+        shallow = root / "a" / "b" / "c"
+        deep = root / "x" / "y" / "z" / "w" / "v"
+
+        shallow.mkdir(parents=True)
+        deep.mkdir(parents=True)
+
+        result = directory_depth_guard.main(["--root", str(root), "--max-depth", "1"])
+        assert result == 1
+
+        captured = capsys.readouterr()
+        # Deeper paths should appear first (reversed sort)
+        output_lines = captured.err.split("\n")
+        violation_lines = [line for line in output_lines if "depth:" in line]
+
+        # Extract depths
+        depths = []
+        for line in violation_lines:
+            if "depth:" in line:
+                depth_str = line.split("depth:")[1].strip().rstrip(")")
+                depths.append(int(depth_str))
+
+        # Check they're in descending order
+        assert depths == sorted(depths, reverse=True)
+
+    def test_main_with_exclusions(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with exclusion patterns."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create deep excluded structure
+        excluded = root / "cache" / "deep" / "nested" / "path"
+        excluded.mkdir(parents=True)
+
+        # Create shallow normal structure
+        normal = root / "code"
+        normal.mkdir()
+
+        result = directory_depth_guard.main([
+            "--root", str(root),
+            "--max-depth", "1",
+            "--exclude", "cache"
+        ])
+        assert result == 0
+
+    def test_main_default_exclusions_applied(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test that default exclusions are applied."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create deep __pycache__ structure
+        pycache = root / "__pycache__" / "deep" / "nested"
+        pycache.mkdir(parents=True)
+
+        result = directory_depth_guard.main(["--root", str(root), "--max-depth", "1"])
+        assert result == 0
+
+    def test_main_argv_none_uses_sys_argv(self, tmp_path: Path, monkeypatch: pytest.MonkeyPatch) -> None:
+        """Test that argv=None uses sys.argv."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        monkeypatch.setattr(sys, "argv", ["directory_depth_guard.py", "--root", str(root)])
+        result = directory_depth_guard.main(None)
+        assert result == 0
+
+    def test_main_script_entry_point(self, tmp_path: Path) -> None:
+        """Test __main__ entry point."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # We can't easily test the __main__ entry point due to module-level imports
+        # Instead, test that main() can be called successfully
+        result = directory_depth_guard.main(["--root", str(root)])
+        assert result == 0
+
+
+class TestEdgeCases:
+    """Test edge cases and unusual scenarios."""
+
+    def test_max_depth_zero(self, tmp_path: Path) -> None:
+        """Test with max depth of zero."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        child = root / "child"
+        child.mkdir()
+
+        violations = directory_depth_guard.scan_directories(root, 0, [])
+        assert len(violations) > 0
+
+    def test_max_depth_negative(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test with negative max depth."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Should technically work but flag everything
+        directory_depth_guard.main(["--root", str(root), "--max-depth", "-1"])
+        # Behavior depends on implementation
+
+    def test_very_deep_nesting(self, tmp_path: Path) -> None:
+        """Test with very deep nesting."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create extremely deep structure
+        path = root
+        for i in range(20):
+            path = path / f"level{i}"
+        path.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 5, [])
+        assert len(violations) > 0
+
+        # Check the deepest violation
+        max_depth = max(v[1] for v in violations)
+        assert max_depth >= 20
+
+    def test_unicode_directory_names(self, tmp_path: Path) -> None:
+        """Test with unicode directory names."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create structure with unicode names
+        unicode_dir = root / "测试" / "目录" / "深度"
+        unicode_dir.mkdir(parents=True)
+
+        violations = directory_depth_guard.scan_directories(root, 2, [])
+        assert len(violations) > 0
+
+    def test_relative_vs_absolute_paths(self, tmp_path: Path) -> None:
+        """Test that relative paths are handled correctly."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        deep = root / "a" / "b" / "c"
+        deep.mkdir(parents=True)
+
+        # Calculate depth with absolute path
+        depth_abs = directory_depth_guard.calculate_depth(deep.resolve(), root.resolve())
+
+        # Calculate depth with relative path if possible
+        depth_rel = directory_depth_guard.calculate_depth(deep, root)
+
+        assert depth_abs == depth_rel == 3
+
+    def test_symlink_handling(self, tmp_path: Path) -> None:
+        """Test handling of symlinked directories."""
+        try:
+            root = tmp_path / "src"
+            root.mkdir()
+
+            # Create a deep structure
+            deep = root / "deep" / "nested" / "path"
+            deep.mkdir(parents=True)
+
+            # Create a symlink to it
+            link = root / "link"
+            link.symlink_to(deep)
+
+            violations = directory_depth_guard.scan_directories(root, 2, [])
+            # Should find violations in the original path
+            assert len(violations) > 0
+        except OSError:
+            pytest.skip("Symlinks not supported on this platform")
+
+    def test_circular_symlinks_do_not_cause_infinite_loop(self, tmp_path: Path) -> None:
+        """Test that circular symlinks don't cause infinite loops."""
+        try:
+            root = tmp_path / "src"
+            root.mkdir()
+
+            dir1 = root / "dir1"
+            dir2 = root / "dir2"
+            dir1.mkdir()
+            dir2.mkdir()
+
+            # Create circular symlinks
+            (dir1 / "link_to_2").symlink_to(dir2)
+            (dir2 / "link_to_1").symlink_to(dir1)
+
+            # Should not hang or crash
+            violations = directory_depth_guard.scan_directories(root, 5, [])
+            assert isinstance(violations, list)
+        except OSError:
+            pytest.skip("Symlinks not supported on this platform")
+
+    def test_mixed_files_and_directories(self, tmp_path: Path) -> None:
+        """Test scanning with mixed files and directories."""
+        root = tmp_path / "src"
+        root.mkdir()
+
+        # Create mixed structure
+        dir1 = root / "dir1"
+        dir1.mkdir()
+        (dir1 / "file1.py").write_text("# code")
+
+        dir2 = dir1 / "dir2"
+        dir2.mkdir()
+        (dir2 / "file2.py").write_text("# code")
+
+        (root / "root_file.py").write_text("# code")
+
+        violations = directory_depth_guard.scan_directories(root, 1, [])
+        # dir2 should be reported (depth 2)
+        assert len(violations) > 0
diff --git a/tests/test_documentation_guard.py b/tests/test_documentation_guard.py
new file mode 100644
index 0000000..9e0bc63
--- /dev/null
+++ b/tests/test_documentation_guard.py
@@ -0,0 +1,605 @@
+from __future__ import annotations
+
+import sys
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import documentation_guard
+
+
+class TestParseArgs:
+    """Test argument parsing."""
+
+    def test_parse_args_defaults(self) -> None:
+        """Test default argument values."""
+        with patch.object(sys, "argv", ["documentation_guard.py"]):
+            args = documentation_guard.parse_args()
+            assert args.root == Path(".")
+
+    def test_parse_args_custom_root(self) -> None:
+        """Test custom root argument."""
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", "/custom/path"]):
+            args = documentation_guard.parse_args()
+            assert args.root == Path("/custom/path")
+
+
+class TestDiscoverSrcModules:
+    """Test src module discovery."""
+
+    def test_discover_src_modules_no_src_directory(self, tmp_path: Path) -> None:
+        """Test when src directory doesn't exist."""
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert result == []
+
+    def test_discover_src_modules_empty_src(self, tmp_path: Path) -> None:
+        """Test empty src directory."""
+        src = tmp_path / "src"
+        src.mkdir()
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert result == []
+
+    def test_discover_src_modules_with_python_files(self, tmp_path: Path) -> None:
+        """Test discovering modules with Python files."""
+        src = tmp_path / "src"
+        module1 = src / "module1"
+        module2 = src / "module2"
+
+        module1.mkdir(parents=True)
+        module2.mkdir(parents=True)
+
+        (module1 / "code.py").write_text("# code")
+        (module2 / "code.py").write_text("# code")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert len(result) == 2
+        assert "src/module1/README.md" in result
+        assert "src/module2/README.md" in result
+
+    def test_discover_src_modules_skip_underscore(self, tmp_path: Path) -> None:
+        """Test skipping directories starting with underscore."""
+        src = tmp_path / "src"
+        module = src / "module"
+        pycache = src / "__pycache__"
+
+        module.mkdir(parents=True)
+        pycache.mkdir(parents=True)
+
+        (module / "code.py").write_text("# code")
+        (pycache / "cached.pyc").write_text("# cached")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert len(result) == 1
+        assert "src/module/README.md" in result
+        assert "__pycache__" not in str(result)
+
+    def test_discover_src_modules_skip_git(self, tmp_path: Path) -> None:
+        """Test skipping .git directory."""
+        src = tmp_path / "src"
+        module = src / "module"
+        git = src / ".git"
+
+        module.mkdir(parents=True)
+        git.mkdir(parents=True)
+
+        (module / "code.py").write_text("# code")
+        (git / "config").write_text("# git")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert len(result) == 1
+        assert ".git" not in str(result)
+
+    def test_discover_src_modules_ignore_files(self, tmp_path: Path) -> None:
+        """Test that direct files in src are ignored."""
+        src = tmp_path / "src"
+        src.mkdir()
+        (src / "script.py").write_text("# script")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert result == []
+
+    def test_discover_src_modules_no_python_files(self, tmp_path: Path) -> None:
+        """Test that modules without Python files are not required."""
+        src = tmp_path / "src"
+        module = src / "module"
+        module.mkdir(parents=True)
+        (module / "data.txt").write_text("data")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert result == []
+
+
+class TestDiscoverArchitectureDocs:
+    """Test architecture documentation discovery."""
+
+    def test_discover_architecture_docs_no_directory(self, tmp_path: Path) -> None:
+        """Test when docs/architecture doesn't exist."""
+        result = documentation_guard.discover_architecture_docs(tmp_path)
+        assert result == []
+
+    def test_discover_architecture_docs_empty(self, tmp_path: Path) -> None:
+        """Test empty architecture directory."""
+        arch_dir = tmp_path / "docs" / "architecture"
+        arch_dir.mkdir(parents=True)
+
+        result = documentation_guard.discover_architecture_docs(tmp_path)
+        assert result == []
+
+    def test_discover_architecture_docs_with_markdown(self, tmp_path: Path) -> None:
+        """Test discovering architecture docs with markdown files."""
+        arch_dir = tmp_path / "docs" / "architecture"
+        arch_dir.mkdir(parents=True)
+        (arch_dir / "system.md").write_text("# System")
+        (arch_dir / "database.md").write_text("# Database")
+
+        result = documentation_guard.discover_architecture_docs(tmp_path)
+        assert result == ["docs/architecture/README.md"]
+
+    def test_discover_architecture_docs_only_non_markdown(self, tmp_path: Path) -> None:
+        """Test architecture directory with only non-markdown files."""
+        arch_dir = tmp_path / "docs" / "architecture"
+        arch_dir.mkdir(parents=True)
+        (arch_dir / "diagram.png").write_text("image")
+
+        result = documentation_guard.discover_architecture_docs(tmp_path)
+        assert result == []
+
+
+class TestDiscoverDomainDocs:
+    """Test domain documentation discovery."""
+
+    def test_discover_domain_docs_no_directory(self, tmp_path: Path) -> None:
+        """Test when docs/domains doesn't exist."""
+        result = documentation_guard.discover_domain_docs(tmp_path)
+        assert result == []
+
+    def test_discover_domain_docs_empty(self, tmp_path: Path) -> None:
+        """Test empty domains directory."""
+        domains_dir = tmp_path / "docs" / "domains"
+        domains_dir.mkdir(parents=True)
+
+        result = documentation_guard.discover_domain_docs(tmp_path)
+        assert result == []
+
+    def test_discover_domain_docs_with_subdirs(self, tmp_path: Path) -> None:
+        """Test discovering domain docs with subdirectories."""
+        domains_dir = tmp_path / "docs" / "domains"
+        domain1 = domains_dir / "trading"
+        domain2 = domains_dir / "risk"
+
+        domain1.mkdir(parents=True)
+        domain2.mkdir(parents=True)
+
+        result = documentation_guard.discover_domain_docs(tmp_path)
+        assert len(result) == 2
+        assert "docs/domains/trading/README.md" in result
+        assert "docs/domains/risk/README.md" in result
+
+    def test_discover_domain_docs_skip_underscore(self, tmp_path: Path) -> None:
+        """Test skipping underscore directories."""
+        domains_dir = tmp_path / "docs" / "domains"
+        domain = domains_dir / "domain"
+        internal = domains_dir / "_internal"
+
+        domain.mkdir(parents=True)
+        internal.mkdir(parents=True)
+
+        result = documentation_guard.discover_domain_docs(tmp_path)
+        assert len(result) == 1
+        assert "_internal" not in str(result)
+
+    def test_discover_domain_docs_ignore_files(self, tmp_path: Path) -> None:
+        """Test that direct files in domains are ignored."""
+        domains_dir = tmp_path / "docs" / "domains"
+        domains_dir.mkdir(parents=True)
+        (domains_dir / "overview.md").write_text("# Overview")
+
+        result = documentation_guard.discover_domain_docs(tmp_path)
+        assert result == []
+
+
+class TestDiscoverOperationsDocs:
+    """Test operations documentation discovery."""
+
+    def test_discover_operations_docs_no_directory(self, tmp_path: Path) -> None:
+        """Test when docs/operations doesn't exist."""
+        result = documentation_guard.discover_operations_docs(tmp_path)
+        assert result == []
+
+    def test_discover_operations_docs_exists(self, tmp_path: Path) -> None:
+        """Test when docs/operations exists."""
+        ops_dir = tmp_path / "docs" / "operations"
+        ops_dir.mkdir(parents=True)
+
+        result = documentation_guard.discover_operations_docs(tmp_path)
+        assert result == ["docs/operations/README.md"]
+
+
+class TestDiscoverReferenceDocs:
+    """Test reference documentation discovery."""
+
+    def test_discover_reference_docs_no_directory(self, tmp_path: Path) -> None:
+        """Test when docs/reference doesn't exist."""
+        result = documentation_guard.discover_reference_docs(tmp_path)
+        assert result == []
+
+    def test_discover_reference_docs_empty(self, tmp_path: Path) -> None:
+        """Test empty reference directory."""
+        ref_dir = tmp_path / "docs" / "reference"
+        ref_dir.mkdir(parents=True)
+
+        result = documentation_guard.discover_reference_docs(tmp_path)
+        assert result == []
+
+    def test_discover_reference_docs_with_subdirs(self, tmp_path: Path) -> None:
+        """Test discovering reference docs with subdirectories."""
+        ref_dir = tmp_path / "docs" / "reference"
+        api = ref_dir / "api"
+        cli = ref_dir / "cli"
+
+        api.mkdir(parents=True)
+        cli.mkdir(parents=True)
+
+        result = documentation_guard.discover_reference_docs(tmp_path)
+        assert len(result) == 2
+        assert "docs/reference/api/README.md" in result
+        assert "docs/reference/cli/README.md" in result
+
+    def test_discover_reference_docs_skip_underscore(self, tmp_path: Path) -> None:
+        """Test skipping underscore directories."""
+        ref_dir = tmp_path / "docs" / "reference"
+        api = ref_dir / "api"
+        private = ref_dir / "_private"
+
+        api.mkdir(parents=True)
+        private.mkdir(parents=True)
+
+        result = documentation_guard.discover_reference_docs(tmp_path)
+        assert len(result) == 1
+        assert "_private" not in str(result)
+
+
+class TestGetBaseRequirements:
+    """Test base documentation requirements."""
+
+    def test_get_base_requirements_minimal(self, tmp_path: Path) -> None:
+        """Test minimal base requirements."""
+        result = documentation_guard.get_base_requirements(tmp_path)
+        assert "README.md" in result
+        assert "CLAUDE.md" not in result
+
+    def test_get_base_requirements_with_claude_md(self, tmp_path: Path) -> None:
+        """Test when CLAUDE.md exists."""
+        (tmp_path / "CLAUDE.md").write_text("# Claude")
+
+        result = documentation_guard.get_base_requirements(tmp_path)
+        assert "README.md" in result
+        assert "CLAUDE.md" in result
+
+    def test_get_base_requirements_with_docs_dir(self, tmp_path: Path) -> None:
+        """Test when docs directory exists."""
+        docs_dir = tmp_path / "docs"
+        docs_dir.mkdir()
+
+        result = documentation_guard.get_base_requirements(tmp_path)
+        assert "README.md" in result
+        assert "docs/README.md" in result
+
+
+class TestDiscoverAllRequirements:
+    """Test comprehensive requirements discovery."""
+
+    def test_discover_all_requirements_minimal(self, tmp_path: Path) -> None:
+        """Test minimal repository structure."""
+        required, info = documentation_guard.discover_all_requirements(tmp_path)
+
+        assert "README.md" in required
+        assert "base" in info
+        assert "modules" in info
+        assert "architecture" in info
+        assert "domains" in info
+        assert "operations" in info
+        assert "reference" in info
+
+    def test_discover_all_requirements_comprehensive(self, tmp_path: Path) -> None:
+        """Test comprehensive repository structure."""
+        # Create various directories
+        (tmp_path / "CLAUDE.md").write_text("# Claude")
+        (tmp_path / "docs").mkdir()
+
+        src = tmp_path / "src" / "module1"
+        src.mkdir(parents=True)
+        (src / "code.py").write_text("# code")
+
+        arch = tmp_path / "docs" / "architecture"
+        arch.mkdir(parents=True)
+        (arch / "system.md").write_text("# System")
+
+        domains = tmp_path / "docs" / "domains" / "trading"
+        domains.mkdir(parents=True)
+
+        ops = tmp_path / "docs" / "operations"
+        ops.mkdir(parents=True)
+
+        ref = tmp_path / "docs" / "reference" / "api"
+        ref.mkdir(parents=True)
+
+        required, info = documentation_guard.discover_all_requirements(tmp_path)
+
+        assert "README.md" in required
+        assert "CLAUDE.md" in required
+        assert "docs/README.md" in required
+        assert "src/module1/README.md" in required
+        assert "docs/architecture/README.md" in required
+        assert "docs/domains/trading/README.md" in required
+        assert "docs/operations/README.md" in required
+        assert "docs/reference/api/README.md" in required
+
+    def test_discover_all_requirements_info_structure(self, tmp_path: Path) -> None:
+        """Test that discovery info has correct structure."""
+        required, info = documentation_guard.discover_all_requirements(tmp_path)
+
+        assert isinstance(info["base"], list)
+        assert isinstance(info["modules"], list)
+        assert isinstance(info["architecture"], list)
+        assert isinstance(info["domains"], list)
+        assert isinstance(info["operations"], list)
+        assert isinstance(info["reference"], list)
+
+
+class TestCheckRequiredDocs:
+    """Test checking for missing documentation."""
+
+    def test_check_required_docs_all_present(self, tmp_path: Path) -> None:
+        """Test when all required docs are present."""
+        (tmp_path / "README.md").write_text("# README")
+        (tmp_path / "CLAUDE.md").write_text("# CLAUDE")
+
+        required = ["README.md", "CLAUDE.md"]
+        missing = documentation_guard.check_required_docs(tmp_path, required)
+        assert missing == []
+
+    def test_check_required_docs_some_missing(self, tmp_path: Path) -> None:
+        """Test when some required docs are missing."""
+        (tmp_path / "README.md").write_text("# README")
+
+        required = ["README.md", "CLAUDE.md", "docs/README.md"]
+        missing = documentation_guard.check_required_docs(tmp_path, required)
+        assert len(missing) == 2
+        assert "CLAUDE.md" in missing
+        assert "docs/README.md" in missing
+
+    def test_check_required_docs_all_missing(self, tmp_path: Path) -> None:
+        """Test when all required docs are missing."""
+        required = ["README.md", "CLAUDE.md"]
+        missing = documentation_guard.check_required_docs(tmp_path, required)
+        assert len(missing) == 2
+        assert set(missing) == set(required)
+
+
+class TestGroupMissingDocs:
+    """Test grouping missing documentation."""
+
+    def test_group_missing_docs_by_category(self) -> None:
+        """Test grouping missing docs by category."""
+        missing = [
+            "README.md",
+            "src/module1/README.md",
+            "docs/architecture/README.md"
+        ]
+        discovery_info = {
+            "base": ["README.md"],
+            "modules": ["src/module1/README.md"],
+            "architecture": ["docs/architecture/README.md"],
+            "domains": [],
+            "operations": [],
+            "reference": []
+        }
+
+        grouped = documentation_guard._group_missing_docs(missing, discovery_info)
+
+        assert "README.md" in grouped["Base"]
+        assert "src/module1/README.md" in grouped["Modules"]
+        assert "docs/architecture/README.md" in grouped["Architecture"]
+
+    def test_group_missing_docs_empty(self) -> None:
+        """Test grouping with no missing docs."""
+        missing = []
+        discovery_info = {
+            "base": [],
+            "modules": [],
+            "architecture": [],
+            "domains": [],
+            "operations": [],
+            "reference": []
+        }
+
+        grouped = documentation_guard._group_missing_docs(missing, discovery_info)
+
+        assert all(len(docs) == 0 for docs in grouped.values())
+
+
+class TestPrintFunctions:
+    """Test output printing functions."""
+
+    def test_print_failure_report(self, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test failure report printing."""
+        grouped = {
+            "Base": ["README.md"],
+            "Modules": ["src/module1/README.md"],
+            "Architecture": [],
+            "Domains": [],
+            "Operations": [],
+            "Reference": []
+        }
+
+        documentation_guard._print_failure_report(grouped)
+        captured = capsys.readouterr()
+
+        assert "Documentation Guard: FAILED" in captured.err
+        assert "README.md" in captured.err
+        assert "src/module1/README.md" in captured.err
+
+    def test_print_success(self, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test success message printing."""
+        documentation_guard._print_success(5)
+        captured = capsys.readouterr()
+
+        assert "documentation_guard" in captured.err
+        assert "All required documentation present" in captured.err
+        assert "5 docs verified" in captured.err
+
+
+class TestMainFunction:
+    """Test main function and CLI behavior."""
+
+    def test_main_root_does_not_exist(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with non-existent root."""
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", str(tmp_path / "missing")]):
+            result = documentation_guard.main()
+            assert result == 1
+
+            captured = capsys.readouterr()
+            assert "does not exist" in captured.err
+
+    def test_main_all_docs_present(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when all required docs are present."""
+        (tmp_path / "README.md").write_text("# README")
+
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", str(tmp_path)]):
+            result = documentation_guard.main()
+            assert result == 0
+
+            captured = capsys.readouterr()
+            assert "All required documentation present" in captured.err
+
+    def test_main_missing_docs(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main when docs are missing."""
+        src = tmp_path / "src" / "module1"
+        src.mkdir(parents=True)
+        (src / "code.py").write_text("# code")
+
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", str(tmp_path)]):
+            result = documentation_guard.main()
+            assert result == 1
+
+            captured = capsys.readouterr()
+            assert "Documentation Guard: FAILED" in captured.err
+            assert "README.md" in captured.err
+            assert "src/module1/README.md" in captured.err
+
+    def test_main_complex_structure(self, tmp_path: Path, capsys: pytest.CaptureFixture[str]) -> None:
+        """Test main with complex directory structure."""
+        # Create required base docs
+        (tmp_path / "README.md").write_text("# README")
+        (tmp_path / "CLAUDE.md").write_text("# Claude")
+        (tmp_path / "docs").mkdir()
+        (tmp_path / "docs" / "README.md").write_text("# Docs")
+
+        # Create src modules
+        module1 = tmp_path / "src" / "module1"
+        module2 = tmp_path / "src" / "module2"
+        module1.mkdir(parents=True)
+        module2.mkdir(parents=True)
+        (module1 / "code.py").write_text("# code")
+        (module2 / "code.py").write_text("# code")
+        (module1 / "README.md").write_text("# Module 1")
+        (module2 / "README.md").write_text("# Module 2")
+
+        # Create architecture docs
+        arch = tmp_path / "docs" / "architecture"
+        arch.mkdir(parents=True)
+        (arch / "system.md").write_text("# System")
+        (arch / "README.md").write_text("# Architecture")
+
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", str(tmp_path)]):
+            result = documentation_guard.main()
+            assert result == 0
+
+    def test_main_script_entry_point(self, tmp_path: Path) -> None:
+        """Test __main__ entry point."""
+        (tmp_path / "README.md").write_text("# README")
+
+        # We can't easily test the __main__ entry point due to module-level imports
+        # Instead, test that main() can be called successfully
+        with patch.object(sys, "argv", ["documentation_guard.py", "--root", str(tmp_path)]):
+            result = documentation_guard.main()
+            assert result == 0
+
+
+class TestCategoryKeys:
+    """Test CATEGORY_KEYS constant."""
+
+    def test_category_keys_structure(self) -> None:
+        """Test that CATEGORY_KEYS has expected structure."""
+        assert len(documentation_guard.CATEGORY_KEYS) == 6
+
+        labels = [label for label, _ in documentation_guard.CATEGORY_KEYS]
+        keys = [key for _, key in documentation_guard.CATEGORY_KEYS]
+
+        assert "Base" in labels
+        assert "Modules" in labels
+        assert "Architecture" in labels
+        assert "Domains" in labels
+        assert "Operations" in labels
+        assert "Reference" in labels
+
+        assert "base" in keys
+        assert "modules" in keys
+        assert "architecture" in keys
+        assert "domains" in keys
+        assert "operations" in keys
+        assert "reference" in keys
+
+
+class TestEdgeCases:
+    """Test edge cases and unusual scenarios."""
+
+    def test_nested_modules_in_src(self, tmp_path: Path) -> None:
+        """Test deeply nested modules in src."""
+        deep_module = tmp_path / "src" / "package" / "subpackage" / "module"
+        deep_module.mkdir(parents=True)
+        (deep_module / "code.py").write_text("# code")
+
+        # Only top-level should be detected
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert "src/package/README.md" in result
+        assert len(result) == 1
+
+    def test_multiple_file_types_in_module(self, tmp_path: Path) -> None:
+        """Test module with various file types."""
+        module = tmp_path / "src" / "module"
+        module.mkdir(parents=True)
+        (module / "code.py").write_text("# python")
+        (module / "data.json").write_text("{}")
+        (module / "config.yaml").write_text("key: value")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert "src/module/README.md" in result
+
+    def test_empty_python_file(self, tmp_path: Path) -> None:
+        """Test module with empty Python file."""
+        module = tmp_path / "src" / "module"
+        module.mkdir(parents=True)
+        (module / "empty.py").write_text("")
+
+        result = documentation_guard.discover_src_modules(tmp_path)
+        assert "src/module/README.md" in result
+
+    def test_symlinks_if_supported(self, tmp_path: Path) -> None:
+        """Test handling of symlinked directories."""
+        try:
+            module = tmp_path / "src" / "module"
+            module.mkdir(parents=True)
+            (module / "code.py").write_text("# code")
+
+            link = tmp_path / "src" / "link"
+            link.symlink_to(module)
+
+            result = documentation_guard.discover_src_modules(tmp_path)
+            # Should find both the original and the link
+            assert "src/module/README.md" in result
+        except OSError:
+            # Skip if symlinks not supported
+            pytest.skip("Symlinks not supported on this platform")
diff --git a/tests/test_environment.py b/tests/test_environment.py
new file mode 100644
index 0000000..dc0eb2f
--- /dev/null
+++ b/tests/test_environment.py
@@ -0,0 +1,209 @@
+"""Unit tests for ci_tools.ci_runtime.environment module."""
+
+from __future__ import annotations
+
+import os
+from pathlib import Path
+
+
+from ci_tools.ci_runtime.environment import load_env_file, load_env_settings
+
+
+class TestLoadEnvFile:
+    """Tests for load_env_file function."""
+
+    def test_load_simple_env_file(self, tmp_path):
+        """Test loading a simple KEY=VALUE env file."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("FOO=bar\nBAZ=qux\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {"FOO": "bar", "BAZ": "qux"}
+
+    def test_load_env_file_with_spaces(self, tmp_path):
+        """Test loading env file with spaces around equals."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("KEY1 = value1\nKEY2= value2\nKEY3 =value3\n")
+
+        result = load_env_file(str(env_file))
+        assert result["KEY1"] == "value1"
+        assert result["KEY2"] == "value2"
+        assert result["KEY3"] == "value3"
+
+    def test_load_env_file_with_comments(self, tmp_path):
+        """Test loading env file with comment lines."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("# This is a comment\nKEY=value\n# Another comment\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {"KEY": "value"}
+
+    def test_load_env_file_with_empty_lines(self, tmp_path):
+        """Test loading env file with empty lines."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("KEY1=value1\n\nKEY2=value2\n\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {"KEY1": "value1", "KEY2": "value2"}
+
+    def test_load_env_file_with_values_containing_equals(self, tmp_path):
+        """Test loading env file where values contain equals signs."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("CONNECTION_STRING=server=localhost;port=5432\n")
+
+        result = load_env_file(str(env_file))
+        assert result["CONNECTION_STRING"] == "server=localhost;port=5432"
+
+    def test_load_env_file_invalid_lines_skipped(self, tmp_path):
+        """Test that lines without equals are skipped."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("KEY1=value1\nINVALIDLINE\nKEY2=value2\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {"KEY1": "value1", "KEY2": "value2"}
+
+    def test_load_env_file_nonexistent_returns_empty(self, tmp_path):
+        """Test loading nonexistent file returns empty dict."""
+        nonexistent = tmp_path / "nonexistent.env"
+        result = load_env_file(str(nonexistent))
+        assert result == {}
+
+    def test_load_env_file_empty_file(self, tmp_path):
+        """Test loading empty file returns empty dict."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("")
+
+        result = load_env_file(str(env_file))
+        assert result == {}
+
+    def test_load_env_file_only_comments(self, tmp_path):
+        """Test loading file with only comments returns empty dict."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("# Comment 1\n# Comment 2\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {}
+
+    def test_load_env_file_with_whitespace_only_lines(self, tmp_path):
+        """Test loading file with whitespace-only lines."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("KEY1=value1\n   \n\t\nKEY2=value2\n")
+
+        result = load_env_file(str(env_file))
+        assert result == {"KEY1": "value1", "KEY2": "value2"}
+
+    def test_load_env_file_expanduser(self, tmp_path, monkeypatch):
+        """Test that path supports tilde expansion."""
+        # Create a file in tmp_path
+        env_file = tmp_path / ".env"
+        env_file.write_text("TEST_KEY=test_value\n")
+
+        # Mock expanduser to return our tmp_path
+        original_expanduser = Path.expanduser
+
+        def mock_expanduser(self):
+            if str(self) == "~/.env":
+                return env_file
+            return original_expanduser(self)
+
+        monkeypatch.setattr(Path, "expanduser", mock_expanduser)
+
+        result = load_env_file("~/.env")
+        assert result == {"TEST_KEY": "test_value"}
+
+    def test_load_env_file_utf8_encoding(self, tmp_path):
+        """Test loading file with UTF-8 characters."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("KEY=café\n", encoding="utf-8")
+
+        result = load_env_file(str(env_file))
+        assert result["KEY"] == "café"
+
+    def test_load_env_file_empty_value(self, tmp_path):
+        """Test loading key with empty value."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("EMPTY_KEY=\nKEY2=value2\n")
+
+        result = load_env_file(str(env_file))
+        assert result["EMPTY_KEY"] == ""
+        assert result["KEY2"] == "value2"
+
+
+class TestLoadEnvSettings:
+    """Tests for load_env_settings function."""
+
+    def test_load_env_settings_populates_environ(self, tmp_path, monkeypatch):
+        """Test that load_env_settings populates os.environ."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("TEST_VAR=test_value\nANOTHER=another_value\n")
+
+        # Clear any existing values
+        monkeypatch.delenv("TEST_VAR", raising=False)
+        monkeypatch.delenv("ANOTHER", raising=False)
+
+        load_env_settings(str(env_file))
+
+        assert os.environ.get("TEST_VAR") == "test_value"
+        assert os.environ.get("ANOTHER") == "another_value"
+
+    def test_load_env_settings_preserves_existing(self, tmp_path, monkeypatch):
+        """Test that load_env_settings doesn't override existing env vars."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("EXISTING=from_file\n")
+
+        # Set existing value
+        monkeypatch.setenv("EXISTING", "original_value")
+
+        load_env_settings(str(env_file))
+
+        # Should preserve the original value
+        assert os.environ["EXISTING"] == "original_value"
+
+    def test_load_env_settings_adds_new_vars(self, tmp_path, monkeypatch):
+        """Test that load_env_settings adds new variables."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("NEW_VAR=new_value\n")
+
+        monkeypatch.delenv("NEW_VAR", raising=False)
+
+        load_env_settings(str(env_file))
+
+        assert os.environ.get("NEW_VAR") == "new_value"
+
+    def test_load_env_settings_nonexistent_file(self, tmp_path, monkeypatch):
+        """Test that load_env_settings handles nonexistent file gracefully."""
+        nonexistent = tmp_path / "nonexistent.env"
+
+        # Should not raise an exception
+        load_env_settings(str(nonexistent))
+
+        # Env should remain unchanged (no crash)
+        assert True
+
+    def test_load_env_settings_empty_file(self, tmp_path):
+        """Test that load_env_settings handles empty file."""
+        env_file = tmp_path / ".env"
+        env_file.write_text("")
+
+        # Should not raise an exception
+        load_env_settings(str(env_file))
+
+    def test_load_env_settings_multiple_calls(self, tmp_path, monkeypatch):
+        """Test multiple calls to load_env_settings."""
+        env_file1 = tmp_path / ".env1"
+        env_file1.write_text("VAR1=value1\n")
+
+        env_file2 = tmp_path / ".env2"
+        env_file2.write_text("VAR2=value2\nVAR1=override\n")
+
+        monkeypatch.delenv("VAR1", raising=False)
+        monkeypatch.delenv("VAR2", raising=False)
+
+        load_env_settings(str(env_file1))
+        assert os.environ["VAR1"] == "value1"
+
+        load_env_settings(str(env_file2))
+        # VAR1 should not be overridden (setdefault behavior)
+        assert os.environ["VAR1"] == "value1"
+        # VAR2 should be added
+        assert os.environ["VAR2"] == "value2"
diff --git a/tests/test_failures.py b/tests/test_failures.py
new file mode 100644
index 0000000..cbf7538
--- /dev/null
+++ b/tests/test_failures.py
@@ -0,0 +1,365 @@
+"""Unit tests for ci_tools.ci_runtime.failures module."""
+
+from __future__ import annotations
+
+from unittest.mock import patch
+from types import SimpleNamespace
+
+import pytest
+
+from ci_tools.ci_runtime.failures import (
+    _gather_focused_diff,
+    _render_coverage_context,
+    build_failure_context,
+)
+from ci_tools.ci_runtime.models import (
+    CommandResult,
+    CoverageCheckResult,
+    CoverageDeficit,
+    FailureContext,
+    CiAbort,
+)
+
+
+class TestGatherFocusedDiff:
+    """Tests for _gather_focused_diff helper function."""
+
+    def test_gathers_diffs_for_implicated_files(self):
+        """Test gathers diffs for all implicated files."""
+        with patch("ci_tools.ci_runtime.failures.gather_file_diff") as mock_diff:
+            mock_diff.side_effect = ["diff for file1", "diff for file2"]
+            result = _gather_focused_diff(["file1.py", "file2.py"])
+            assert "diff for file1" in result
+            assert "diff for file2" in result
+            assert mock_diff.call_count == 2
+
+    def test_skips_files_with_empty_diffs(self):
+        """Test skips files that have no diff."""
+        with patch("ci_tools.ci_runtime.failures.gather_file_diff") as mock_diff:
+            mock_diff.side_effect = ["diff content", ""]
+            result = _gather_focused_diff(["file1.py", "file2.py"])
+            assert "diff content" in result
+            assert result.count("\n\n") == 0  # Only one diff, no separator
+
+    def test_joins_multiple_diffs_with_double_newline(self):
+        """Test joins multiple diffs with double newline separator."""
+        with patch("ci_tools.ci_runtime.failures.gather_file_diff") as mock_diff:
+            mock_diff.side_effect = ["diff1", "diff2", "diff3"]
+            result = _gather_focused_diff(["a.py", "b.py", "c.py"])
+            parts = result.split("\n\n")
+            assert len(parts) == 3
+            assert "diff1" in parts[0]
+            assert "diff2" in parts[1]
+            assert "diff3" in parts[2]
+
+    def test_handles_empty_file_list(self):
+        """Test handles empty implicated file list."""
+        result = _gather_focused_diff([])
+        assert result == ""
+
+    def test_passes_relative_paths_to_gather_file_diff(self):
+        """Test passes file paths to gather_file_diff correctly."""
+        with patch("ci_tools.ci_runtime.failures.gather_file_diff") as mock_diff:
+            mock_diff.return_value = "diff"
+            _gather_focused_diff(["src/module.py"])
+            mock_diff.assert_called_once_with("src/module.py")
+
+
+class TestRenderCoverageContext:
+    """Tests for _render_coverage_context helper function."""
+
+    def test_generates_summary_and_log_excerpt(self):
+        """Test generates summary and log excerpt for coverage deficits."""
+        report = CoverageCheckResult(
+            table_text="Name    Cover\nfile.py   50%",
+            deficits=[CoverageDeficit(path="file.py", coverage=50.0)],
+            threshold=80.0,
+        )
+        summary, log_excerpt, implicated = _render_coverage_context(report)
+        assert "Coverage guard triggered" in summary
+        assert "file.py: 50.0%" in summary
+        assert "80%" in summary
+        assert "threshold" in log_excerpt.lower()
+        assert implicated == ["file.py"]
+
+    def test_formats_deficit_list(self):
+        """Test formats deficit list with proper formatting."""
+        report = CoverageCheckResult(
+            table_text="table",
+            deficits=[
+                CoverageDeficit(path="module1.py", coverage=65.5),
+                CoverageDeficit(path="module2.py", coverage=72.3),
+            ],
+            threshold=80.0,
+        )
+        summary, log_excerpt, implicated = _render_coverage_context(report)
+        assert "- module1.py: 65.5%" in summary
+        assert "- module2.py: 72.3%" in summary
+        assert len(implicated) == 2
+
+    def test_includes_table_text_in_log_excerpt(self):
+        """Test includes coverage table in log excerpt."""
+        report = CoverageCheckResult(
+            table_text="Name     Stmts   Cover\nfile.py    100    45%",
+            deficits=[CoverageDeficit(path="file.py", coverage=45.0)],
+            threshold=80.0,
+        )
+        summary, log_excerpt, implicated = _render_coverage_context(report)
+        assert "Name     Stmts   Cover" in log_excerpt
+        assert "file.py    100    45%" in log_excerpt
+
+    def test_returns_implicated_paths(self):
+        """Test returns list of implicated file paths."""
+        report = CoverageCheckResult(
+            table_text="",
+            deficits=[
+                CoverageDeficit(path="a.py", coverage=10.0),
+                CoverageDeficit(path="b.py", coverage=20.0),
+            ],
+            threshold=80.0,
+        )
+        summary, log_excerpt, implicated = _render_coverage_context(report)
+        assert implicated == ["a.py", "b.py"]
+
+    def test_formats_threshold_without_decimal_places(self):
+        """Test formats threshold as integer percentage."""
+        report = CoverageCheckResult(
+            table_text="",
+            deficits=[CoverageDeficit(path="file.py", coverage=70.0)],
+            threshold=85.5,
+        )
+        summary, log_excerpt, implicated = _render_coverage_context(report)
+        assert "86%" in summary  # 85.5 rounds to 86
+
+
+class TestBuildFailureContext:
+    """Tests for build_failure_context function."""
+
+    def test_handles_coverage_report(self, capsys):
+        """Test handles coverage report scenario."""
+        args = SimpleNamespace(log_tail=100)
+        result = CommandResult(returncode=1, stdout="", stderr="")
+        report = CoverageCheckResult(
+            table_text="table",
+            deficits=[CoverageDeficit(path="module.py", coverage=60.0)],
+            threshold=80.0,
+        )
+        with patch(
+            "ci_tools.ci_runtime.failures._gather_focused_diff"
+        ) as mock_focused:
+            mock_focused.return_value = "focused diff"
+            context = build_failure_context(args, result, report)
+            assert isinstance(context, FailureContext)
+            assert "Coverage" in context.log_excerpt
+            assert context.coverage_report == report
+            assert "module.py" in context.implicated_files
+            captured = capsys.readouterr()
+            assert "Coverage below" in captured.out
+
+    def test_handles_regular_ci_failure(self, capsys):
+        """Test handles regular CI failure without coverage report."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(
+            returncode=1, stdout="test output\nerror occurred", stderr=""
+        )
+        with patch("ci_tools.ci_runtime.failures.summarize_failure") as mock_summarize:
+            with patch(
+                "ci_tools.ci_runtime.failures._gather_focused_diff"
+            ) as mock_focused:
+                with patch(
+                    "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+                ) as mock_missing:
+                    with patch(
+                        "ci_tools.ci_runtime.failures.detect_attribute_error"
+                    ) as mock_attr:
+                        mock_summarize.return_value = ("summary", ["file.py"])
+                        mock_missing.return_value = None
+                        mock_attr.return_value = None
+                        mock_focused.return_value = "diff"
+                        context = build_failure_context(args, result, None)
+                        assert context.summary == "summary"
+                        assert context.implicated_files == ["file.py"]
+                        captured = capsys.readouterr()
+                        assert "CI failed" in captured.out
+
+    def test_aborts_on_missing_symbol_error(self, capsys):
+        """Test aborts when missing symbol error detected."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=1, stdout="ImportError detected", stderr="")
+        with patch("ci_tools.ci_runtime.failures.summarize_failure") as mock_summarize:
+            with patch(
+                "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+            ) as mock_missing:
+                mock_summarize.return_value = ("", [])
+                mock_missing.return_value = "Missing symbol hint"
+                with pytest.raises(CiAbort) as exc_info:
+                    build_failure_context(args, result, None)
+                assert "Manual intervention required" in str(exc_info.value)
+                captured = capsys.readouterr()
+                assert "Missing symbol hint" in captured.err
+
+    def test_aborts_on_attribute_error(self, capsys):
+        """Test aborts when attribute error detected."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=1, stdout="AttributeError occurred", stderr="")
+        with patch("ci_tools.ci_runtime.failures.summarize_failure") as mock_summarize:
+            with patch(
+                "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+            ) as mock_missing:
+                with patch(
+                    "ci_tools.ci_runtime.failures.detect_attribute_error"
+                ) as mock_attr:
+                    mock_summarize.return_value = ("", [])
+                    mock_missing.return_value = None
+                    mock_attr.return_value = "Attribute error hint"
+                    with pytest.raises(CiAbort):
+                        build_failure_context(args, result, None)
+                    captured = capsys.readouterr()
+                    assert "Attribute error hint" in captured.err
+
+    def test_uses_tail_text_for_log_excerpt(self):
+        """Test uses tail_text to extract log excerpt."""
+        args = SimpleNamespace(log_tail=5)
+        long_output = "\n".join([f"line{i}" for i in range(20)])
+        result = CommandResult(returncode=1, stdout=long_output, stderr="")
+        with patch("ci_tools.ci_runtime.failures.tail_text") as mock_tail:
+            with patch(
+                "ci_tools.ci_runtime.failures.summarize_failure"
+            ) as mock_summarize:
+                with patch(
+                    "ci_tools.ci_runtime.failures._gather_focused_diff"
+                ) as mock_focused:
+                    with patch(
+                        "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+                    ) as mock_missing:
+                        with patch(
+                            "ci_tools.ci_runtime.failures.detect_attribute_error"
+                        ) as mock_attr:
+                            mock_tail.return_value = "last 5 lines"
+                            mock_summarize.return_value = ("summary", [])
+                            mock_missing.return_value = None
+                            mock_attr.return_value = None
+                            mock_focused.return_value = ""
+                            context = build_failure_context(args, result, None)
+                            mock_tail.assert_called_once_with(long_output, 5)
+                            assert context.log_excerpt == "last 5 lines"
+
+    def test_gathers_focused_diff_for_implicated_files(self):
+        """Test gathers focused diff for implicated files."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=1, stdout="", stderr="")
+        with patch("ci_tools.ci_runtime.failures.summarize_failure") as mock_summarize:
+            with patch(
+                "ci_tools.ci_runtime.failures._gather_focused_diff"
+            ) as mock_focused:
+                with patch(
+                    "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+                ) as mock_missing:
+                    with patch(
+                        "ci_tools.ci_runtime.failures.detect_attribute_error"
+                    ) as mock_attr:
+                        mock_summarize.return_value = ("summary", ["a.py", "b.py"])
+                        mock_focused.return_value = "focused diff content"
+                        mock_missing.return_value = None
+                        mock_attr.return_value = None
+                        context = build_failure_context(args, result, None)
+                        mock_focused.assert_called_once_with(["a.py", "b.py"])
+                        assert context.focused_diff == "focused diff content"
+
+    def test_coverage_report_included_in_context(self):
+        """Test coverage report is included in failure context."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=0, stdout="", stderr="")
+        report = CoverageCheckResult(
+            table_text="", deficits=[CoverageDeficit("f.py", 50.0)], threshold=80.0
+        )
+        with patch("ci_tools.ci_runtime.failures._gather_focused_diff"):
+            context = build_failure_context(args, result, report)
+            assert context.coverage_report == report
+
+    def test_prints_coverage_deficits_with_details(self, capsys):
+        """Test prints coverage deficit details."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=1, stdout="", stderr="")
+        report = CoverageCheckResult(
+            table_text="",
+            deficits=[
+                CoverageDeficit(path="module1.py", coverage=55.5),
+                CoverageDeficit(path="module2.py", coverage=70.0),
+            ],
+            threshold=85.0,
+        )
+        with patch("ci_tools.ci_runtime.failures._gather_focused_diff"):
+            build_failure_context(args, result, report)
+            captured = capsys.readouterr()
+            assert "module1.py (55.5%)" in captured.out
+            assert "module2.py (70.0%)" in captured.out
+
+    def test_handles_empty_implicated_files(self):
+        """Test handles scenario with no implicated files."""
+        args = SimpleNamespace(log_tail=50)
+        result = CommandResult(returncode=1, stdout="generic error", stderr="")
+        with patch("ci_tools.ci_runtime.failures.summarize_failure") as mock_summarize:
+            with patch(
+                "ci_tools.ci_runtime.failures._gather_focused_diff"
+            ) as mock_focused:
+                with patch(
+                    "ci_tools.ci_runtime.failures.detect_missing_symbol_error"
+                ) as mock_missing:
+                    with patch(
+                        "ci_tools.ci_runtime.failures.detect_attribute_error"
+                    ) as mock_attr:
+                        mock_summarize.return_value = ("generic failure", [])
+                        mock_focused.return_value = ""
+                        mock_missing.return_value = None
+                        mock_attr.return_value = None
+                        context = build_failure_context(args, result, None)
+                        assert context.implicated_files == []
+                        assert context.focused_diff == ""
+
+
+class TestFailureContext:
+    """Tests for FailureContext dataclass."""
+
+    def test_dataclass_initialization(self):
+        """Test FailureContext can be initialized."""
+        context = FailureContext(
+            log_excerpt="log",
+            summary="summary",
+            implicated_files=["file.py"],
+            focused_diff="diff",
+            coverage_report=None,
+        )
+        assert context.log_excerpt == "log"
+        assert context.summary == "summary"
+        assert context.implicated_files == ["file.py"]
+        assert context.focused_diff == "diff"
+        assert context.coverage_report is None
+
+    def test_coverage_report_can_be_none(self):
+        """Test coverage_report field can be None."""
+        context = FailureContext(
+            log_excerpt="",
+            summary="",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=None,
+        )
+        assert context.coverage_report is None
+
+    def test_coverage_report_can_hold_result(self):
+        """Test coverage_report field can hold CoverageCheckResult."""
+        report = CoverageCheckResult(
+            table_text="table",
+            deficits=[CoverageDeficit("file.py", 60.0)],
+            threshold=80.0,
+        )
+        context = FailureContext(
+            log_excerpt="",
+            summary="",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=report,
+        )
+        assert context.coverage_report == report
+        assert context.coverage_report.threshold == 80.0
diff --git a/tests/test_function_size_guard.py b/tests/test_function_size_guard.py
new file mode 100644
index 0000000..dc1a547
--- /dev/null
+++ b/tests/test_function_size_guard.py
@@ -0,0 +1,429 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import function_size_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = function_size_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_function_lines == 80
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = function_size_guard.parse_args(
+        ["--root", "custom", "--max-function-lines", "50", "--exclude", "tests"]
+    )
+    assert args.root == Path("custom")
+    assert args.max_function_lines == 50
+    assert args.exclude == [Path("tests")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(function_size_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(function_size_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_iter_python_files_directory(tmp_path: Path):
+    """Test iter_python_files with a directory."""
+    (tmp_path / "file1.py").write_text("# file1")
+    (tmp_path / "file2.py").write_text("# file2")
+    (tmp_path / "subdir").mkdir()
+    (tmp_path / "subdir" / "file3.py").write_text("# file3")
+
+    files = list(function_size_guard.iter_python_files(tmp_path))
+    assert len(files) == 3
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert function_size_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert function_size_guard.is_excluded(path, exclusions) is False
+
+
+def test_is_excluded_handles_attribute_error():
+    """Test is_excluded handles AttributeError correctly."""
+    path = Path("/project/src/module.py")
+    exclusions = [Path("/other/path")]
+    result = function_size_guard.is_excluded(path, exclusions)
+    assert result is False
+
+
+def test_count_function_lines_basic():
+    """Test counting lines for basic function."""
+    source = textwrap.dedent(
+        """
+        def foo():
+            x = 1
+            return x
+        """
+    ).strip()
+
+    tree = function_size_guard.ast.parse(source)
+    func_node = tree.body[0]
+    count = function_size_guard.count_function_lines(func_node)
+    assert count == 3
+
+
+def test_count_function_lines_no_end_lineno():
+    """Test counting lines when end_lineno is None."""
+    source = "def foo(): pass"
+    tree = function_size_guard.ast.parse(source)
+    func_node = tree.body[0]
+
+    # Simulate missing end_lineno
+    if hasattr(func_node, "end_lineno"):
+        original_end = func_node.end_lineno
+        func_node.end_lineno = None
+        count = function_size_guard.count_function_lines(func_node)
+        assert count == 0
+        func_node.end_lineno = original_end
+
+
+def test_count_function_lines_async_function():
+    """Test counting lines for async function."""
+    source = textwrap.dedent(
+        """
+        async def async_foo():
+            x = 1
+            await something()
+            return x
+        """
+    ).strip()
+
+    tree = function_size_guard.ast.parse(source)
+    func_node = tree.body[0]
+    count = function_size_guard.count_function_lines(func_node)
+    assert count == 4
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the line limit."""
+    py_file = tmp_path / "small.py"
+    write_module(
+        py_file,
+        """
+        def small_function():
+            return 1
+        """,
+    )
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+
+
+def test_scan_file_exceeds_limit(tmp_path: Path):
+    """Test scanning a file that exceeds the limit."""
+    py_file = tmp_path / "large.py"
+    lines = "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    content = f"def large_function():\n{lines}"
+    py_file.write_text(content)
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 1
+    assert violations[0][0] == str(py_file)
+    assert violations[0][1] == "large_function"
+    assert violations[0][2] == 1  # Line number
+    assert violations[0][3] > 10  # Line count
+
+
+def test_scan_file_multiple_functions(tmp_path: Path):
+    """Test scanning a file with multiple functions."""
+    py_file = tmp_path / "multi.py"
+    write_module(
+        py_file,
+        """
+        def small_function():
+            return 1
+
+        def large_function():
+            x = 1
+            y = 2
+            z = 3
+            a = 4
+            b = 5
+            c = 6
+            d = 7
+            e = 8
+            f = 9
+            g = 10
+            return x + y + z + a + b + c + d + e + f + g
+        """,
+    )
+
+    violations = function_size_guard.scan_file(py_file, limit=5)
+    assert len(violations) == 1
+    assert violations[0][1] == "large_function"
+
+
+def test_scan_file_syntax_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test scan_file with syntax error."""
+    py_file = tmp_path / "bad.py"
+    py_file.write_text("def foo(\n")  # Missing closing paren
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+    captured = capsys.readouterr()
+    assert "failed to parse" in captured.err
+
+
+def test_scan_file_unicode_decode_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test scan_file with Unicode decode error."""
+    py_file = tmp_path / "bad_encoding.py"
+    py_file.write_bytes(b"\xff\xfe\x00\x00")  # Invalid UTF-8
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+    captured = capsys.readouterr()
+    assert "failed to parse" in captured.err
+
+
+def test_scan_file_async_functions(tmp_path: Path):
+    """Test scanning file with async functions."""
+    py_file = tmp_path / "async_funcs.py"
+    lines = "\n".join([f"    line_{i} = {i}" for i in range(15)])
+    content = f"async def large_async():\n{lines}"
+    py_file.write_text(content)
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 1
+    assert violations[0][1] == "large_async"
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "small.py",
+        """
+        def small_function():
+            return 1
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(["--root", str(root), "--max-function-lines", "10"])
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    py_file = root / "large.py"
+
+    lines = "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    content = f"def large_function():\n{lines}"
+    py_file.write_text(content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(["--root", str(root), "--max-function-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Oversized functions detected" in captured.err
+    assert "large_function" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    large_func = "def large():\n" + "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    (root / "included.py").write_text(large_func)
+    (excluded / "excluded.py").write_text(large_func)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(
+            ["--root", str(root), "--max-function-lines", "10", "--exclude", str(excluded)]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_handles_multiple_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles multiple violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_func = "def large():\n" + "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    (root / "file1.py").write_text(large_func)
+    (root / "file2.py").write_text(large_func)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(["--root", str(root), "--max-function-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "file1.py" in captured.err
+    assert "file2.py" in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_func = "def large():\n" + "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    (root / "zebra.py").write_text(large_func)
+    (root / "alpha.py").write_text(large_func)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(["--root", str(root), "--max-function-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [
+        line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line
+    ]
+    assert len(err_lines) == 2
+    assert "alpha.py" in err_lines[0]
+    assert "zebra.py" in err_lines[1]
+
+
+def test_main_traverse_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles traversal errors."""
+    missing = tmp_path / "missing"
+
+    result = function_size_guard.main(["--root", str(missing)])
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to traverse" in captured.err
+
+
+def test_scan_file_no_functions(tmp_path: Path):
+    """Test scanning a file with no functions."""
+    py_file = tmp_path / "no_funcs.py"
+    write_module(
+        py_file,
+        """
+        x = 1
+        y = 2
+        """,
+    )
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+
+
+def test_scan_file_nested_functions(tmp_path: Path):
+    """Test scanning file with nested functions."""
+    py_file = tmp_path / "nested.py"
+    lines = "\n".join([f"        inner_line_{i} = {i}" for i in range(15)])
+    content = f"def outer():\n    def inner():\n{lines}\n    return inner"
+    py_file.write_text(content)
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    # Should detect the large inner function
+    assert len(violations) >= 1
+
+
+def test_scan_file_methods_in_class(tmp_path: Path):
+    """Test scanning methods inside classes."""
+    py_file = tmp_path / "methods.py"
+    lines = "\n".join([f"        line_{i} = {i}" for i in range(20)])
+    content = f"class Foo:\n    def large_method(self):\n{lines}"
+    py_file.write_text(content)
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 1
+    assert violations[0][1] == "large_method"
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_func = "def large():\n" + "\n".join([f"    line_{i} = {i}" for i in range(20)])
+    (root / "module.py").write_text(large_func)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = function_size_guard.main(["--root", str(root), "--max-function-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "module.py" in captured.err
+    assert "large" in captured.err
+
+
+def test_iter_python_files_empty_directory(tmp_path: Path):
+    """Test iter_python_files with empty directory."""
+    files = list(function_size_guard.iter_python_files(tmp_path))
+    assert len(files) == 0
+
+
+def test_is_excluded_multiple_exclusions():
+    """Test exclusion with multiple patterns."""
+    path = Path("/project/tests/test_module.py").resolve()
+    exclusions = [
+        Path("/project/vendor").resolve(),
+        Path("/project/tests").resolve(),
+    ]
+    assert function_size_guard.is_excluded(path, exclusions) is True
+
+
+def test_count_function_lines_single_line():
+    """Test counting lines for single-line function."""
+    source = "def foo(): return 1"
+    tree = function_size_guard.ast.parse(source)
+    func_node = tree.body[0]
+    count = function_size_guard.count_function_lines(func_node)
+    assert count == 1
+
+
+def test_scan_file_with_decorators(tmp_path: Path):
+    """Test scanning functions with decorators."""
+    py_file = tmp_path / "decorated.py"
+    lines = "\n".join([f"    line_{i} = {i}" for i in range(15)])
+    content = f"@decorator\ndef decorated_func():\n{lines}"
+    py_file.write_text(content)
+
+    violations = function_size_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 1
+    assert violations[0][1] == "decorated_func"
diff --git a/tests/test_heuristics.py b/tests/test_heuristics.py
new file mode 100644
index 0000000..e100fd3
--- /dev/null
+++ b/tests/test_heuristics.py
@@ -0,0 +1,265 @@
+"""Unit tests for ci_tools.ci_runtime.heuristics module."""
+
+from __future__ import annotations
+
+from ci_tools.ci_runtime.heuristics import (
+    detect_attribute_error,
+    detect_missing_symbol_error,
+    summarize_failure,
+    tail_text,
+)
+
+
+class TestDetectMissingSymbolError:
+    """Tests for detect_missing_symbol_error function."""
+
+    def test_import_error_detected(self):
+        """Test detection of ImportError with missing symbol."""
+        log = "ImportError: cannot import name 'MyClass' from 'mymodule'"
+        result = detect_missing_symbol_error(log)
+        assert result is not None
+        assert "ImportError detected" in result
+        assert "MyClass" in result
+        assert "mymodule" in result
+
+    def test_no_import_error(self):
+        """Test returns None when no ImportError present."""
+        log = "Some other error occurred"
+        result = detect_missing_symbol_error(log)
+        assert result is None
+
+    def test_import_error_multiline_log(self):
+        """Test ImportError detection in multiline logs."""
+        log = """
+        Traceback (most recent call last):
+          File "test.py", line 5, in <module>
+        ImportError: cannot import name 'foo' from 'bar'
+        """
+        result = detect_missing_symbol_error(log)
+        assert result is not None
+        assert "foo" in result
+        assert "bar" in result
+
+    def test_empty_log(self):
+        """Test empty log returns None."""
+        result = detect_missing_symbol_error("")
+        assert result is None
+
+
+class TestDetectAttributeError:
+    """Tests for detect_attribute_error function."""
+
+    def test_attribute_error_with_repo_file(self):
+        """Test AttributeError detection with file from repository."""
+        from ci_tools.ci_runtime.config import REPO_ROOT
+
+        # Create a log with a file that is in the repo
+        test_file = REPO_ROOT / "ci_tools" / "ci_runtime" / "models.py"
+
+        log = f'''
+        Traceback (most recent call last):
+          File "{test_file}", line 10, in test_function
+            obj.missing_attribute()
+        AttributeError: 'MyClass' object has no attribute 'missing_attribute'
+        '''
+        result = detect_attribute_error(log)
+        # Should successfully detect the error with a repo file
+        assert result is not None
+        assert "AttributeError detected" in result
+        assert "missing_attribute" in result
+        assert "ci_tools" in result or "models.py" in result
+
+    def test_attribute_error_no_file_match(self):
+        """Test AttributeError without matching repository file."""
+        log = """
+        AttributeError: 'str' object has no attribute 'nonexistent'
+        """
+        result = detect_attribute_error(log)
+        # Without a File line, should return None
+        assert result is None
+
+    def test_no_attribute_error(self):
+        """Test returns None when no AttributeError present."""
+        log = "ValueError: invalid value provided"
+        result = detect_attribute_error(log)
+        assert result is None
+
+    def test_empty_log(self):
+        """Test empty log returns None."""
+        result = detect_attribute_error("")
+        assert result is None
+
+    def test_attribute_error_pattern_matching(self):
+        """Test attribute error pattern matching."""
+        log = "AttributeError: 'NoneType' object has no attribute 'value'"
+        result = detect_attribute_error(log)
+        # Without file frame, returns None
+        assert result is None
+
+    def test_attribute_error_with_oserror(self, monkeypatch):
+        """Test AttributeError handling when resolve() raises OSError."""
+        from pathlib import Path
+
+        # Create a log with a file path that will trigger OSError
+        log = '''
+        Traceback (most recent call last):
+          File "/invalid/path/that/does/not/exist.py", line 10, in test_function
+            obj.missing_method()
+        AttributeError: 'MyClass' object has no attribute 'missing_method'
+        '''
+
+        # Mock Path.resolve to raise OSError
+        original_resolve = Path.resolve
+
+        def mock_resolve(self):
+            if "does/not/exist" in str(self):
+                raise OSError("Cannot resolve path")
+            return original_resolve(self)
+
+        monkeypatch.setattr(Path, "resolve", mock_resolve)
+
+        result = detect_attribute_error(log)
+        # Should return None when OSError occurs and no valid candidate found
+        assert result is None
+
+    def test_attribute_error_outside_repo(self):
+        """Test AttributeError with file outside repository."""
+        # Using a path that's definitely not in the repo
+        log = '''
+        Traceback (most recent call last):
+          File "/usr/lib/python3.12/site-packages/test.py", line 10, in test_function
+            obj.bad_attr()
+        AttributeError: 'dict' object has no attribute 'bad_attr'
+        '''
+        result = detect_attribute_error(log)
+        # Should return None for files outside the repo
+        assert result is None
+
+
+class TestSummarizeFailure:
+    """Tests for summarize_failure function."""
+
+    def test_pyright_errors_detected(self):
+        """Test summarization of pyright type errors."""
+        log = """
+        /Users/john/project/src/module.py:42: error: Type mismatch
+        /Users/john/project/src/helper.py:10: error: Missing type annotation
+        """
+        summary, files = summarize_failure(log)
+        assert "pyright reported type errors" in summary
+        # The regex captures everything after /Users/[^:]+/
+        assert "module.py:42" in summary or "src/module.py:42" in summary
+        assert "helper.py:10" in summary or "src/helper.py:10" in summary
+        # Files list contains the relative paths
+        assert any("module.py" in f for f in files)
+        assert any("helper.py" in f for f in files)
+        assert len(files) == 2
+
+    def test_duplicate_files_deduplicated(self):
+        """Test that duplicate files are deduplicated."""
+        log = """
+        /Users/john/project/src/module.py:42: error: Type mismatch
+        /Users/john/project/src/module.py:50: error: Another error
+        """
+        summary, files = summarize_failure(log)
+        assert len(files) == 1
+        # The file path is extracted as everything after /Users/[^:]+/
+        assert any("module.py" in f for f in files)
+
+    def test_pyright_lines_skipped(self):
+        """Test that lines containing 'pyright' are skipped."""
+        log = """
+        Running pyright analysis...
+        /Users/john/project/src/module.py:42: error: Type mismatch
+        pyright: finished in 1.2s
+        """
+        summary, files = summarize_failure(log)
+        assert "pyright reported type errors" in summary
+        assert any("module.py" in f for f in files)
+
+    def test_no_errors_found(self):
+        """Test empty summary when no errors found."""
+        log = "All tests passed successfully"
+        summary, files = summarize_failure(log)
+        assert summary == ""
+        assert files == []
+
+    def test_empty_log(self):
+        """Test empty log returns empty results."""
+        summary, files = summarize_failure("")
+        assert summary == ""
+        assert files == []
+
+    def test_mixed_content(self):
+        """Test log with mixed content."""
+        log = """
+        Starting tests...
+        /Users/jane/myrepo/tests/test_foo.py:15: assertion failed
+        Some other output
+        /Users/jane/myrepo/src/core.py:99: error message
+        """
+        summary, files = summarize_failure(log)
+        if files:  # If pattern matches
+            # Files contain paths after /Users/jane/myrepo/
+            assert any("test_foo.py" in f for f in files) or any("core.py" in f for f in files)
+
+
+class TestTailText:
+    """Tests for tail_text function."""
+
+    def test_tail_single_line(self):
+        """Test tail with single line text."""
+        text = "single line"
+        result = tail_text(text, 5)
+        assert result == "single line"
+
+    def test_tail_exact_lines(self):
+        """Test tail with exact number of lines requested."""
+        text = "line1\nline2\nline3"
+        result = tail_text(text, 3)
+        assert result == "line1\nline2\nline3"
+
+    def test_tail_fewer_lines(self):
+        """Test tail requesting fewer lines than available."""
+        text = "line1\nline2\nline3\nline4\nline5"
+        result = tail_text(text, 3)
+        assert result == "line3\nline4\nline5"
+
+    def test_tail_more_lines_than_available(self):
+        """Test tail requesting more lines than available."""
+        text = "line1\nline2"
+        result = tail_text(text, 10)
+        assert result == "line1\nline2"
+
+    def test_tail_zero_lines(self):
+        """Test tail with zero lines requested."""
+        text = "line1\nline2\nline3"
+        result = tail_text(text, 0)
+        # Python list[-0:] returns the entire list
+        assert result == "line1\nline2\nline3"
+
+    def test_tail_empty_text(self):
+        """Test tail with empty text."""
+        result = tail_text("", 5)
+        assert result == ""
+
+    def test_tail_with_trailing_newline(self):
+        """Test tail with text ending in newline."""
+        text = "line1\nline2\nline3\n"
+        result = tail_text(text, 2)
+        # splitlines() doesn't include trailing empty string
+        assert "line2" in result
+        assert "line3" in result
+
+    def test_tail_single_line_request(self):
+        """Test tail requesting single line."""
+        text = "line1\nline2\nline3"
+        result = tail_text(text, 1)
+        assert result == "line3"
+
+    def test_tail_negative_lines(self):
+        """Test tail with negative line count."""
+        text = "line1\nline2\nline3"
+        result = tail_text(text, -1)
+        # Python list[-(-1):] = list[1:] which skips the first element
+        assert result == "line2\nline3"
diff --git a/tests/test_inheritance_guard.py b/tests/test_inheritance_guard.py
new file mode 100644
index 0000000..f29ddcf
--- /dev/null
+++ b/tests/test_inheritance_guard.py
@@ -0,0 +1,542 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import inheritance_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = inheritance_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_depth == 2
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = inheritance_guard.parse_args(
+        ["--root", "custom", "--max-depth", "3", "--exclude", "tests"]
+    )
+    assert args.root == Path("custom")
+    assert args.max_depth == 3
+    assert args.exclude == [Path("tests")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(inheritance_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(inheritance_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert inheritance_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert inheritance_guard.is_excluded(path, exclusions) is False
+
+
+def test_extract_base_names_simple():
+    """Test extracting base names from simple inheritance."""
+    source = "class Child(Parent): pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    assert base_names == ["Parent"]
+
+
+def test_extract_base_names_multiple():
+    """Test extracting multiple base names."""
+    source = "class Child(Parent1, Parent2): pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    assert base_names == ["Parent1", "Parent2"]
+
+
+def test_extract_base_names_attribute():
+    """Test extracting base names with module attributes."""
+    source = "class Child(module.Parent): pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    assert base_names == ["module.Parent"]
+
+
+def test_extract_base_names_nested_attribute():
+    """Test extracting base names with nested module attributes."""
+    source = "class Child(package.module.Parent): pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    assert base_names == ["package.module.Parent"]
+
+
+def test_extract_base_names_no_bases():
+    """Test extracting base names with no bases."""
+    source = "class Child: pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    assert base_names == []
+
+
+def test_build_class_hierarchy_basic():
+    """Test building class hierarchy."""
+    source = textwrap.dedent(
+        """
+        class Parent:
+            pass
+
+        class Child(Parent):
+            pass
+        """
+    ).strip()
+
+    tree = inheritance_guard.ast.parse(source)
+    hierarchy = inheritance_guard.build_class_hierarchy(tree)
+
+    assert "Parent" in hierarchy
+    assert "Child" in hierarchy
+    assert hierarchy["Parent"] == []
+    assert hierarchy["Child"] == ["Parent"]
+
+
+def test_build_class_hierarchy_multiple_classes():
+    """Test building hierarchy with multiple classes."""
+    source = textwrap.dedent(
+        """
+        class A:
+            pass
+
+        class B(A):
+            pass
+
+        class C(B):
+            pass
+        """
+    ).strip()
+
+    tree = inheritance_guard.ast.parse(source)
+    hierarchy = inheritance_guard.build_class_hierarchy(tree)
+
+    assert hierarchy["A"] == []
+    assert hierarchy["B"] == ["A"]
+    assert hierarchy["C"] == ["B"]
+
+
+def test_calculate_depth_no_inheritance():
+    """Test calculating depth with no inheritance."""
+    hierarchy = {"Child": []}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 0
+
+
+def test_calculate_depth_single_level():
+    """Test calculating depth with single level inheritance."""
+    hierarchy = {"Parent": [], "Child": ["Parent"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 1
+
+
+def test_calculate_depth_multiple_levels():
+    """Test calculating depth with multiple levels."""
+    hierarchy = {"GrandParent": [], "Parent": ["GrandParent"], "Child": ["Parent"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 2
+
+
+def test_calculate_depth_unknown_class():
+    """Test calculating depth for unknown class."""
+    hierarchy = {"Known": []}
+    depth = inheritance_guard.calculate_depth("Unknown", hierarchy)
+    assert depth == 0
+
+
+def test_calculate_depth_external_base():
+    """Test calculating depth with external base class."""
+    hierarchy = {"Child": ["ExternalBase"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 1
+
+
+def test_calculate_depth_ignores_object():
+    """Test calculating depth ignores object base class."""
+    hierarchy = {"Child": ["object"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 0
+
+
+def test_calculate_depth_ignores_protocol():
+    """Test calculating depth ignores Protocol base class."""
+    hierarchy = {"Child": ["Protocol"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 0
+
+
+def test_calculate_depth_ignores_abc():
+    """Test calculating depth ignores ABC base class."""
+    hierarchy = {"Child": ["ABC"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 0
+
+
+def test_calculate_depth_cycle_detection():
+    """Test calculating depth handles cycles."""
+    hierarchy = {"A": ["B"], "B": ["A"]}
+    depth = inheritance_guard.calculate_depth("A", hierarchy)
+    assert depth >= 0  # Should not crash
+
+
+def test_calculate_depth_multiple_bases():
+    """Test calculating depth with multiple bases."""
+    hierarchy = {"Base1": [], "Base2": ["Base1"], "Child": ["Base2", "Base1"]}
+    depth = inheritance_guard.calculate_depth("Child", hierarchy)
+    assert depth == 2  # Max depth from Base2
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the depth limit."""
+    py_file = tmp_path / "simple.py"
+    write_module(
+        py_file,
+        """
+        class Parent:
+            pass
+
+        class Child(Parent):
+            pass
+        """,
+    )
+
+    violations = inheritance_guard.scan_file(py_file, max_depth=2)
+    assert len(violations) == 0
+
+
+def test_scan_file_exceeds_limit(tmp_path: Path):
+    """Test scanning a file that exceeds the depth limit."""
+    py_file = tmp_path / "deep.py"
+    write_module(
+        py_file,
+        """
+        class GrandParent:
+            pass
+
+        class Parent(GrandParent):
+            pass
+
+        class Child(Parent):
+            pass
+
+        class DeepChild(Child):
+            pass
+        """,
+    )
+
+    violations = inheritance_guard.scan_file(py_file, max_depth=2)
+    assert len(violations) >= 1
+    assert any(v[1] == "DeepChild" for v in violations)
+
+
+def test_scan_file_syntax_error(tmp_path: Path):
+    """Test scan_file with syntax error."""
+    py_file = tmp_path / "bad.py"
+    py_file.write_text("class Foo:\n    def method(self\n")
+
+    with pytest.raises(RuntimeError, match="failed to parse Python source"):
+        inheritance_guard.scan_file(py_file, max_depth=2)
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "simple.py",
+        """
+        class Parent:
+            pass
+
+        class Child(Parent):
+            pass
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(["--root", str(root), "--max-depth", "2"])
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "deep.py",
+        """
+        class A:
+            pass
+
+        class B(A):
+            pass
+
+        class C(B):
+            pass
+
+        class D(C):
+            pass
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(["--root", str(root), "--max-depth", "2"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Deep inheritance detected" in captured.err
+    assert "composition over inheritance" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    deep_hierarchy = """
+        class A:
+            pass
+        class B(A):
+            pass
+        class C(B):
+            pass
+        class D(C):
+            pass
+    """
+    write_module(root / "included.py", deep_hierarchy)
+    write_module(excluded / "excluded.py", deep_hierarchy)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(
+            ["--root", str(root), "--max-depth", "1", "--exclude", str(excluded)]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    deep_hierarchy = """
+        class A:
+            pass
+        class B(A):
+            pass
+        class C(B):
+            pass
+    """
+    write_module(root / "zebra.py", deep_hierarchy)
+    write_module(root / "alpha.py", deep_hierarchy)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(["--root", str(root), "--max-depth", "1"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [
+        line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line
+    ]
+    assert len(err_lines) >= 2
+
+
+def test_main_scan_file_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles scan_file errors."""
+    root = tmp_path / "src"
+    root.mkdir()
+    (root / "bad.py").write_text("class Foo:\n    def method(self\n")
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(["--root", str(root)])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to parse" in captured.err
+
+
+def test_format_inheritance_violation():
+    """Test formatting inheritance violation messages."""
+    violation = inheritance_guard._format_inheritance_violation(
+        Path("/project/src/module.py"),
+        class_name="DeepClass",
+        lineno=10,
+        depth=3,
+        base_names=["Parent1", "Parent2"],
+        limit=2,
+        repo_root=Path("/project"),
+    )
+
+    assert "module.py:10" in violation or "module.py" in violation
+    assert "DeepClass" in violation
+    assert "depth 3" in violation
+    assert "limit 2" in violation
+    assert "Parent1, Parent2" in violation
+
+
+def test_collect_inheritance_violations(tmp_path: Path):
+    """Test collecting inheritance violations."""
+    py_file = tmp_path / "test.py"
+    write_module(
+        py_file,
+        """
+        class A:
+            pass
+        class B(A):
+            pass
+        class C(B):
+            pass
+        """,
+    )
+
+    violations = inheritance_guard._collect_inheritance_violations(
+        py_file, max_depth=1, repo_root=tmp_path
+    )
+
+    assert len(violations) >= 1
+    assert any("C" in v for v in violations)
+
+
+def test_print_inheritance_report(capsys: pytest.CaptureFixture):
+    """Test printing inheritance report."""
+    violations = ["violation1", "violation2"]
+    inheritance_guard._print_inheritance_report(violations, limit=2)
+
+    captured = capsys.readouterr()
+    assert "Deep inheritance detected" in captured.err
+    assert "depth 2" in captured.err
+    assert "composition over inheritance" in captured.err
+    assert "violation1" in captured.err
+    assert "violation2" in captured.err
+
+
+def test_main_traverse_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles traversal errors."""
+    missing = tmp_path / "missing"
+
+    result = inheritance_guard.main(["--root", str(missing)])
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to traverse" in captured.err
+
+
+def test_scan_file_no_classes(tmp_path: Path):
+    """Test scanning a file with no classes."""
+    py_file = tmp_path / "no_classes.py"
+    write_module(
+        py_file,
+        """
+        def function():
+            pass
+        """,
+    )
+
+    violations = inheritance_guard.scan_file(py_file, max_depth=2)
+    assert len(violations) == 0
+
+
+def test_calculate_depth_diamond_inheritance():
+    """Test calculating depth with diamond inheritance pattern."""
+    hierarchy = {
+        "Base": [],
+        "Left": ["Base"],
+        "Right": ["Base"],
+        "Diamond": ["Left", "Right"],
+    }
+    depth = inheritance_guard.calculate_depth("Diamond", hierarchy)
+    assert depth == 2
+
+
+def test_extract_base_names_complex_expression():
+    """Test extract_base_names handles non-Name and non-Attribute bases."""
+    source = "class Child(Parent if condition else Other): pass"
+    tree = inheritance_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    base_names = inheritance_guard.extract_base_names(class_node)
+    # Should handle gracefully, possibly returning empty or partial
+    assert isinstance(base_names, list)
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(
+        root / "module.py",
+        """
+        class A:
+            pass
+        class B(A):
+            pass
+        class C(B):
+            pass
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = inheritance_guard.main(["--root", str(root), "--max-depth", "1"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "module.py" in captured.err
+
+
+def test_calculate_depth_visited_passed():
+    """Test calculate_depth with pre-populated visited set."""
+    hierarchy = {"A": [], "B": ["A"]}
+    visited = {"A"}
+    depth = inheritance_guard.calculate_depth("B", hierarchy, visited)
+    assert depth >= 0
diff --git a/tests/test_messaging.py b/tests/test_messaging.py
new file mode 100644
index 0000000..a1f908a
--- /dev/null
+++ b/tests/test_messaging.py
@@ -0,0 +1,505 @@
+"""Unit tests for ci_tools.ci_runtime.messaging module."""
+
+from __future__ import annotations
+
+import subprocess
+from unittest.mock import Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.messaging import (
+    request_commit_message,
+    commit_and_push,
+)
+from ci_tools.ci_runtime.models import (
+    CommitMessageError,
+    GitCommandAbort,
+)
+
+
+class TestRequestCommitMessage:
+    """Tests for request_commit_message function."""
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_simple_commit_message(self, mock_invoke):
+        """Test generating a simple single-line commit message."""
+        mock_invoke.return_value = "Fixed authentication bug"
+
+        summary, body_lines = request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff content",
+            extra_context="",
+            detailed=False,
+        )
+
+        assert summary == "Fixed authentication bug"
+        assert body_lines == []
+        mock_invoke.assert_called_once()
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_detailed_commit_message(self, mock_invoke):
+        """Test generating a detailed commit message with body."""
+        response = """Updated user authentication system
+
+- Added JWT token validation
+- Implemented password hashing with bcrypt
+- Updated login endpoint to return refresh tokens"""
+        mock_invoke.return_value = response
+
+        summary, body_lines = request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="medium",
+            staged_diff="diff content",
+            extra_context="",
+            detailed=True,
+        )
+
+        assert summary == "Updated user authentication system"
+        # Leading blank lines are removed, so first body line is content
+        assert len(body_lines) >= 3
+        assert "JWT token" in "\n".join(body_lines)
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_commit_message_with_extra_context(self, mock_invoke):
+        """Test commit message generation with extra context."""
+        mock_invoke.return_value = "Added new feature"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="low",
+            staged_diff="diff content",
+            extra_context="This fixes issue #123",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "This fixes issue #123" in prompt
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_commit_message_includes_model_config(self, mock_invoke):
+        """Test that prompt includes model configuration."""
+        mock_invoke.return_value = "Fixed bug"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff",
+            extra_context="",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "gpt-5-codex" in prompt
+        assert "high" in prompt
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_commit_message_with_none_reasoning_effort(self, mock_invoke):
+        """Test commit message with None reasoning effort."""
+        mock_invoke.return_value = "Fixed bug"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort=None,
+            staged_diff="diff",
+            extra_context="",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "default" in prompt
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_empty_response_raises_error(self, mock_invoke):
+        """Test that empty response raises CommitMessageError."""
+        mock_invoke.return_value = ""
+
+        with pytest.raises(CommitMessageError):
+            request_commit_message(
+                model="gpt-5-codex",
+                reasoning_effort="high",
+                staged_diff="diff",
+                extra_context="",
+                detailed=False,
+            )
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_whitespace_only_response_raises_error(self, mock_invoke):
+        """Test that whitespace-only response raises error."""
+        mock_invoke.return_value = "   \n  \n  "
+
+        with pytest.raises(CommitMessageError):
+            request_commit_message(
+                model="gpt-5-codex",
+                reasoning_effort="high",
+                staged_diff="diff",
+                extra_context="",
+                detailed=False,
+            )
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_strips_trailing_whitespace_from_lines(self, mock_invoke):
+        """Test that trailing whitespace is stripped from lines."""
+        response = "Fixed bug  \n  \n- Detail line   \n- Another detail  "
+        mock_invoke.return_value = response
+
+        summary, body_lines = request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff",
+            extra_context="",
+            detailed=True,
+        )
+
+        assert summary == "Fixed bug"
+        assert all(not line.endswith(" ") for line in body_lines if line)
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_removes_leading_blank_lines_from_body(self, mock_invoke):
+        """Test that leading blank lines are removed from body."""
+        response = "Summary\n\n\n\n- First bullet"
+        mock_invoke.return_value = response
+
+        summary, body_lines = request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff",
+            extra_context="",
+            detailed=True,
+        )
+
+        assert summary == "Summary"
+        # All leading blank lines are removed
+        assert body_lines[0] == "- First bullet"
+        assert "- First bullet" in body_lines
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_detailed_prompt_includes_instructions(self, mock_invoke):
+        """Test that detailed prompt includes specific instructions."""
+        mock_invoke.return_value = "Summary"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff",
+            extra_context="",
+            detailed=True,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "bullet" in prompt.lower()
+        assert "72" in prompt  # character limit
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_simple_prompt_avoids_shell_commands(self, mock_invoke):
+        """Test that prompt warns against running shell commands."""
+        mock_invoke.return_value = "Summary"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="diff",
+            extra_context="",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "diff --git" in prompt  # Warning about not using this command
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_passes_staged_diff_to_prompt(self, mock_invoke):
+        """Test that staged diff is included in prompt."""
+        mock_invoke.return_value = "Summary"
+        diff = "--- a/file.py\n+++ b/file.py"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff=diff,
+            extra_context="",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert diff in prompt
+
+    @patch("ci_tools.ci_runtime.messaging.invoke_codex")
+    def test_handles_empty_staged_diff(self, mock_invoke):
+        """Test handling of empty staged diff."""
+        mock_invoke.return_value = "Summary"
+
+        request_commit_message(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="",
+            extra_context="",
+            detailed=False,
+        )
+
+        call_args = mock_invoke.call_args
+        prompt = call_args[0][0]
+        assert "no staged diff" in prompt
+
+
+class TestCommitAndPush:
+    """Tests for commit_and_push function."""
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_without_push(self, mock_run):
+        """Test creating commit without pushing."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        commit_and_push("Summary line", [], push=False)
+
+        # Should only call git commit once
+        assert mock_run.call_count == 1
+        call_args = mock_run.call_args_list[0][0][0]
+        assert call_args[0] == "git"
+        assert call_args[1] == "commit"
+        assert "Summary line" in call_args
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_with_body(self, mock_run):
+        """Test creating commit with body text."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        body_lines = ["", "- Detail 1", "- Detail 2"]
+        commit_and_push("Summary", body_lines, push=False)
+
+        call_args = mock_run.call_args_list[0][0][0]
+        assert "Summary" in call_args
+        # Body should be included
+        assert "-m" in call_args
+        body_index = call_args.index("-m")
+        # Find the second -m (for body)
+        second_m_index = call_args.index("-m", body_index + 1)
+        body_text = call_args[second_m_index + 1]
+        assert "Detail 1" in body_text
+        assert "Detail 2" in body_text
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_without_body(self, mock_run):
+        """Test creating commit without body text."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        commit_and_push("Summary", [], push=False)
+
+        call_args = mock_run.call_args_list[0][0][0]
+        # Should only have one -m flag
+        m_count = call_args.count("-m")
+        assert m_count == 1
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_with_empty_body_lines(self, mock_run):
+        """Test that empty body lines are handled correctly."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        body_lines = ["", "  ", ""]
+        commit_and_push("Summary", body_lines, push=False)
+
+        call_args = mock_run.call_args_list[0][0][0]
+        # Should only have one -m since body is whitespace only
+        m_count = call_args.count("-m")
+        assert m_count == 1
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_and_push_to_origin(self, mock_run):
+        """Test committing and pushing to origin."""
+        mock_run.return_value = Mock(returncode=0, stdout="main\n", stderr="")
+
+        commit_and_push("Summary", [], push=True)
+
+        assert mock_run.call_count == 3
+        # First call: git commit
+        assert mock_run.call_args_list[0][0][0][1] == "commit"
+        # Second call: git rev-parse to get branch
+        assert "rev-parse" in mock_run.call_args_list[1][0][0]
+        # Third call: git push
+        push_args = mock_run.call_args_list[2][0][0]
+        assert push_args[0] == "git"
+        assert push_args[1] == "push"
+        assert push_args[2] == "origin"
+        assert push_args[3] == "main"
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    @patch.dict("os.environ", {"GIT_REMOTE": "upstream"})
+    def test_commit_and_push_to_custom_remote(self, mock_run):
+        """Test pushing to custom remote from environment."""
+        mock_run.return_value = Mock(returncode=0, stdout="feature\n", stderr="")
+
+        commit_and_push("Summary", [], push=True)
+
+        # Check push command uses custom remote
+        push_args = mock_run.call_args_list[2][0][0]
+        assert "upstream" in push_args
+        assert "feature" in push_args
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_failure_raises_abort(self, mock_run):
+        """Test that commit failure raises GitCommandAbort."""
+        mock_run.side_effect = subprocess.CalledProcessError(
+            1, ["git", "commit"], output="", stderr="commit failed"
+        )
+
+        with pytest.raises(GitCommandAbort) as exc_info:
+            commit_and_push("Summary", [], push=False)
+
+        assert "commit" in str(exc_info.value).lower()
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_push_failure_raises_abort(self, mock_run):
+        """Test that push failure raises GitCommandAbort."""
+        # First call (commit) succeeds, second (rev-parse) succeeds, third (push) fails
+        mock_run.side_effect = [
+            Mock(returncode=0, stdout="", stderr=""),  # commit success
+            Mock(returncode=0, stdout="main\n", stderr=""),  # branch name
+            subprocess.CalledProcessError(
+                1, ["git", "push"], output="", stderr="push failed"
+            ),
+        ]
+
+        with pytest.raises(GitCommandAbort) as exc_info:
+            commit_and_push("Summary", [], push=True)
+
+        assert "push" in str(exc_info.value).lower()
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_prints_info_messages(self, mock_run, capsys):
+        """Test that informational messages are printed."""
+        mock_run.return_value = Mock(returncode=0, stdout="main\n", stderr="")
+
+        commit_and_push("Summary", [], push=True)
+
+        captured = capsys.readouterr()
+        assert "Creating commit" in captured.out
+        assert "Pushing" in captured.out
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_uses_check_true(self, mock_run):
+        """Test that commit command uses check=True."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        commit_and_push("Summary", [], push=False)
+
+        # Check that check=True was passed
+        kwargs = mock_run.call_args_list[0][1]
+        assert kwargs.get("check") is True
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_commit_uses_live_output(self, mock_run):
+        """Test that commit command uses live=True for output."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        commit_and_push("Summary", [], push=False)
+
+        # Check that live=True was passed
+        kwargs = mock_run.call_args_list[0][1]
+        assert kwargs.get("live") is True
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_push_uses_live_output(self, mock_run):
+        """Test that push command uses live=True for output."""
+        mock_run.return_value = Mock(returncode=0, stdout="main\n", stderr="")
+
+        commit_and_push("Summary", [], push=True)
+
+        # Check that push command uses live=True
+        kwargs = mock_run.call_args_list[2][1]
+        assert kwargs.get("live") is True
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_branch_detection_strips_whitespace(self, mock_run):
+        """Test that branch name is stripped of whitespace."""
+        mock_run.side_effect = [
+            Mock(returncode=0, stdout="", stderr=""),  # commit
+            Mock(returncode=0, stdout="  feature-branch  \n", stderr=""),  # branch
+            Mock(returncode=0, stdout="", stderr=""),  # push
+        ]
+
+        commit_and_push("Summary", [], push=True)
+
+        # Check that branch name was stripped
+        push_args = mock_run.call_args_list[2][0][0]
+        assert "feature-branch" in push_args
+        # Verify no whitespace in branch name
+        branch_arg = push_args[3]
+        assert branch_arg == "feature-branch"
+
+    @patch("ci_tools.ci_runtime.messaging.run_command")
+    def test_body_with_multiple_paragraphs(self, mock_run):
+        """Test commit body with multiple paragraphs."""
+        mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+
+        body_lines = [
+            "",
+            "First paragraph with details.",
+            "",
+            "Second paragraph with more info.",
+            "- Bullet point",
+        ]
+        commit_and_push("Summary", body_lines, push=False)
+
+        call_args = mock_run.call_args_list[0][0][0]
+        # Find body text
+        m_indices = [i for i, x in enumerate(call_args) if x == "-m"]
+        assert len(m_indices) == 2
+        body_text = call_args[m_indices[1] + 1]
+        assert "First paragraph" in body_text
+        assert "Second paragraph" in body_text
+        assert "Bullet point" in body_text
+
+
+class TestCommitMessageErrorFactory:
+    """Tests for CommitMessageError factory methods."""
+
+    def test_empty_response_factory(self):
+        """Test empty_response factory method."""
+        error = CommitMessageError.empty_response()
+        assert isinstance(error, CommitMessageError)
+        assert "empty" in str(error).lower()
+
+
+class TestGitCommandAbortFactories:
+    """Tests for GitCommandAbort factory methods."""
+
+    def test_commit_failed_factory(self):
+        """Test commit_failed factory method."""
+        exc = subprocess.CalledProcessError(
+            1, ["git", "commit"], output="", stderr="nothing to commit"
+        )
+        error = GitCommandAbort.commit_failed(exc)
+        assert isinstance(error, GitCommandAbort)
+        assert "commit" in str(error).lower()
+        assert "status 1" in str(error)
+
+    def test_push_failed_factory(self):
+        """Test push_failed factory method."""
+        exc = subprocess.CalledProcessError(
+            128, ["git", "push"], output="", stderr="failed to push"
+        )
+        error = GitCommandAbort.push_failed(exc)
+        assert isinstance(error, GitCommandAbort)
+        assert "push" in str(error).lower()
+        assert "status 128" in str(error)
+
+    def test_commit_failed_includes_stderr(self):
+        """Test that stderr is included in commit error."""
+        exc = subprocess.CalledProcessError(
+            1, ["git", "commit"], output="", stderr="pre-commit hook failed"
+        )
+        error = GitCommandAbort.commit_failed(exc)
+        assert "pre-commit hook failed" in str(error)
+
+    def test_push_failed_includes_stderr(self):
+        """Test that stderr is included in push error."""
+        exc = subprocess.CalledProcessError(
+            1, ["git", "push"], output="", stderr="authentication failed"
+        )
+        error = GitCommandAbort.push_failed(exc)
+        assert "authentication failed" in str(error)
diff --git a/tests/test_method_count_guard.py b/tests/test_method_count_guard.py
new file mode 100644
index 0000000..a866384
--- /dev/null
+++ b/tests/test_method_count_guard.py
@@ -0,0 +1,579 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import method_count_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = method_count_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_public_methods == 15
+    assert args.max_total_methods == 25
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = method_count_guard.parse_args(
+        [
+            "--root",
+            "custom",
+            "--max-public-methods",
+            "10",
+            "--max-total-methods",
+            "20",
+            "--exclude",
+            "tests",
+        ]
+    )
+    assert args.root == Path("custom")
+    assert args.max_public_methods == 10
+    assert args.max_total_methods == 20
+    assert args.exclude == [Path("tests")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(method_count_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(method_count_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert method_count_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert method_count_guard.is_excluded(path, exclusions) is False
+
+
+def test_count_methods_basic():
+    """Test counting methods in a basic class."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def method1(self):
+                pass
+
+            def method2(self):
+                pass
+
+            def _private_method(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 2  # method1, method2
+    assert total_count == 3  # method1, method2, _private_method
+
+
+def test_count_methods_excludes_dunder():
+    """Test that dunder methods are excluded from counts."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def __init__(self):
+                pass
+
+            def __str__(self):
+                pass
+
+            def method(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 1  # Only method
+    assert total_count == 1
+
+
+def test_count_methods_excludes_properties():
+    """Test that properties are excluded from counts."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            @property
+            def prop(self):
+                return self._value
+
+            def method(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 1  # Only method
+    assert total_count == 1
+
+
+def test_count_methods_private_methods():
+    """Test counting private methods."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def public_method(self):
+                pass
+
+            def _private_method(self):
+                pass
+
+            def __private_name_mangled(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 1  # Only public_method
+    assert total_count == 2  # public_method, _private_method (not __private_name_mangled)
+
+
+def test_count_methods_no_methods():
+    """Test counting methods in class with no methods."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            x = 1
+            y = 2
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 0
+    assert total_count == 0
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the method count limit."""
+    py_file = tmp_path / "small.py"
+    write_module(
+        py_file,
+        """
+        class SmallClass:
+            def method1(self):
+                pass
+
+            def method2(self):
+                pass
+        """,
+    )
+
+    violations = method_count_guard.scan_file(py_file, max_public=5, max_total=10)
+    assert len(violations) == 0
+
+
+def test_scan_file_exceeds_public_limit(tmp_path: Path):
+    """Test scanning a file that exceeds public method limit."""
+    py_file = tmp_path / "many_public.py"
+    methods = "\n".join([f"    def method_{i}(self):\n        pass" for i in range(20)])
+    content = f"class ManyMethods:\n{methods}"
+    py_file.write_text(content)
+
+    violations = method_count_guard.scan_file(py_file, max_public=10, max_total=30)
+    assert len(violations) == 1
+    assert violations[0][1] == "ManyMethods"
+    assert violations[0][3] == 20  # public_count
+
+
+def test_scan_file_exceeds_total_limit(tmp_path: Path):
+    """Test scanning a file that exceeds total method limit."""
+    py_file = tmp_path / "many_total.py"
+    methods = "\n".join([f"    def _method_{i}(self):\n        pass" for i in range(30)])
+    content = f"class ManyMethods:\n{methods}"
+    py_file.write_text(content)
+
+    violations = method_count_guard.scan_file(py_file, max_public=25, max_total=20)
+    assert len(violations) == 1
+    assert violations[0][1] == "ManyMethods"
+    assert violations[0][4] == 30  # total_count
+
+
+def test_scan_file_syntax_error(tmp_path: Path):
+    """Test scan_file with syntax error."""
+    py_file = tmp_path / "bad.py"
+    py_file.write_text("class Foo:\n    def method(self\n")
+
+    with pytest.raises(RuntimeError, match="failed to parse Python source"):
+        method_count_guard.scan_file(py_file, max_public=10, max_total=20)
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "small.py",
+        """
+        class SmallClass:
+            def method1(self):
+                pass
+
+            def method2(self):
+                pass
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(
+            ["--root", str(root), "--max-public-methods", "5", "--max-total-methods", "10"]
+        )
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    py_file = root / "many.py"
+
+    methods = "\n".join([f"    def method_{i}(self):\n        pass" for i in range(20)])
+    content = f"class ManyMethods:\n{methods}"
+    py_file.write_text(content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(
+            ["--root", str(root), "--max-public-methods", "5", "--max-total-methods", "10"]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "too many methods" in captured.err
+    assert "ManyMethods" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    many_methods = "class ManyMethods:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "included.py").write_text(many_methods)
+    (excluded / "excluded.py").write_text(many_methods)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(
+            [
+                "--root",
+                str(root),
+                "--max-public-methods",
+                "5",
+                "--max-total-methods",
+                "10",
+                "--exclude",
+                str(excluded),
+            ]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    many_methods = "class ManyMethods:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "zebra.py").write_text(many_methods)
+    (root / "alpha.py").write_text(many_methods)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(
+            ["--root", str(root), "--max-public-methods", "5", "--max-total-methods", "10"]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [
+        line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line
+    ]
+    assert len(err_lines) >= 2
+
+
+def test_format_method_violation_public():
+    """Test formatting violation message for public methods."""
+    violation = method_count_guard._format_method_violation(
+        Path("/project/src/module.py"),
+        class_name="BigClass",
+        lineno=10,
+        public_count=20,
+        total_count=25,
+        max_public=15,
+        max_total=30,
+        repo_root=Path("/project"),
+    )
+
+    assert "module.py:10" in violation or "module.py" in violation
+    assert "BigClass" in violation
+    assert "20 public methods" in violation
+    assert "limit 15" in violation
+
+
+def test_format_method_violation_total():
+    """Test formatting violation message for total methods."""
+    violation = method_count_guard._format_method_violation(
+        Path("/project/src/module.py"),
+        class_name="BigClass",
+        lineno=10,
+        public_count=10,
+        total_count=30,
+        max_public=15,
+        max_total=25,
+        repo_root=Path("/project"),
+    )
+
+    assert "BigClass" in violation
+    assert "30 total methods" in violation
+    assert "limit 25" in violation
+
+
+def test_format_method_violation_both():
+    """Test formatting violation message for both limits exceeded."""
+    violation = method_count_guard._format_method_violation(
+        Path("/project/src/module.py"),
+        class_name="BigClass",
+        lineno=10,
+        public_count=20,
+        total_count=30,
+        max_public=15,
+        max_total=25,
+        repo_root=Path("/project"),
+    )
+
+    assert "BigClass" in violation
+    assert "20 public methods" in violation
+    assert "30 total methods" in violation
+
+
+def test_collect_method_violations(tmp_path: Path):
+    """Test collecting method violations."""
+    py_file = tmp_path / "test.py"
+    methods = "\n".join([f"    def method_{i}(self):\n        pass" for i in range(20)])
+    content = f"class BigClass:\n{methods}"
+    py_file.write_text(content)
+
+    violations = method_count_guard._collect_method_violations(
+        py_file, max_public=5, max_total=10, repo_root=tmp_path
+    )
+
+    assert len(violations) == 1
+    assert "BigClass" in violations[0]
+
+
+def test_print_method_report(capsys: pytest.CaptureFixture):
+    """Test printing method count report."""
+    violations = ["violation1", "violation2"]
+    method_count_guard._print_method_report(violations)
+
+    captured = capsys.readouterr()
+    assert "too many methods" in captured.err
+    assert "multi-concern indicator" in captured.err
+    assert "service objects" in captured.err
+    assert "violation1" in captured.err
+    assert "violation2" in captured.err
+
+
+def test_main_traverse_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles traversal errors."""
+    missing = tmp_path / "missing"
+
+    result = method_count_guard.main(["--root", str(missing)])
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to traverse" in captured.err
+
+
+def test_main_scan_file_error(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles scan_file errors."""
+    root = tmp_path / "src"
+    root.mkdir()
+    (root / "bad.py").write_text("class Foo:\n    def method(self\n")
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(["--root", str(root)])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "failed to parse" in captured.err
+
+
+def test_scan_file_multiple_classes(tmp_path: Path):
+    """Test scanning file with multiple classes."""
+    py_file = tmp_path / "multi.py"
+    write_module(
+        py_file,
+        """
+        class SmallClass:
+            def method(self):
+                pass
+
+        class BigClass:
+            def method1(self):
+                pass
+            def method2(self):
+                pass
+            def method3(self):
+                pass
+            def method4(self):
+                pass
+            def method5(self):
+                pass
+            def method6(self):
+                pass
+        """,
+    )
+
+    violations = method_count_guard.scan_file(py_file, max_public=3, max_total=5)
+    assert len(violations) == 1
+    assert violations[0][1] == "BigClass"
+
+
+def test_count_methods_mixed_decorators():
+    """Test counting methods with various decorators."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            @property
+            def prop1(self):
+                return 1
+
+            @staticmethod
+            def static():
+                pass
+
+            @classmethod
+            def cls(cls):
+                pass
+
+            def regular(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    # Should count static, cls, and regular, but not prop1
+    assert total_count == 3
+
+
+def test_scan_file_no_classes(tmp_path: Path):
+    """Test scanning file with no classes."""
+    py_file = tmp_path / "no_classes.py"
+    write_module(
+        py_file,
+        """
+        def function():
+            pass
+        """,
+    )
+
+    violations = method_count_guard.scan_file(py_file, max_public=5, max_total=10)
+    assert len(violations) == 0
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    many_methods = "class ManyMethods:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "module.py").write_text(many_methods)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = method_count_guard.main(
+            ["--root", str(root), "--max-public-methods", "5", "--max-total-methods", "10"]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "module.py" in captured.err
+
+
+def test_count_methods_only_dunder():
+    """Test class with only dunder methods."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def __init__(self):
+                pass
+
+            def __str__(self):
+                pass
+
+            def __repr__(self):
+                pass
+        """
+    ).strip()
+
+    tree = method_count_guard.ast.parse(source)
+    class_node = tree.body[0]
+    public_count, total_count = method_count_guard.count_methods(class_node)
+
+    assert public_count == 0
+    assert total_count == 0
+
+
+def test_iter_python_files_empty_directory(tmp_path: Path):
+    """Test iter_python_files with empty directory."""
+    files = list(method_count_guard.iter_python_files(tmp_path))
+    assert len(files) == 0
diff --git a/tests/test_models.py b/tests/test_models.py
new file mode 100644
index 0000000..d164fbd
--- /dev/null
+++ b/tests/test_models.py
@@ -0,0 +1,473 @@
+"""Unit tests for ci_tools.ci_runtime.models module."""
+
+from __future__ import annotations
+
+import subprocess
+
+import pytest
+
+from ci_tools.ci_runtime.models import (
+    CiAbort,
+    CiError,
+    CodexCliError,
+    CommandResult,
+    CommitMessageError,
+    CoverageCheckResult,
+    CoverageDeficit,
+    FailureContext,
+    GitCommandAbort,
+    ModelSelectionAbort,
+    PatchApplyError,
+    PatchAttemptState,
+    PatchLifecycleAbort,
+    PatchPrompt,
+    ReasoningEffortAbort,
+    RepositoryStateAbort,
+    RuntimeOptions,
+)
+
+
+class TestCiError:
+    """Tests for CiError base exception."""
+
+    def test_default_message_no_detail(self):
+        """Test CiError with no detail uses default message."""
+        error = CiError()
+        assert str(error) == "CI automation failure"
+        assert error.detail is None
+
+    def test_with_detail(self):
+        """Test CiError with detail appends it to message."""
+        error = CiError(detail="something went wrong")
+        assert str(error) == "CI automation failure: something went wrong"
+        assert error.detail == "something went wrong"
+
+    def test_is_runtime_error(self):
+        """Test CiError is a RuntimeError subclass."""
+        error = CiError()
+        assert isinstance(error, RuntimeError)
+
+
+class TestCodexCliError:
+    """Tests for CodexCliError exception."""
+
+    def test_exit_status_with_output(self):
+        """Test factory method with exit status and output."""
+        error = CodexCliError.exit_status(returncode=127, output="command not found")
+        assert "exit status 127" in str(error)
+        assert "command not found" in str(error)
+
+    def test_exit_status_without_output(self):
+        """Test factory method with no output."""
+        error = CodexCliError.exit_status(returncode=1, output=None)
+        assert "exit status 1" in str(error)
+        assert "(no output)" in str(error)
+
+    def test_exit_status_empty_output(self):
+        """Test factory method with empty string output."""
+        error = CodexCliError.exit_status(returncode=2, output="   ")
+        assert "exit status 2" in str(error)
+        assert "(no output)" in str(error)
+
+
+class TestCommitMessageError:
+    """Tests for CommitMessageError exception."""
+
+    def test_empty_response_factory(self):
+        """Test empty_response factory method."""
+        error = CommitMessageError.empty_response()
+        assert "Commit message response was empty" in str(error)
+        assert error.detail is None
+
+
+class TestCiAbort:
+    """Tests for CiAbort base exception."""
+
+    def test_default_exit_code(self):
+        """Test CiAbort defaults to exit code 1."""
+        abort = CiAbort()
+        assert abort.exit_code == 1
+        assert abort.code == 1
+
+    def test_custom_exit_code(self):
+        """Test CiAbort with custom exit code."""
+        abort = CiAbort(code=42)
+        assert abort.exit_code == 42
+        assert abort.code == 42
+
+    def test_with_detail(self):
+        """Test CiAbort with detail message."""
+        abort = CiAbort(detail="user cancelled")
+        assert "user cancelled" in str(abort)
+        assert abort.detail == "user cancelled"
+
+    def test_is_system_exit(self):
+        """Test CiAbort is a SystemExit subclass."""
+        abort = CiAbort()
+        assert isinstance(abort, SystemExit)
+
+
+class TestGitCommandAbort:
+    """Tests for GitCommandAbort exception."""
+
+    def test_commit_failed_with_stderr(self):
+        """Test commit_failed factory with stderr output."""
+        exc = subprocess.CalledProcessError(
+            returncode=1,
+            cmd=["git", "commit"],
+            stderr="fatal: not a git repository",
+        )
+        abort = GitCommandAbort.commit_failed(exc)
+        assert "git commit" in str(abort)
+        assert "status 1" in str(abort)
+        assert "fatal: not a git repository" in str(abort)
+
+    def test_commit_failed_with_output(self):
+        """Test commit_failed factory with output instead of stderr."""
+        exc = subprocess.CalledProcessError(
+            returncode=1,
+            cmd=["git", "commit"],
+            output="nothing to commit",
+        )
+        abort = GitCommandAbort.commit_failed(exc)
+        assert "nothing to commit" in str(abort)
+
+    def test_commit_failed_no_output(self):
+        """Test commit_failed factory with no output."""
+        exc = subprocess.CalledProcessError(returncode=1, cmd=["git", "commit"])
+        abort = GitCommandAbort.commit_failed(exc)
+        assert "git commit" in str(abort)
+        assert "status 1" in str(abort)
+
+    def test_push_failed_with_stderr(self):
+        """Test push_failed factory with stderr."""
+        exc = subprocess.CalledProcessError(
+            returncode=128,
+            cmd=["git", "push"],
+            stderr="error: failed to push",
+        )
+        abort = GitCommandAbort.push_failed(exc)
+        assert "git push" in str(abort)
+        assert "status 128" in str(abort)
+        assert "failed to push" in str(abort)
+
+
+class TestRepositoryStateAbort:
+    """Tests for RepositoryStateAbort exception."""
+
+    def test_detached_head_factory(self):
+        """Test detached_head factory method."""
+        abort = RepositoryStateAbort.detached_head()
+        assert "detached HEAD detected" in str(abort)
+        assert "checkout a branch" in str(abort)
+
+
+class TestModelSelectionAbort:
+    """Tests for ModelSelectionAbort exception."""
+
+    def test_unsupported_model_factory(self):
+        """Test unsupported_model factory method."""
+        abort = ModelSelectionAbort.unsupported_model(
+            received="gpt-4", required="gpt-5-codex"
+        )
+        assert "gpt-5-codex" in str(abort)
+        assert "gpt-4" in str(abort)
+
+
+class TestReasoningEffortAbort:
+    """Tests for ReasoningEffortAbort exception."""
+
+    def test_unsupported_choice_factory(self):
+        """Test unsupported_choice factory method."""
+        abort = ReasoningEffortAbort.unsupported_choice(
+            received="extreme", allowed=["low", "medium", "high"]
+        )
+        assert "extreme" in str(abort)
+        assert "low, medium, high" in str(abort)
+
+
+class TestPatchLifecycleAbort:
+    """Tests for PatchLifecycleAbort exception."""
+
+    def test_attempts_exhausted(self):
+        """Test attempts_exhausted factory method."""
+        abort = PatchLifecycleAbort.attempts_exhausted()
+        assert "unable to obtain a valid patch" in str(abort)
+
+    def test_missing_patch(self):
+        """Test missing_patch factory method."""
+        abort = PatchLifecycleAbort.missing_patch()
+        assert "empty or NOOP patch" in str(abort)
+
+    def test_user_declined(self):
+        """Test user_declined factory method."""
+        abort = PatchLifecycleAbort.user_declined()
+        assert "user declined" in str(abort)
+
+    def test_retries_exhausted(self):
+        """Test retries_exhausted factory method."""
+        abort = PatchLifecycleAbort.retries_exhausted()
+        assert "exhausting retries" in str(abort)
+        assert "manual review required" in str(abort)
+
+
+class TestCommandResult:
+    """Tests for CommandResult dataclass."""
+
+    def test_ok_property_success(self):
+        """Test ok property returns True for zero exit code."""
+        result = CommandResult(returncode=0, stdout="success", stderr="")
+        assert result.ok is True
+
+    def test_ok_property_failure(self):
+        """Test ok property returns False for non-zero exit code."""
+        result = CommandResult(returncode=1, stdout="", stderr="error")
+        assert result.ok is False
+
+    def test_combined_output(self):
+        """Test combined_output concatenates stdout and stderr."""
+        result = CommandResult(returncode=0, stdout="hello\n", stderr="world\n")
+        assert result.combined_output == "hello\nworld\n"
+
+    def test_combined_output_empty(self):
+        """Test combined_output with empty streams."""
+        result = CommandResult(returncode=0, stdout="", stderr="")
+        assert result.combined_output == ""
+
+
+class TestRuntimeOptions:
+    """Tests for RuntimeOptions dataclass."""
+
+    def test_creation_with_all_fields(self):
+        """Test RuntimeOptions with all fields."""
+        options = RuntimeOptions(
+            command_tokens=["make", "test"],
+            command_env={"FOO": "bar"},
+            patch_approval_mode="prompt",
+            automation_mode=True,
+            auto_stage_enabled=False,
+            commit_message_enabled=True,
+            auto_push_enabled=False,
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+        )
+        assert options.command_tokens == ["make", "test"]
+        assert options.command_env == {"FOO": "bar"}
+        assert options.patch_approval_mode == "prompt"
+        assert options.automation_mode is True
+        assert options.auto_stage_enabled is False
+        assert options.commit_message_enabled is True
+        assert options.auto_push_enabled is False
+        assert options.model_name == "gpt-5-codex"
+        assert options.reasoning_effort == "high"
+
+
+class TestFailureContext:
+    """Tests for FailureContext dataclass."""
+
+    def test_creation_without_coverage_report(self):
+        """Test FailureContext creation without coverage report."""
+        context = FailureContext(
+            log_excerpt="Error: test failed",
+            summary="Tests failed in module X",
+            implicated_files=["src/module.py"],
+            focused_diff="diff --git...",
+            coverage_report=None,
+        )
+        assert context.log_excerpt == "Error: test failed"
+        assert context.summary == "Tests failed in module X"
+        assert context.implicated_files == ["src/module.py"]
+        assert context.focused_diff == "diff --git..."
+        assert context.coverage_report is None
+
+    def test_creation_with_coverage_report(self):
+        """Test FailureContext with coverage report."""
+        coverage = CoverageCheckResult(
+            table_text="coverage table", deficits=[], threshold=80.0
+        )
+        context = FailureContext(
+            log_excerpt="",
+            summary="",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=coverage,
+        )
+        assert context.coverage_report is not None
+        assert context.coverage_report.threshold == 80.0
+
+
+class TestPatchAttemptState:
+    """Tests for PatchAttemptState dataclass."""
+
+    def test_initial_state(self):
+        """Test initial state of PatchAttemptState."""
+        state = PatchAttemptState(max_attempts=5)
+        assert state.max_attempts == 5
+        assert state.patch_attempt == 1
+        assert state.extra_retry_budget == 3
+        assert state.last_error is None
+
+    def test_ensure_budget_success(self):
+        """Test ensure_budget when within budget."""
+        state = PatchAttemptState(max_attempts=5, patch_attempt=3)
+        state.ensure_budget()  # Should not raise
+
+    def test_ensure_budget_exhausted(self):
+        """Test ensure_budget raises when budget exceeded."""
+        state = PatchAttemptState(max_attempts=5, patch_attempt=6)
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            state.ensure_budget()
+        assert "unable to obtain a valid patch" in str(exc_info.value)
+
+    def test_record_failure_increments_attempt(self):
+        """Test record_failure increments patch_attempt."""
+        state = PatchAttemptState(max_attempts=5)
+        state.record_failure("test error", retryable=True)
+        assert state.patch_attempt == 2
+        assert state.last_error == "test error"
+
+    def test_record_failure_uses_retry_budget(self):
+        """Test record_failure uses extra retry budget when at max."""
+        state = PatchAttemptState(max_attempts=3, patch_attempt=3)
+        state.record_failure("error 1", retryable=True)
+        assert state.max_attempts == 4
+        assert state.extra_retry_budget == 2
+        assert state.patch_attempt == 4
+
+    def test_record_failure_exhausts_retries(self):
+        """Test record_failure raises when retries exhausted."""
+        state = PatchAttemptState(
+            max_attempts=3, patch_attempt=3, extra_retry_budget=0
+        )
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            state.record_failure("final error", retryable=True)
+        assert "exhausting retries" in str(exc_info.value)
+
+    def test_record_failure_non_retryable(self):
+        """Test record_failure with non-retryable error."""
+        state = PatchAttemptState(max_attempts=3, patch_attempt=3)
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            state.record_failure("fatal error", retryable=False)
+        assert "exhausting retries" in str(exc_info.value)
+
+
+class TestPatchApplyError:
+    """Tests for PatchApplyError exception."""
+
+    def test_git_apply_failed_factory(self):
+        """Test git_apply_failed factory method."""
+        error = PatchApplyError.git_apply_failed(output="error: patch failed")
+        assert "git apply" in str(error)
+        assert "patch failed" in str(error)
+        assert error.retryable is True
+
+    def test_git_apply_failed_no_output(self):
+        """Test git_apply_failed with no output."""
+        error = PatchApplyError.git_apply_failed(output="")
+        assert "(no output)" in str(error)
+
+    def test_preflight_failed_factory(self):
+        """Test preflight_failed factory method."""
+        error = PatchApplyError.preflight_failed(
+            check_output="git check failed", dry_output="patch dry run failed"
+        )
+        assert "Patch dry-run failed" in str(error)
+        assert "git check failed" in str(error)
+        assert "patch dry run failed" in str(error)
+        assert error.retryable is True
+
+    def test_patch_exit_factory(self):
+        """Test patch_exit factory method."""
+        error = PatchApplyError.patch_exit(returncode=1, output="malformed patch")
+        assert "exited with status 1" in str(error)
+        assert "malformed patch" in str(error)
+        assert error.retryable is True
+
+    def test_custom_retryable_flag(self):
+        """Test PatchApplyError with custom retryable flag."""
+        error = PatchApplyError(detail="fatal", retryable=False)
+        assert error.retryable is False
+
+
+class TestCoverageDeficit:
+    """Tests for CoverageDeficit dataclass."""
+
+    def test_creation(self):
+        """Test CoverageDeficit creation."""
+        deficit = CoverageDeficit(path="src/module.py", coverage=65.5)
+        assert deficit.path == "src/module.py"
+        assert deficit.coverage == 65.5
+
+
+class TestCoverageCheckResult:
+    """Tests for CoverageCheckResult dataclass."""
+
+    def test_creation_with_deficits(self):
+        """Test CoverageCheckResult with deficits."""
+        deficits = [
+            CoverageDeficit(path="src/a.py", coverage=70.0),
+            CoverageDeficit(path="src/b.py", coverage=75.5),
+        ]
+        result = CoverageCheckResult(
+            table_text="Name | Stmts | Miss | Cover\n",
+            deficits=deficits,
+            threshold=80.0,
+        )
+        assert result.table_text == "Name | Stmts | Miss | Cover\n"
+        assert len(result.deficits) == 2
+        assert result.threshold == 80.0
+
+    def test_creation_no_deficits(self):
+        """Test CoverageCheckResult with no deficits."""
+        result = CoverageCheckResult(table_text="", deficits=[], threshold=80.0)
+        assert result.deficits == []
+
+
+class TestPatchPrompt:
+    """Tests for PatchPrompt dataclass."""
+
+    def test_creation_with_all_fields(self):
+        """Test PatchPrompt with all fields."""
+        context = FailureContext(
+            log_excerpt="error log",
+            summary="summary",
+            implicated_files=["file.py"],
+            focused_diff="diff",
+            coverage_report=None,
+        )
+        prompt = PatchPrompt(
+            command="make test",
+            failure_context=context,
+            git_diff="diff output",
+            git_status="M file.py",
+            iteration=2,
+            patch_error="previous patch failed",
+            attempt=3,
+        )
+        assert prompt.command == "make test"
+        assert prompt.failure_context == context
+        assert prompt.git_diff == "diff output"
+        assert prompt.git_status == "M file.py"
+        assert prompt.iteration == 2
+        assert prompt.patch_error == "previous patch failed"
+        assert prompt.attempt == 3
+
+    def test_creation_no_patch_error(self):
+        """Test PatchPrompt without patch error."""
+        context = FailureContext(
+            log_excerpt="",
+            summary="",
+            implicated_files=[],
+            focused_diff="",
+            coverage_report=None,
+        )
+        prompt = PatchPrompt(
+            command="pytest",
+            failure_context=context,
+            git_diff="",
+            git_status="",
+            iteration=1,
+            patch_error=None,
+            attempt=1,
+        )
+        assert prompt.patch_error is None
diff --git a/tests/test_module_guard.py b/tests/test_module_guard.py
new file mode 100644
index 0000000..0c29fdf
--- /dev/null
+++ b/tests/test_module_guard.py
@@ -0,0 +1,374 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import module_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = module_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_module_lines == 600
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = module_guard.parse_args(
+        ["--root", "custom", "--max-module-lines", "100", "--exclude", "tests", "--exclude", "vendor"]
+    )
+    assert args.root == Path("custom")
+    assert args.max_module_lines == 100
+    assert args.exclude == [Path("tests"), Path("vendor")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(module_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(module_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_iter_python_files_directory(tmp_path: Path):
+    """Test iter_python_files with a directory."""
+    (tmp_path / "file1.py").write_text("# file1")
+    (tmp_path / "file2.py").write_text("# file2")
+    (tmp_path / "subdir").mkdir()
+    (tmp_path / "subdir" / "file3.py").write_text("# file3")
+    (tmp_path / "readme.txt").write_text("# readme")
+
+    files = list(module_guard.iter_python_files(tmp_path))
+    assert len(files) == 3
+    assert all(f.suffix == ".py" for f in files)
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert module_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert module_guard.is_excluded(path, exclusions) is False
+
+
+def test_is_excluded_value_error():
+    """Test exclusion handles ValueError correctly."""
+    path = Path("/project/src/module.py")
+    exclusions = [Path("/other/path")]
+    result = module_guard.is_excluded(path, exclusions)
+    assert result is False
+
+
+def test_count_lines_basic():
+    """Test line counting with basic content."""
+    tmp_file = Path("/tmp/test_module.py")
+    tmp_file.write_text("line1\nline2\nline3\n")
+
+    count = module_guard.count_lines(tmp_file)
+    assert count == 3
+
+    tmp_file.unlink()
+
+
+def test_count_lines_ignores_comments(tmp_path: Path):
+    """Test line counting ignores comment-only lines."""
+    py_file = tmp_path / "test.py"
+    write_module(
+        py_file,
+        """
+        # This is a comment
+        def foo():
+            # Another comment
+            pass
+
+        # More comments
+        """,
+    )
+
+    count = module_guard.count_lines(py_file)
+    assert count == 2  # Only "def foo():" and "pass" count
+
+
+def test_count_lines_ignores_empty_lines(tmp_path: Path):
+    """Test line counting ignores empty lines."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("\n\nline1\n\n\nline2\n\n")
+
+    count = module_guard.count_lines(py_file)
+    assert count == 2
+
+
+def test_count_lines_with_inline_comments(tmp_path: Path):
+    """Test line counting with inline comments."""
+    py_file = tmp_path / "test.py"
+    write_module(
+        py_file,
+        """
+        def foo():  # inline comment
+            x = 1  # another inline comment
+            return x
+        """,
+    )
+
+    count = module_guard.count_lines(py_file)
+    assert count == 3  # All three lines have significant content
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the line limit."""
+    py_file = tmp_path / "small.py"
+    write_module(
+        py_file,
+        """
+        def foo():
+            return 1
+        """,
+    )
+
+    result = module_guard.scan_file(py_file, limit=10)
+    assert result is None
+
+
+def test_scan_file_exceeds_limit(tmp_path: Path):
+    """Test scanning a file that exceeds the limit."""
+    py_file = tmp_path / "large.py"
+    content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    py_file.write_text(content)
+
+    result = module_guard.scan_file(py_file, limit=10)
+    assert result is not None
+    assert result[0] == py_file
+    assert result[1] == 20
+
+
+def test_scan_file_oserror(tmp_path: Path):
+    """Test scan_file raises RuntimeError on OSError."""
+    py_file = tmp_path / "nonexistent.py"
+
+    with pytest.raises(RuntimeError, match="failed to read Python source"):
+        module_guard.scan_file(py_file, limit=10)
+
+
+def test_scan_file_unicode_decode_error(tmp_path: Path):
+    """Test scan_file raises RuntimeError on UnicodeDecodeError."""
+    py_file = tmp_path / "bad_encoding.py"
+    py_file.write_bytes(b"\xff\xfe\x00\x00")  # Invalid UTF-8
+
+    with pytest.raises(RuntimeError, match="failed to read Python source"):
+        module_guard.scan_file(py_file, limit=10)
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "small.py",
+        """
+        def foo():
+            return 1
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root), "--max-module-lines", "10"])
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    py_file = root / "large.py"
+    content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    py_file.write_text(content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root), "--max-module-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Oversized modules detected" in captured.err
+    assert "large.py" in captured.err
+    assert "20 lines" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    large_content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    (root / "included.py").write_text(large_content)
+    (excluded / "excluded.py").write_text(large_content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(
+            ["--root", str(root), "--max-module-lines", "10", "--exclude", str(excluded)]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_handles_multiple_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles multiple violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    (root / "large1.py").write_text(large_content)
+    (root / "large2.py").write_text(large_content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root), "--max-module-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "large1.py" in captured.err
+    assert "large2.py" in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    (root / "zebra.py").write_text(large_content)
+    (root / "alpha.py").write_text(large_content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root), "--max-module-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line]
+    assert len(err_lines) == 2
+    assert "alpha.py" in err_lines[0]
+    assert "zebra.py" in err_lines[1]
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_content = "\n".join([f"line_{i} = {i}" for i in range(20)])
+    (root / "module.py").write_text(large_content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root), "--max-module-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "src/module.py" in captured.err or "src\\module.py" in captured.err
+
+
+def test_main_scan_file_error(tmp_path: Path, capsys: pytest.CaptureFixture, monkeypatch):
+    """Test main function handles scan_file errors."""
+    root = tmp_path / "src"
+    root.mkdir()
+    (root / "test.py").write_text("# test")
+
+    def mock_scan_file(path, limit):
+        raise RuntimeError("Test error")
+
+    monkeypatch.setattr(module_guard, "scan_file", mock_scan_file)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = module_guard.main(["--root", str(root)])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Test error" in captured.err
+
+
+def test_iter_python_files_empty_directory(tmp_path: Path):
+    """Test iter_python_files with empty directory."""
+    files = list(module_guard.iter_python_files(tmp_path))
+    assert len(files) == 0
+
+
+def test_count_lines_only_comments(tmp_path: Path):
+    """Test line counting with only comments."""
+    py_file = tmp_path / "comments.py"
+    py_file.write_text("# comment1\n# comment2\n# comment3\n")
+
+    count = module_guard.count_lines(py_file)
+    assert count == 0
+
+
+def test_count_lines_mixed_content(tmp_path: Path):
+    """Test line counting with mixed content."""
+    py_file = tmp_path / "mixed.py"
+    write_module(
+        py_file,
+        """
+        # Header comment
+
+        def foo():
+            # Function comment
+            x = 1
+
+            # Another comment
+            return x
+
+        # Footer comment
+        """,
+    )
+
+    count = module_guard.count_lines(py_file)
+    assert count == 3  # def foo():, x = 1, return x (empty lines don't count)
+
+
+def test_is_excluded_multiple_exclusions():
+    """Test exclusion with multiple patterns."""
+    path = Path("/project/tests/test_module.py").resolve()
+    exclusions = [
+        Path("/project/vendor").resolve(),
+        Path("/project/tests").resolve(),
+        Path("/project/docs").resolve(),
+    ]
+    assert module_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_partial_match():
+    """Test exclusion doesn't match partial paths."""
+    path = Path("/project/src/tests_helper.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert module_guard.is_excluded(path, exclusions) is False
diff --git a/tests/test_patch_cycle.py b/tests/test_patch_cycle.py
new file mode 100644
index 0000000..cbc90d4
--- /dev/null
+++ b/tests/test_patch_cycle.py
@@ -0,0 +1,803 @@
+"""Unit tests for ci_tools.ci_runtime.patch_cycle module."""
+
+from __future__ import annotations
+
+from unittest.mock import Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.patch_cycle import (
+    _obtain_patch_diff,
+    _validate_patch_candidate,
+    _apply_patch_candidate,
+    _should_apply_patch,
+    request_and_apply_patches,
+)
+from ci_tools.ci_runtime.models import (
+    PatchApplyError,
+    PatchAttemptState,
+    PatchLifecycleAbort,
+    PatchPrompt,
+)
+
+
+class TestObtainPatchDiff:
+    """Tests for _obtain_patch_diff function."""
+
+    @patch("ci_tools.ci_runtime.patch_cycle.request_codex_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle.extract_unified_diff")
+    def test_returns_diff_text_on_success(self, mock_extract, mock_request):
+        """Test returning diff text when Codex responds with valid patch."""
+        mock_request.return_value = "response with diff"
+        mock_extract.return_value = "diff --git a/file.py b/file.py"
+        options = Mock(model_name="gpt-5-codex", reasoning_effort="high")
+        prompt = Mock()
+
+        result = _obtain_patch_diff(options=options, prompt=prompt)
+
+        assert result == "diff --git a/file.py b/file.py"
+        mock_request.assert_called_once_with(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            prompt=prompt,
+        )
+
+    @patch("ci_tools.ci_runtime.patch_cycle.request_codex_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle.extract_unified_diff")
+    def test_raises_when_no_diff_extracted(self, mock_extract, mock_request):
+        """Test raising exception when no diff is extracted."""
+        mock_request.return_value = "response without diff"
+        mock_extract.return_value = None
+        options = Mock(model_name="gpt-5-codex", reasoning_effort="high")
+        prompt = Mock()
+
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            _obtain_patch_diff(options=options, prompt=prompt)
+
+        assert "empty or NOOP patch" in str(exc_info.value)
+
+    @patch("ci_tools.ci_runtime.patch_cycle.request_codex_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle.extract_unified_diff")
+    def test_raises_when_empty_diff(self, mock_extract, mock_request):
+        """Test raising exception when diff is empty string."""
+        mock_request.return_value = "response"
+        mock_extract.return_value = ""
+        options = Mock(model_name="gpt-5-codex", reasoning_effort="high")
+        prompt = Mock()
+
+        with pytest.raises(PatchLifecycleAbort):
+            _obtain_patch_diff(options=options, prompt=prompt)
+
+
+class TestValidatePatchCandidate:
+    """Tests for _validate_patch_candidate function."""
+
+    def test_accepts_valid_patch(self):
+        """Test accepting a valid patch."""
+        diff_text = "diff --git a/file.py b/file.py\n--- a/file.py\n+++ b/file.py"
+        seen_patches = set()
+
+        error = _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert error is None
+        assert diff_text in seen_patches
+
+    def test_rejects_duplicate_patch(self):
+        """Test rejecting duplicate patch."""
+        diff_text = "diff --git a/file.py b/file.py"
+        seen_patches = {diff_text}
+
+        error = _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert error is not None
+        assert "Duplicate patch" in error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.has_unified_diff_header")
+    def test_rejects_patch_without_headers(self, mock_has_header):
+        """Test rejecting patch without unified diff headers."""
+        mock_has_header.return_value = False
+        diff_text = "some content without headers"
+        seen_patches = set()
+
+        error = _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert error is not None
+        assert "missing unified diff headers" in error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.has_unified_diff_header")
+    @patch("ci_tools.ci_runtime.patch_cycle.patch_looks_risky")
+    def test_rejects_risky_patch(self, mock_risky, mock_has_header):
+        """Test rejecting patch that looks risky."""
+        mock_has_header.return_value = True
+        mock_risky.return_value = (True, "Contains DROP TABLE")
+        diff_text = "diff --git a/file.py b/file.py"
+        seen_patches = set()
+
+        error = _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert error is not None
+        assert "Contains DROP TABLE" in error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.has_unified_diff_header")
+    @patch("ci_tools.ci_runtime.patch_cycle.patch_looks_risky")
+    def test_rejects_risky_patch_without_reason(self, mock_risky, mock_has_header):
+        """Test rejecting risky patch without specific reason."""
+        mock_has_header.return_value = True
+        mock_risky.return_value = (True, None)
+        diff_text = "diff --git a/file.py b/file.py"
+        seen_patches = set()
+
+        error = _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert error is not None
+        assert "failed safety checks" in error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.has_unified_diff_header")
+    @patch("ci_tools.ci_runtime.patch_cycle.patch_looks_risky")
+    def test_adds_patch_to_seen_set(self, mock_risky, mock_has_header):
+        """Test patch is added to seen set."""
+        mock_has_header.return_value = True
+        mock_risky.return_value = (False, None)
+        diff_text = "diff --git a/file.py b/file.py"
+        seen_patches = set()
+
+        _validate_patch_candidate(
+            diff_text, seen_patches=seen_patches, max_patch_lines=1500
+        )
+
+        assert diff_text in seen_patches
+
+
+class TestApplyPatchCandidate:
+    """Tests for _apply_patch_candidate function."""
+
+    @patch("ci_tools.ci_runtime.patch_cycle.apply_patch")
+    def test_returns_true_on_success(self, mock_apply):
+        """Test returning True when patch applies successfully."""
+        diff_text = "diff --git a/file.py b/file.py"
+        state = PatchAttemptState(max_attempts=3)
+
+        result = _apply_patch_candidate(diff_text, state=state)
+
+        assert result is True
+        assert state.last_error is None
+        mock_apply.assert_called_once_with(diff_text)
+
+    @patch("ci_tools.ci_runtime.patch_cycle.apply_patch")
+    def test_returns_false_on_retryable_error(self, mock_apply):
+        """Test returning False when patch apply fails with retryable error."""
+        mock_apply.side_effect = PatchApplyError(
+            detail="git apply failed", retryable=True
+        )
+        diff_text = "diff --git a/file.py b/file.py"
+        state = PatchAttemptState(max_attempts=3)
+
+        result = _apply_patch_candidate(diff_text, state=state)
+
+        assert result is False
+        assert state.last_error is not None
+        assert "git apply failed" in state.last_error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.apply_patch")
+    def test_returns_false_on_non_retryable_error(self, mock_apply):
+        """Test returning False when patch apply fails with non-retryable error."""
+        mock_apply.side_effect = PatchApplyError(
+            detail="fatal error", retryable=False
+        )
+        diff_text = "diff --git a/file.py b/file.py"
+        state = PatchAttemptState(max_attempts=3)
+
+        result = _apply_patch_candidate(diff_text, state=state)
+
+        assert result is False
+        assert "fatal error" in state.last_error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.apply_patch")
+    def test_handles_runtime_error(self, mock_apply):
+        """Test handling generic RuntimeError from apply_patch."""
+        mock_apply.side_effect = RuntimeError("unexpected error")
+        diff_text = "diff --git a/file.py b/file.py"
+        state = PatchAttemptState(max_attempts=3)
+
+        result = _apply_patch_candidate(diff_text, state=state)
+
+        assert result is False
+        assert "unexpected error" in state.last_error
+
+    @patch("ci_tools.ci_runtime.patch_cycle.apply_patch")
+    def test_records_failure_with_retryable_flag(self, mock_apply):
+        """Test recording failure preserves retryable flag."""
+        error = PatchApplyError(detail="fail", retryable=True)
+        mock_apply.side_effect = error
+        state = Mock()
+        diff_text = "diff --git a/file.py b/file.py"
+
+        _apply_patch_candidate(diff_text, state=state)
+
+        state.record_failure.assert_called_once_with("Patch application failed: fail", retryable=True)
+
+
+class TestShouldApplyPatch:
+    """Tests for _should_apply_patch function."""
+
+    def test_auto_mode_returns_true(self):
+        """Test auto mode returns True without prompting."""
+        result = _should_apply_patch(approval_mode="auto", attempt=1)
+
+        assert result is True
+
+    @patch("builtins.input", return_value="y")
+    def test_prompt_mode_accepts_yes(self, mock_input):
+        """Test prompt mode accepts 'y' or 'yes'."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert result is True
+        mock_input.assert_called_once()
+
+    @patch("builtins.input", return_value="yes")
+    def test_prompt_mode_accepts_yes_word(self, mock_input):
+        """Test prompt mode accepts 'yes'."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=2)
+
+        assert result is True
+
+    @patch("builtins.input", return_value="")
+    def test_prompt_mode_accepts_empty_input(self, mock_input):
+        """Test prompt mode accepts empty input as yes."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert result is True
+
+    @patch("builtins.input", return_value="n")
+    def test_prompt_mode_rejects_no(self, mock_input):
+        """Test prompt mode rejects 'n'."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert result is False
+
+    @patch("builtins.input", return_value="q")
+    def test_prompt_mode_quit_raises_abort(self, mock_input):
+        """Test prompt mode raises abort on 'q'."""
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert "user declined" in str(exc_info.value)
+
+    @patch("builtins.input", return_value="quit")
+    def test_prompt_mode_quit_word_raises_abort(self, mock_input):
+        """Test prompt mode raises abort on 'quit'."""
+        with pytest.raises(PatchLifecycleAbort):
+            _should_apply_patch(approval_mode="prompt", attempt=1)
+
+    @patch("builtins.input", return_value="  YES  ")
+    def test_prompt_mode_strips_and_lowercases(self, mock_input):
+        """Test prompt mode strips whitespace and lowercases."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert result is True
+
+    @patch("builtins.input", return_value="invalid")
+    def test_prompt_mode_rejects_invalid_input(self, mock_input):
+        """Test prompt mode rejects invalid input."""
+        result = _should_apply_patch(approval_mode="prompt", attempt=1)
+
+        assert result is False
+
+
+class TestRequestAndApplyPatches:
+    """Tests for request_and_apply_patches function."""
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_successful_patch_on_first_attempt(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test successful patch application on first attempt."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status", "post-patch status"]
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        mock_obtain.assert_called_once()
+        mock_apply.assert_called_once()
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_retries_on_validation_error(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test retrying when validation fails."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status"] * 10
+        mock_obtain.side_effect = ["bad diff", "good diff"]
+        mock_validate.side_effect = ["Validation error", None]
+        mock_should_apply.return_value = True
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=2,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        assert mock_obtain.call_count == 2
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_retries_when_user_declines(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test retrying when user declines patch."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status"] * 10
+        mock_obtain.side_effect = ["diff1", "diff2"]
+        mock_validate.return_value = None
+        mock_should_apply.side_effect = [False, True]
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=2,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="prompt",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        assert mock_obtain.call_count == 2
+        assert mock_should_apply.call_count == 2
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_retries_when_apply_fails(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test retrying when patch application fails."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status"] * 10
+        mock_obtain.side_effect = ["diff1", "diff2"]
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.side_effect = [False, True]
+
+        args = Mock(
+            command="pytest",
+            patch_retries=2,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        assert mock_apply.call_count == 2
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_raises_when_budget_exhausted(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test raising exception when patch budget exhausted."""
+        mock_diff.return_value = "current diff"
+        mock_status.return_value = "git status"
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.return_value = False
+
+        args = Mock(
+            command="pytest",
+            patch_retries=0,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            request_and_apply_patches(
+                args=args,
+                options=options,
+                failure_ctx=failure_ctx,
+                iteration=1,
+                seen_patches=seen_patches,
+            )
+
+        assert "exhausting retries" in str(exc_info.value)
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_shows_post_patch_status_when_not_clean(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+        capsys,
+    ):
+        """Test showing git status after patch when not clean."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status", "M file.py"]
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        captured = capsys.readouterr()
+        assert "git status after patch" in captured.out
+        assert "M file.py" in captured.out
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_shows_clean_message_when_no_status(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+        capsys,
+    ):
+        """Test showing clean message when no git status."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status", ""]
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        captured = capsys.readouterr()
+        assert "Working tree is clean" in captured.out
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_passes_correct_patch_prompt(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test passing correct PatchPrompt to obtain function."""
+        mock_diff.return_value = "current diff"
+        mock_status.return_value = "git status"
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.return_value = True
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=2,
+            seen_patches=seen_patches,
+        )
+
+        # Verify PatchPrompt structure
+        call_args = mock_obtain.call_args
+        prompt = call_args[1]["prompt"]
+        assert isinstance(prompt, PatchPrompt)
+        assert prompt.command == "pytest"
+        assert prompt.failure_context == failure_ctx
+        assert prompt.git_diff == "current diff"
+        assert prompt.git_status == "git status"
+        assert prompt.iteration == 2
+        assert prompt.attempt == 1
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    @patch("ci_tools.ci_runtime.patch_cycle._apply_patch_candidate")
+    def test_includes_previous_error_in_prompt(
+        self,
+        mock_apply,
+        mock_should_apply,
+        mock_validate,
+        mock_obtain,
+        mock_status,
+        mock_diff,
+    ):
+        """Test including previous patch error in prompt."""
+        mock_diff.return_value = "current diff"
+        mock_status.side_effect = ["git status"] * 10
+        mock_obtain.side_effect = ["diff1", "diff2"]
+        mock_validate.return_value = None
+        mock_should_apply.return_value = True
+        mock_apply.side_effect = [False, True]
+
+        args = Mock(
+            command="pytest",
+            patch_retries=2,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        request_and_apply_patches(
+            args=args,
+            options=options,
+            failure_ctx=failure_ctx,
+            iteration=1,
+            seen_patches=seen_patches,
+        )
+
+        # Second call should have error from first attempt
+        second_call = mock_obtain.call_args_list[1]
+        prompt = second_call[1]["prompt"]
+        # The error is stored in state which is passed to prompt
+        assert prompt.attempt == 2
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    def test_raises_on_missing_patch(self, mock_obtain, mock_status, mock_diff):
+        """Test raising exception when _obtain_patch_diff raises."""
+        mock_diff.return_value = "current diff"
+        mock_status.return_value = "git status"
+        mock_obtain.side_effect = PatchLifecycleAbort.missing_patch()
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="auto",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        with pytest.raises(PatchLifecycleAbort):
+            request_and_apply_patches(
+                args=args,
+                options=options,
+                failure_ctx=failure_ctx,
+                iteration=1,
+                seen_patches=seen_patches,
+            )
+
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle.gather_git_status")
+    @patch("ci_tools.ci_runtime.patch_cycle._obtain_patch_diff")
+    @patch("ci_tools.ci_runtime.patch_cycle._validate_patch_candidate")
+    @patch("ci_tools.ci_runtime.patch_cycle._should_apply_patch")
+    def test_user_quit_propagates(
+        self, mock_should_apply, mock_validate, mock_obtain, mock_status, mock_diff
+    ):
+        """Test user quit exception propagates."""
+        mock_diff.return_value = "current diff"
+        mock_status.return_value = "git status"
+        mock_obtain.return_value = "diff content"
+        mock_validate.return_value = None
+        mock_should_apply.side_effect = PatchLifecycleAbort.user_declined()
+
+        args = Mock(
+            command="pytest",
+            patch_retries=1,
+            max_patch_lines=1500,
+        )
+        options = Mock(
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="prompt",
+        )
+        failure_ctx = Mock()
+        seen_patches = set()
+
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            request_and_apply_patches(
+                args=args,
+                options=options,
+                failure_ctx=failure_ctx,
+                iteration=1,
+                seen_patches=seen_patches,
+            )
+
+        assert "user declined" in str(exc_info.value)
diff --git a/tests/test_patching.py b/tests/test_patching.py
new file mode 100644
index 0000000..05b4b3d
--- /dev/null
+++ b/tests/test_patching.py
@@ -0,0 +1,483 @@
+"""Unit tests for ci_tools.ci_runtime.patching module."""
+
+from __future__ import annotations
+
+from unittest.mock import Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.patching import (
+    _extract_diff_paths,
+    patch_looks_risky,
+    _ensure_trailing_newline,
+    _apply_patch_with_git,
+    _patch_already_applied,
+    _apply_patch_with_patch_tool,
+    apply_patch,
+)
+from ci_tools.ci_runtime.models import PatchApplyError
+
+
+class TestExtractDiffPaths:
+    """Tests for _extract_diff_paths helper function."""
+
+    def test_extracts_paths_from_diff_headers(self):
+        """Test extracts file paths from diff headers."""
+        patch_text = """diff --git a/ci.py b/ci.py
+index 123..456 100644
+--- a/ci.py
++++ b/ci.py
+"""
+        with patch("ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("ci.py",)):
+            paths = _extract_diff_paths(patch_text)
+            assert "ci.py" in paths
+
+    def test_ignores_non_protected_paths(self):
+        """Test ignores paths that aren't protected."""
+        patch_text = """diff --git a/src/module.py b/src/module.py
+index 123..456 100644
+--- a/src/module.py
++++ b/src/module.py
+"""
+        with patch("ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("ci.py",)):
+            paths = _extract_diff_paths(patch_text)
+            assert len(paths) == 0
+
+    def test_handles_multiple_files(self):
+        """Test handles multiple files in patch."""
+        patch_text = """diff --git a/ci.py b/ci.py
+--- a/ci.py
++++ b/ci.py
+diff --git a/Makefile b/Makefile
+--- a/Makefile
++++ b/Makefile
+"""
+        with patch(
+            "ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES",
+            ("ci.py", "Makefile"),
+        ):
+            paths = _extract_diff_paths(patch_text)
+            assert "ci.py" in paths
+            assert "Makefile" in paths
+
+    def test_handles_malformed_diff_lines(self):
+        """Test handles malformed diff lines gracefully."""
+        patch_text = """diff --git incomplete
+diff --git a/file.py
+"""
+        with patch("ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("file.py",)):
+            paths = _extract_diff_paths(patch_text)
+            assert len(paths) == 0
+
+    def test_returns_empty_set_when_no_diffs(self):
+        """Test returns empty set when no diff headers found."""
+        patch_text = "just some text\nno diffs here"
+        paths = _extract_diff_paths(patch_text)
+        assert len(paths) == 0
+
+    def test_handles_paths_with_prefixes(self):
+        """Test correctly matches path prefixes."""
+        patch_text = """diff --git a/ci_tools/module.py b/ci_tools/module.py
+--- a/ci_tools/module.py
++++ b/ci_tools/module.py
+"""
+        with patch(
+            "ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("ci_tools/",)
+        ):
+            paths = _extract_diff_paths(patch_text)
+            assert "ci_tools/module.py" in paths
+
+
+class TestPatchLooksRisky:
+    """Tests for patch_looks_risky safety checker."""
+
+    def test_empty_patch_is_risky(self):
+        """Test empty patch content is flagged as risky."""
+        risky, reason = patch_looks_risky("", max_lines=100)
+        assert risky is True
+        assert "empty" in reason.lower()
+
+    def test_large_patch_exceeds_limit(self):
+        """Test patch exceeding line limit is flagged."""
+        large_patch = "\n".join(["+new line" for _ in range(150)])
+        risky, reason = patch_looks_risky(large_patch, max_lines=100)
+        assert risky is True
+        assert "150" in reason
+        assert "100" in reason
+
+    def test_protected_path_modification_is_risky(self):
+        """Test modifying protected paths is flagged."""
+        patch_text = """diff --git a/ci.py b/ci.py
+--- a/ci.py
++++ b/ci.py
+@@ -1 +1 @@
+-old
++new
+"""
+        with patch(
+            "ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("ci.py",)
+        ):
+            risky, reason = patch_looks_risky(patch_text, max_lines=1000)
+            assert risky is True
+            assert "protected path" in reason.lower()
+            assert "ci.py" in reason
+
+    def test_risky_pattern_detected(self):
+        """Test risky patterns in diff are detected."""
+        patch_text = """diff --git a/file.py b/file.py
+--- a/file.py
++++ b/file.py
+@@ -1 +1 @@
+-old
++subprocess.run(['rm', '-rf', '/'])
+"""
+        with patch("ci_tools.ci_runtime.patching.risky_pattern_in_diff") as mock_risky:
+            mock_risky.return_value = "rm -rf"
+            risky, reason = patch_looks_risky(patch_text, max_lines=1000)
+            assert risky is True
+            assert "risky pattern" in reason.lower()
+
+    def test_safe_patch_passes_checks(self):
+        """Test safe patch passes all checks."""
+        patch_text = """diff --git a/src/module.py b/src/module.py
+--- a/src/module.py
++++ b/src/module.py
+@@ -1 +1 @@
+-def old():
++def new():
+"""
+        with patch("ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES", ("ci.py",)):
+            with patch(
+                "ci_tools.ci_runtime.patching.risky_pattern_in_diff"
+            ) as mock_risky:
+                mock_risky.return_value = None
+                risky, reason = patch_looks_risky(patch_text, max_lines=1000)
+                assert risky is False
+                assert reason is None
+
+    def test_multiple_protected_paths_listed(self):
+        """Test multiple protected paths are listed in reason."""
+        patch_text = """diff --git a/ci.py b/ci.py
+diff --git a/Makefile b/Makefile
+"""
+        with patch(
+            "ci_tools.ci_runtime.patching.PROTECTED_PATH_PREFIXES",
+            ("ci.py", "Makefile"),
+        ):
+            risky, reason = patch_looks_risky(patch_text, max_lines=1000)
+            assert risky is True
+            assert "ci.py" in reason
+            assert "Makefile" in reason
+
+
+class TestEnsureTrailingNewline:
+    """Tests for _ensure_trailing_newline helper."""
+
+    def test_adds_newline_when_missing(self):
+        """Test adds newline when not present."""
+        result = _ensure_trailing_newline("patch content")
+        assert result == "patch content\n"
+
+    def test_preserves_existing_newline(self):
+        """Test doesn't add extra newline when already present."""
+        result = _ensure_trailing_newline("patch content\n")
+        assert result == "patch content\n"
+
+    def test_handles_empty_string(self):
+        """Test handles empty string."""
+        result = _ensure_trailing_newline("")
+        assert result == "\n"
+
+    def test_handles_multiple_trailing_newlines(self):
+        """Test preserves multiple trailing newlines."""
+        result = _ensure_trailing_newline("content\n\n")
+        assert result == "content\n\n"
+
+
+class TestApplyPatchWithGit:
+    """Tests for _apply_patch_with_git function."""
+
+    def test_successful_git_apply(self):
+        """Test successful patch application via git."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),  # git apply --check
+                Mock(returncode=0, stdout="Applied patch", stderr=""),  # git apply
+            ]
+            applied, diagnostics = _apply_patch_with_git("diff content")
+            assert applied is True
+            assert mock_run.call_count == 2
+
+    def test_git_apply_check_fails(self):
+        """Test when git apply --check fails."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=1, stdout="", stderr="error: patch failed"
+            )
+            applied, diagnostics = _apply_patch_with_git("diff content")
+            assert applied is False
+            assert "error: patch failed" in diagnostics
+
+    def test_git_apply_fails_after_check_passes(self):
+        """Test when git apply fails after check passes."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),  # check passes
+                Mock(returncode=1, stdout="", stderr="apply failed"),  # apply fails
+            ]
+            with pytest.raises(PatchApplyError) as exc_info:
+                _apply_patch_with_git("diff content")
+            assert exc_info.value.retryable is True
+
+    def test_prints_stdout_on_success(self, capsys):
+        """Test prints stdout when git apply succeeds."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),
+                Mock(returncode=0, stdout="Patched file.py", stderr=""),
+            ]
+            _apply_patch_with_git("diff")
+            captured = capsys.readouterr()
+            assert "Patched file.py" in captured.out
+
+    def test_uses_whitespace_nowarn_flag(self):
+        """Test uses --whitespace=nowarn flag."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),
+                Mock(returncode=0, stdout="", stderr=""),
+            ]
+            _apply_patch_with_git("diff")
+            for call_obj in mock_run.call_args_list:
+                args = call_obj[0][0]
+                if "apply" in args:
+                    assert "--whitespace=nowarn" in args
+
+
+class TestPatchAlreadyApplied:
+    """Tests for _patch_already_applied function."""
+
+    def test_returns_true_when_reverse_check_succeeds(self, capsys):
+        """Test returns True when reverse check passes."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(returncode=0, stdout="", stderr="")
+            result = _patch_already_applied("diff content")
+            assert result is True
+            captured = capsys.readouterr()
+            assert "already applied" in captured.out.lower()
+
+    def test_returns_false_when_reverse_check_fails(self):
+        """Test returns False when reverse check fails."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(returncode=1, stdout="", stderr="")
+            result = _patch_already_applied("diff content")
+            assert result is False
+
+    def test_uses_reverse_flag(self):
+        """Test uses --reverse flag for checking."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(returncode=1, stdout="", stderr="")
+            _patch_already_applied("diff")
+            args = mock_run.call_args[0][0]
+            assert "--reverse" in args
+
+
+class TestApplyPatchWithPatchTool:
+    """Tests for _apply_patch_with_patch_tool function."""
+
+    def test_successful_patch_application(self):
+        """Test successful patch using patch utility."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),  # dry run
+                Mock(returncode=0, stdout="patching file", stderr=""),  # actual
+            ]
+            _apply_patch_with_patch_tool("diff", check_output="git check failed")
+            assert mock_run.call_count == 2
+
+    def test_dry_run_failure_raises_error(self):
+        """Test dry run failure raises PatchApplyError."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=1, stdout="", stderr="dry run failed"
+            )
+            with pytest.raises(PatchApplyError) as exc_info:
+                _apply_patch_with_patch_tool("diff", check_output="check failed")
+            assert "dry-run" in str(exc_info.value).lower()
+            assert exc_info.value.retryable is True
+
+    def test_actual_patch_failure_raises_error(self):
+        """Test actual patch failure raises PatchApplyError."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),  # dry run passes
+                Mock(returncode=1, stdout="", stderr="patch failed"),  # actual fails
+            ]
+            with pytest.raises(PatchApplyError) as exc_info:
+                _apply_patch_with_patch_tool("diff", check_output="")
+            assert "exit" in str(exc_info.value).lower()
+
+    def test_sets_patch_create_backup_env_var(self):
+        """Test sets PATCH_CREATE_BACKUP=no environment variable."""
+        with patch("subprocess.run") as mock_run:
+            with patch("os.environ", {}):
+                mock_run.side_effect = [
+                    Mock(returncode=0, stdout="", stderr=""),
+                    Mock(returncode=0, stdout="", stderr=""),
+                ]
+                _apply_patch_with_patch_tool("diff", check_output="")
+                for call_obj in mock_run.call_args_list:
+                    env = call_obj[1]["env"]
+                    if "PATCH_CREATE_BACKUP" in env:
+                        assert env["PATCH_CREATE_BACKUP"] == "no"
+
+    def test_uses_batch_and_forward_flags(self):
+        """Test uses --batch and --forward flags."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),
+                Mock(returncode=0, stdout="", stderr=""),
+            ]
+            _apply_patch_with_patch_tool("diff", check_output="")
+            for call_obj in mock_run.call_args_list:
+                args = call_obj[0][0]
+                if "patch" in args:
+                    assert "--batch" in args
+                    assert "--forward" in args
+
+    def test_prints_stdout_on_success(self, capsys):
+        """Test prints stdout when patch succeeds."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.side_effect = [
+                Mock(returncode=0, stdout="", stderr=""),
+                Mock(returncode=0, stdout="patching file.py", stderr=""),
+            ]
+            _apply_patch_with_patch_tool("diff", check_output="")
+            captured = capsys.readouterr()
+            assert "patching file.py" in captured.out
+
+
+class TestApplyPatch:
+    """Tests for apply_patch public API."""
+
+    def test_applies_with_git_when_possible(self):
+        """Test prefers git apply when it works."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            mock_git_apply.return_value = (True, "")
+            apply_patch("diff content")
+            mock_git_apply.assert_called_once()
+
+    def test_falls_back_to_patch_tool_when_git_fails(self):
+        """Test falls back to patch utility when git fails."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            with patch(
+                "ci_tools.ci_runtime.patching._patch_already_applied"
+            ) as mock_already:
+                with patch(
+                    "ci_tools.ci_runtime.patching._apply_patch_with_patch_tool"
+                ) as mock_patch_tool:
+                    mock_git_apply.return_value = (False, "git failed")
+                    mock_already.return_value = False
+                    apply_patch("diff content")
+                    mock_patch_tool.assert_called_once()
+
+    def test_skips_when_already_applied(self):
+        """Test skips application when patch already applied."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            with patch(
+                "ci_tools.ci_runtime.patching._patch_already_applied"
+            ) as mock_already:
+                with patch(
+                    "ci_tools.ci_runtime.patching._apply_patch_with_patch_tool"
+                ) as mock_patch_tool:
+                    mock_git_apply.return_value = (False, "")
+                    mock_already.return_value = True
+                    apply_patch("diff content")
+                    mock_patch_tool.assert_not_called()
+
+    def test_ensures_trailing_newline(self):
+        """Test ensures patch has trailing newline."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            mock_git_apply.return_value = (True, "")
+            apply_patch("diff without newline")
+            call_args = mock_git_apply.call_args[0][0]
+            assert call_args.endswith("\n")
+
+    def test_propagates_patch_apply_errors(self):
+        """Test propagates PatchApplyError exceptions."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            with patch(
+                "ci_tools.ci_runtime.patching._patch_already_applied"
+            ) as mock_already:
+                with patch(
+                    "ci_tools.ci_runtime.patching._apply_patch_with_patch_tool"
+                ) as mock_patch_tool:
+                    mock_git_apply.return_value = (False, "check failed")
+                    mock_already.return_value = False
+                    mock_patch_tool.side_effect = PatchApplyError.patch_exit(
+                        returncode=1, output="failed"
+                    )
+                    with pytest.raises(PatchApplyError):
+                        apply_patch("diff")
+
+    def test_handles_empty_check_output(self):
+        """Test handles empty check output from git."""
+        with patch(
+            "ci_tools.ci_runtime.patching._apply_patch_with_git"
+        ) as mock_git_apply:
+            with patch(
+                "ci_tools.ci_runtime.patching._patch_already_applied"
+            ) as mock_already:
+                with patch(
+                    "ci_tools.ci_runtime.patching._apply_patch_with_patch_tool"
+                ) as mock_patch_tool:
+                    mock_git_apply.return_value = (False, "")
+                    mock_already.return_value = False
+                    mock_patch_tool.return_value = None
+                    apply_patch("diff")
+                    mock_patch_tool.assert_called_once()
+
+
+class TestPatchApplyError:
+    """Tests for PatchApplyError exception class."""
+
+    def test_git_apply_failed_factory(self):
+        """Test git_apply_failed factory method."""
+        error = PatchApplyError.git_apply_failed(output="git error message")
+        assert error.retryable is True
+        assert "git apply" in str(error).lower()
+        assert "git error message" in str(error)
+
+    def test_preflight_failed_factory(self):
+        """Test preflight_failed factory method."""
+        error = PatchApplyError.preflight_failed(
+            check_output="git check failed", dry_output="patch dry-run failed"
+        )
+        assert error.retryable is True
+        assert "preflight" in str(error).lower() or "dry-run" in str(error).lower()
+
+    def test_patch_exit_factory(self):
+        """Test patch_exit factory method."""
+        error = PatchApplyError.patch_exit(returncode=2, output="rejected")
+        assert error.retryable is True
+        assert "2" in str(error)
+
+    def test_retryable_attribute_default_true(self):
+        """Test retryable attribute defaults to True."""
+        error = PatchApplyError(detail="test error")
+        assert error.retryable is True
+
+    def test_retryable_attribute_can_be_false(self):
+        """Test retryable attribute can be set to False."""
+        error = PatchApplyError(detail="test error", retryable=False)
+        assert error.retryable is False
diff --git a/tests/test_policy_checks.py b/tests/test_policy_checks.py
new file mode 100644
index 0000000..bf6c518
--- /dev/null
+++ b/tests/test_policy_checks.py
@@ -0,0 +1,189 @@
+"""Unit tests for policy_checks module."""
+
+from __future__ import annotations
+
+import sys
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts.policy_checks import PolicyViolation, main
+
+
+def test_policy_violation_imported():
+    """Test PolicyViolation is correctly imported."""
+    assert PolicyViolation is not None
+    exc = PolicyViolation("test")
+    assert isinstance(exc, Exception)
+
+
+def test_main_calls_all_checks():
+    """Test main calls all policy check functions."""
+    with patch("ci_tools.scripts.policy_checks.purge_bytecode_artifacts") as mock_purge, \
+         patch("ci_tools.scripts.policy_checks._check_keyword_policy") as mock_keyword, \
+         patch("ci_tools.scripts.policy_checks._check_flagged_tokens") as mock_flagged, \
+         patch("ci_tools.scripts.policy_checks._check_function_lengths") as mock_lengths, \
+         patch("ci_tools.scripts.policy_checks._check_broad_excepts") as mock_broad, \
+         patch("ci_tools.scripts.policy_checks._check_silent_handlers") as mock_silent, \
+         patch("ci_tools.scripts.policy_checks._check_generic_raises") as mock_generic, \
+         patch("ci_tools.scripts.policy_checks._check_literal_fallbacks") as mock_literal, \
+         patch("ci_tools.scripts.policy_checks._check_boolean_fallbacks") as mock_boolean, \
+         patch("ci_tools.scripts.policy_checks._check_conditional_literals") as mock_conditional, \
+         patch("ci_tools.scripts.policy_checks._check_backward_compat") as mock_backward, \
+         patch("ci_tools.scripts.policy_checks._check_legacy_artifacts") as mock_legacy, \
+         patch("ci_tools.scripts.policy_checks._check_sync_calls") as mock_sync, \
+         patch("ci_tools.scripts.policy_checks._check_suppressions") as mock_suppressions, \
+         patch("ci_tools.scripts.policy_checks._check_duplicate_functions") as mock_duplicates, \
+         patch("ci_tools.scripts.policy_checks._check_bytecode_artifacts") as mock_bytecode:
+
+        result = main()
+
+        assert result == 0
+        mock_purge.assert_called_once()
+        mock_keyword.assert_called_once()
+        mock_flagged.assert_called_once()
+        mock_lengths.assert_called_once()
+        mock_broad.assert_called_once()
+        mock_silent.assert_called_once()
+        mock_generic.assert_called_once()
+        mock_literal.assert_called_once()
+        mock_boolean.assert_called_once()
+        mock_conditional.assert_called_once()
+        mock_backward.assert_called_once()
+        mock_legacy.assert_called_once()
+        mock_sync.assert_called_once()
+        mock_suppressions.assert_called_once()
+        mock_duplicates.assert_called_once()
+        mock_bytecode.assert_called_once()
+
+
+def test_main_returns_zero_on_success():
+    """Test main returns 0 when all checks pass."""
+    with patch("ci_tools.scripts.policy_checks.purge_bytecode_artifacts"), \
+         patch("ci_tools.scripts.policy_checks._check_keyword_policy"), \
+         patch("ci_tools.scripts.policy_checks._check_flagged_tokens"), \
+         patch("ci_tools.scripts.policy_checks._check_function_lengths"), \
+         patch("ci_tools.scripts.policy_checks._check_broad_excepts"), \
+         patch("ci_tools.scripts.policy_checks._check_silent_handlers"), \
+         patch("ci_tools.scripts.policy_checks._check_generic_raises"), \
+         patch("ci_tools.scripts.policy_checks._check_literal_fallbacks"), \
+         patch("ci_tools.scripts.policy_checks._check_boolean_fallbacks"), \
+         patch("ci_tools.scripts.policy_checks._check_conditional_literals"), \
+         patch("ci_tools.scripts.policy_checks._check_backward_compat"), \
+         patch("ci_tools.scripts.policy_checks._check_legacy_artifacts"), \
+         patch("ci_tools.scripts.policy_checks._check_sync_calls"), \
+         patch("ci_tools.scripts.policy_checks._check_suppressions"), \
+         patch("ci_tools.scripts.policy_checks._check_duplicate_functions"), \
+         patch("ci_tools.scripts.policy_checks._check_bytecode_artifacts"):
+
+        result = main()
+        assert result == 0
+
+
+def test_main_propagates_policy_violation():
+    """Test main propagates PolicyViolation exceptions."""
+    with patch("ci_tools.scripts.policy_checks.purge_bytecode_artifacts"), \
+         patch("ci_tools.scripts.policy_checks._check_keyword_policy", side_effect=PolicyViolation("test error")):
+
+        with pytest.raises(PolicyViolation) as exc:
+            main()
+        assert "test error" in str(exc.value)
+
+
+def test_main_checks_called_in_order():
+    """Test main calls checks in the correct order."""
+    call_order = []
+
+    def make_tracker(name):
+        def tracker():
+            call_order.append(name)
+        return tracker
+
+    with patch("ci_tools.scripts.policy_checks.purge_bytecode_artifacts", make_tracker("purge")), \
+         patch("ci_tools.scripts.policy_checks._check_keyword_policy", make_tracker("keyword")), \
+         patch("ci_tools.scripts.policy_checks._check_flagged_tokens", make_tracker("flagged")), \
+         patch("ci_tools.scripts.policy_checks._check_function_lengths", make_tracker("lengths")), \
+         patch("ci_tools.scripts.policy_checks._check_broad_excepts", make_tracker("broad")), \
+         patch("ci_tools.scripts.policy_checks._check_silent_handlers", make_tracker("silent")), \
+         patch("ci_tools.scripts.policy_checks._check_generic_raises", make_tracker("generic")), \
+         patch("ci_tools.scripts.policy_checks._check_literal_fallbacks", make_tracker("literal")), \
+         patch("ci_tools.scripts.policy_checks._check_boolean_fallbacks", make_tracker("boolean")), \
+         patch("ci_tools.scripts.policy_checks._check_conditional_literals", make_tracker("conditional")), \
+         patch("ci_tools.scripts.policy_checks._check_backward_compat", make_tracker("backward")), \
+         patch("ci_tools.scripts.policy_checks._check_legacy_artifacts", make_tracker("legacy")), \
+         patch("ci_tools.scripts.policy_checks._check_sync_calls", make_tracker("sync")), \
+         patch("ci_tools.scripts.policy_checks._check_suppressions", make_tracker("suppressions")), \
+         patch("ci_tools.scripts.policy_checks._check_duplicate_functions", make_tracker("duplicates")), \
+         patch("ci_tools.scripts.policy_checks._check_bytecode_artifacts", make_tracker("bytecode")):
+
+        main()
+
+        # Verify purge is called first
+        assert call_order[0] == "purge"
+        # Verify bytecode check is called last
+        assert call_order[-1] == "bytecode"
+        # Verify all checks were called
+        assert len(call_order) == 16
+
+
+def test_main_stops_on_first_violation():
+    """Test main stops execution on first violation."""
+    call_order = []
+
+    def make_tracker(name):
+        def tracker():
+            call_order.append(name)
+        return tracker
+
+    with patch("ci_tools.scripts.policy_checks.purge_bytecode_artifacts", make_tracker("purge")), \
+         patch("ci_tools.scripts.policy_checks._check_keyword_policy", make_tracker("keyword")), \
+         patch("ci_tools.scripts.policy_checks._check_flagged_tokens",
+               side_effect=PolicyViolation("error")), \
+         patch("ci_tools.scripts.policy_checks._check_function_lengths", make_tracker("lengths")):
+
+        with pytest.raises(PolicyViolation):
+            main()
+
+        # purge and keyword should have been called, but not lengths
+        assert "purge" in call_order
+        assert "keyword" in call_order
+        assert "lengths" not in call_order
+
+
+def test_module_exports():
+    """Test module exports expected symbols."""
+    from ci_tools.scripts import policy_checks
+
+    assert hasattr(policy_checks, "PolicyViolation")
+    assert hasattr(policy_checks, "main")
+    assert hasattr(policy_checks, "purge_bytecode_artifacts")
+
+
+def test_main_as_script_success(monkeypatch):
+    """Test running module as script with successful checks."""
+    with patch("ci_tools.scripts.policy_checks.main", return_value=0):
+        with pytest.raises(SystemExit) as exc:
+            # Simulate running as __main__
+            exec(compile(
+                "import sys; from ci_tools.scripts.policy_checks import main; sys.exit(main())",
+                "<string>",
+                "exec"
+            ))
+        assert exc.value.code == 0
+
+
+def test_main_as_script_with_violation(monkeypatch, capsys):
+    """Test running module as script with policy violation."""
+    def mock_main():
+        raise PolicyViolation("test violation")
+
+    with patch("ci_tools.scripts.policy_checks.main", side_effect=mock_main):
+        with pytest.raises(SystemExit) as exc:
+            # Simulate the __main__ block behavior
+            try:
+                mock_main()
+            except PolicyViolation as err:
+                print(err, file=sys.stderr)
+                raise SystemExit(1)
+
+        assert exc.value.code == 1
diff --git a/tests/test_policy_collectors_ast.py b/tests/test_policy_collectors_ast.py
new file mode 100644
index 0000000..afde12d
--- /dev/null
+++ b/tests/test_policy_collectors_ast.py
@@ -0,0 +1,667 @@
+"""Unit tests for policy_collectors_ast module."""
+
+from __future__ import annotations
+
+import ast
+import textwrap
+from pathlib import Path
+
+
+from ci_tools.scripts.policy_collectors_ast import (
+    collect_backward_compat_blocks,
+    collect_bool_fallbacks,
+    collect_broad_excepts,
+    collect_bytecode_artifacts,
+    collect_conditional_literal_returns,
+    collect_duplicate_functions,
+    collect_forbidden_sync_calls,
+    collect_generic_raises,
+    collect_literal_fallbacks,
+    collect_long_functions,
+    collect_silent_handlers,
+    contains_literal_dataset,
+    purge_bytecode_artifacts,
+)
+
+
+def write_module(path: Path, source: str) -> None:
+    """Helper to write a Python module."""
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(textwrap.dedent(source).strip() + "\n", encoding="utf-8")
+
+
+def test_contains_literal_dataset_constant():
+    """Test contains_literal_dataset with simple constant."""
+    source = "42"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_string():
+    """Test contains_literal_dataset with string."""
+    source = "'hello'"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_list():
+    """Test contains_literal_dataset with list."""
+    source = "[1, 2, 3]"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_tuple():
+    """Test contains_literal_dataset with tuple."""
+    source = "(1, 2, 3)"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_set():
+    """Test contains_literal_dataset with set."""
+    source = "{1, 2, 3}"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_dict():
+    """Test contains_literal_dataset with dict."""
+    source = "{'a': 1, 'b': 2}"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_nested():
+    """Test contains_literal_dataset with nested structures."""
+    source = "{'key': [1, 2, {'nested': 3}]}"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is True
+
+
+def test_contains_literal_dataset_false():
+    """Test contains_literal_dataset returns false for non-literals."""
+    source = "x + 1"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert contains_literal_dataset(node) is False
+
+
+def test_collect_long_functions(tmp_path, monkeypatch):
+    """Test collect_long_functions finds oversized functions."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        def short():
+            return 1
+
+        def long_function():
+            line1 = 1
+            line2 = 2
+            line3 = 3
+            line4 = 4
+            line5 = 5
+            line6 = 6
+            line7 = 7
+            line8 = 8
+            line9 = 9
+            line10 = 10
+            line11 = 11
+            line12 = 12
+            line13 = 13
+            line14 = 14
+            line15 = 15
+            return line15
+        """,
+    )
+
+    results = list(collect_long_functions(threshold=10))
+    assert len(results) >= 1
+    assert any(entry.name == "long_function" for entry in results)
+
+
+def test_collect_long_functions_skips_init(tmp_path, monkeypatch):
+    """Test collect_long_functions skips __init__.py files."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        src_root / "__init__.py",
+        """
+        def long_function():
+            line1 = 1
+            line2 = 2
+            line3 = 3
+            line4 = 4
+            line5 = 5
+            line6 = 6
+            line7 = 7
+            line8 = 8
+            line9 = 9
+            line10 = 10
+            return line10
+        """,
+    )
+
+    results = list(collect_long_functions(threshold=5))
+    assert len(results) == 0
+
+
+def test_collect_broad_excepts(tmp_path, monkeypatch):
+    """Test collect_broad_excepts finds broad exception handlers."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except Exception:
+            handle()
+        """,
+    )
+
+    results = collect_broad_excepts()
+    assert len(results) >= 1
+
+
+def test_collect_broad_excepts_with_suppression(tmp_path, monkeypatch):
+    """Test collect_broad_excepts respects suppression comments."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except Exception:  # policy_guard: allow-broad-except
+            handle()
+        """,
+    )
+
+    results = collect_broad_excepts()
+    # Should not include the suppressed handler
+    matching = [r for r in results if "module.py" in r[0]]
+    assert len(matching) == 0
+
+
+def test_collect_broad_excepts_bare_except(tmp_path, monkeypatch):
+    """Test collect_broad_excepts finds bare except."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except:
+            handle()
+        """,
+    )
+
+    results = collect_broad_excepts()
+    assert len(results) >= 1
+
+
+def test_collect_broad_excepts_tuple(tmp_path, monkeypatch):
+    """Test collect_broad_excepts finds Exception in tuple."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except (ValueError, Exception):
+            handle()
+        """,
+    )
+
+    results = collect_broad_excepts()
+    assert len(results) >= 1
+
+
+def test_collect_silent_handlers(tmp_path, monkeypatch):
+    """Test collect_silent_handlers finds silent exception handlers."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except ValueError:
+            pass
+        """,
+    )
+
+    results = collect_silent_handlers()
+    assert len(results) >= 1
+    assert any("pass" in reason for _, _, reason in results)
+
+
+def test_collect_silent_handlers_with_suppression(tmp_path, monkeypatch):
+    """Test collect_silent_handlers respects suppression comments."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        try:
+            risky()
+        except ValueError:  # policy_guard: allow-silent-handler
+            pass
+        """,
+    )
+
+    results = collect_silent_handlers()
+    matching = [r for r in results if "module.py" in r[0]]
+    assert len(matching) == 0
+
+
+def test_collect_generic_raises(tmp_path, monkeypatch):
+    """Test collect_generic_raises finds generic exception raises."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        def foo():
+            raise Exception("generic error")
+        """,
+    )
+
+    results = collect_generic_raises()
+    assert len(results) >= 1
+
+
+def test_collect_generic_raises_base_exception(tmp_path, monkeypatch):
+    """Test collect_generic_raises finds BaseException."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        def foo():
+            raise BaseException()
+        """,
+    )
+
+    results = collect_generic_raises()
+    assert len(results) >= 1
+
+
+def test_collect_literal_fallbacks_dict_get(tmp_path, monkeypatch):
+    """Test collect_literal_fallbacks finds dict.get with literal default."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = data.get('key', 'default')
+        """,
+    )
+
+    results = collect_literal_fallbacks()
+    assert len(results) >= 1
+    assert any("get literal fallback" in reason for _, _, reason in results)
+
+
+def test_collect_literal_fallbacks_getattr(tmp_path, monkeypatch):
+    """Test collect_literal_fallbacks finds getattr with literal default."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = getattr(obj, 'attr', 'default')
+        """,
+    )
+
+    results = collect_literal_fallbacks()
+    assert len(results) >= 1
+
+
+def test_collect_literal_fallbacks_os_getenv(tmp_path, monkeypatch):
+    """Test collect_literal_fallbacks finds os.getenv with literal default."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        import os
+        x = os.getenv('VAR', 'default')
+        """,
+    )
+
+    results = collect_literal_fallbacks()
+    assert len(results) >= 1
+
+
+def test_collect_literal_fallbacks_setdefault(tmp_path, monkeypatch):
+    """Test collect_literal_fallbacks finds setdefault with literal."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        data.setdefault('key', 'default')
+        """,
+    )
+
+    results = collect_literal_fallbacks()
+    assert len(results) >= 1
+
+
+def test_collect_bool_fallbacks_or(tmp_path, monkeypatch):
+    """Test collect_bool_fallbacks finds literal fallback via or."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = value or 'default'
+        """,
+    )
+
+    results = collect_bool_fallbacks()
+    assert len(results) >= 1
+
+
+def test_collect_bool_fallbacks_ternary(tmp_path, monkeypatch):
+    """Test collect_bool_fallbacks finds literal in ternary."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = 'yes' if condition else 'no'
+        """,
+    )
+
+    results = collect_bool_fallbacks()
+    assert len(results) >= 1
+
+
+def test_collect_conditional_literal_returns(tmp_path, monkeypatch):
+    """Test collect_conditional_literal_returns finds literal returns after None check."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def foo(x):
+            if x is None:
+                return 'default'
+        """,
+    )
+
+    results = collect_conditional_literal_returns()
+    assert len(results) >= 1
+
+
+def test_collect_backward_compat_blocks_if_statement(tmp_path, monkeypatch):
+    """Test collect_backward_compat_blocks finds legacy if statements."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        if legacy_mode:
+            handle_legacy()
+        """,
+    )
+
+    results = collect_backward_compat_blocks()
+    assert len(results) >= 1
+    assert any("conditional legacy guard" in reason for _, _, reason in results)
+
+
+def test_collect_backward_compat_blocks_attribute(tmp_path, monkeypatch):
+    """Test collect_backward_compat_blocks finds legacy attributes."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = obj.method_legacy()
+        """,
+    )
+
+    results = collect_backward_compat_blocks()
+    assert len(results) >= 1
+    assert any("legacy attribute" in reason for _, _, reason in results)
+
+
+def test_collect_backward_compat_blocks_name(tmp_path, monkeypatch):
+    """Test collect_backward_compat_blocks finds legacy symbols."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        x = func_deprecated()
+        """,
+    )
+
+    results = collect_backward_compat_blocks()
+    assert len(results) >= 1
+    assert any("legacy symbol" in reason for _, _, reason in results)
+
+
+def test_collect_forbidden_sync_calls_time_sleep(tmp_path, monkeypatch):
+    """Test collect_forbidden_sync_calls finds time.sleep."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        import time
+        time.sleep(1)
+        """,
+    )
+
+    results = collect_forbidden_sync_calls()
+    assert len(results) >= 1
+    assert any("time.sleep" in reason for _, _, reason in results)
+
+
+def test_collect_forbidden_sync_calls_subprocess(tmp_path, monkeypatch):
+    """Test collect_forbidden_sync_calls finds subprocess.run."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        import subprocess
+        subprocess.run(['ls'])
+        """,
+    )
+
+    results = collect_forbidden_sync_calls()
+    assert len(results) >= 1
+    assert any("subprocess.run" in reason for _, _, reason in results)
+
+
+def test_collect_forbidden_sync_calls_requests(tmp_path, monkeypatch):
+    """Test collect_forbidden_sync_calls finds requests.get."""
+    src_root = tmp_path / "src"
+    src_root.mkdir()
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    write_module(
+        src_root / "module.py",
+        """
+        import requests
+        requests.get('http://example.com')
+        """,
+    )
+
+    results = collect_forbidden_sync_calls()
+    assert len(results) >= 1
+    assert any("requests.get" in reason for _, _, reason in results)
+
+
+def test_collect_duplicate_functions(tmp_path, monkeypatch):
+    """Test collect_duplicate_functions finds duplicate implementations."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module1.py",
+        """
+        def helper(x):
+            result = x + 1
+            return result
+        """,
+    )
+
+    write_module(
+        tmp_path / "module2.py",
+        """
+        def helper(y):
+            output = y + 1
+            return output
+        """,
+    )
+
+    results = collect_duplicate_functions(min_length=3)
+    assert len(results) >= 1
+
+
+def test_collect_duplicate_functions_min_length(tmp_path, monkeypatch):
+    """Test collect_duplicate_functions respects min_length."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module1.py",
+        """
+        def tiny():
+            return 1
+        """,
+    )
+
+    write_module(
+        tmp_path / "module2.py",
+        """
+        def tiny():
+            return 1
+        """,
+    )
+
+    results = collect_duplicate_functions(min_length=10)
+    assert len(results) == 0
+
+
+def test_collect_duplicate_functions_same_file(tmp_path, monkeypatch):
+    """Test collect_duplicate_functions ignores duplicates in same file."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def helper1(x):
+            result = x + 1
+            return result
+
+        def helper2(y):
+            output = y + 1
+            return output
+        """,
+    )
+
+    results = collect_duplicate_functions(min_length=3)
+    # Should not report duplicates from same file
+    matching = [
+        group for group in results
+        if all("module.py" in str(entry.path) for entry in group)
+    ]
+    assert len(matching) == 0
+
+
+def test_collect_bytecode_artifacts(tmp_path, monkeypatch):
+    """Test collect_bytecode_artifacts finds .pyc files."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    (tmp_path / "module.pyc").write_bytes(b"fake bytecode")
+    pycache = tmp_path / "__pycache__"
+    pycache.mkdir()
+
+    results = collect_bytecode_artifacts()
+    assert len(results) >= 2
+    assert any(".pyc" in path for path in results)
+    assert any("__pycache__" in path for path in results)
+
+
+def test_purge_bytecode_artifacts(tmp_path, monkeypatch):
+    """Test purge_bytecode_artifacts removes .pyc files."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_ast.ROOT", tmp_path)
+
+    pyc_file = tmp_path / "module.pyc"
+    pyc_file.write_bytes(b"fake bytecode")
+    pycache = tmp_path / "__pycache__"
+    pycache.mkdir()
+    (pycache / "test.pyc").write_bytes(b"fake")
+
+    assert pyc_file.exists()
+    assert pycache.exists()
+
+    purge_bytecode_artifacts()
+
+    assert not pyc_file.exists()
+    assert not pycache.exists()
+
+
+def test_purge_bytecode_artifacts_handles_missing(tmp_path, monkeypatch):
+    """Test purge_bytecode_artifacts handles already deleted files."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    # Run on empty directory
+    purge_bytecode_artifacts()
+    # Should not raise any errors
diff --git a/tests/test_policy_collectors_text.py b/tests/test_policy_collectors_text.py
new file mode 100644
index 0000000..6fe93ac
--- /dev/null
+++ b/tests/test_policy_collectors_text.py
@@ -0,0 +1,496 @@
+"""Unit tests for policy_collectors_text module."""
+
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+
+
+from ci_tools.scripts.policy_collectors_text import (
+    collect_flagged_tokens,
+    collect_legacy_configs,
+    collect_legacy_modules,
+    collect_suppressions,
+    scan_keywords,
+)
+
+
+def write_module(path: Path, source: str) -> None:
+    """Helper to write a Python module."""
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(textwrap.dedent(source).strip() + "\n", encoding="utf-8")
+
+
+def write_file(path: Path, content: str) -> None:
+    """Helper to write a file."""
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(content, encoding="utf-8")
+
+
+def test_scan_keywords_finds_banned_keywords(tmp_path, monkeypatch):
+    """Test scan_keywords finds banned keywords."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def legacy_handler():
+            return fallback_value
+        """,
+    )
+
+    results = scan_keywords()
+    assert "legacy" in results
+    assert "fallback" in results
+
+
+def test_scan_keywords_case_insensitive(tmp_path, monkeypatch):
+    """Test scan_keywords is case insensitive."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def LEGACY_handler():
+            return Fallback_value
+        """,
+    )
+
+    results = scan_keywords()
+    assert "legacy" in results or "LEGACY" in results.get("legacy", {})
+    assert "fallback" in results or "Fallback" in results.get("fallback", {})
+
+
+def test_scan_keywords_tracks_line_numbers(tmp_path, monkeypatch):
+    """Test scan_keywords tracks line numbers."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        legacy = True
+        x = 1
+        legacy = False
+        """,
+    )
+
+    results = scan_keywords()
+    if "legacy" in results:
+        files = results["legacy"]
+        assert len(files) > 0
+        for lines in files.values():
+            assert len(lines) >= 2
+
+
+def test_scan_keywords_handles_tokenize_error(tmp_path, monkeypatch):
+    """Test scan_keywords handles tokenization errors gracefully."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    # Write file with incomplete string that causes tokenize error
+    path = tmp_path / "bad.py"
+    path.write_text('x = "incomplete string\n', encoding="utf-8")
+
+    # Should not raise, just skip the file
+    scan_keywords()
+    # Just ensure it completes without error
+
+
+def test_scan_keywords_filters_by_name_token_type(tmp_path, monkeypatch):
+    """Test scan_keywords only matches NAME tokens."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # Comment with legacy keyword
+        x = "legacy string"
+        legacy_variable = 1
+        """,
+    )
+
+    scan_keywords()
+    # Should find legacy_variable but not the string or comment
+    # This is a characteristic test to ensure tokenization works
+
+
+def test_collect_flagged_tokens_todo(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds TODO."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # TODO: implement this
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "TODO" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_fixme(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds FIXME."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # FIXME: broken logic
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "FIXME" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_hack(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds HACK."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # HACK: temporary workaround
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "HACK" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_workaround(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds WORKAROUND."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # WORKAROUND: for known bug
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "WORKAROUND" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_legacy(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds LEGACY."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # LEGACY: old implementation
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "LEGACY" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_deprecated(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens finds DEPRECATED."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        # DEPRECATED: use new_function instead
+        def foo():
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 1
+    assert any(token == "DEPRECATED" for _, _, token in results)
+
+
+def test_collect_flagged_tokens_tracks_line_numbers(tmp_path, monkeypatch):
+    """Test collect_flagged_tokens tracks correct line numbers."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def first():
+            pass
+        # TODO: line 4
+        def second():
+            # FIXME: line 6
+            pass
+        """,
+    )
+
+    results = collect_flagged_tokens()
+    assert len(results) >= 2
+    line_numbers = [lineno for _, lineno, _ in results]
+    assert 3 in line_numbers or 4 in line_numbers  # TODO
+    assert 5 in line_numbers or 6 in line_numbers  # FIXME
+
+
+def test_collect_suppressions_noqa(tmp_path, monkeypatch):
+    """Test collect_suppressions finds # noqa."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def foo():
+            long_line = 1  # noqa
+        """,
+    )
+
+    results = collect_suppressions()
+    assert len(results) >= 1
+    assert any("# noqa" in token for _, _, token in results)
+
+
+def test_collect_suppressions_pylint(tmp_path, monkeypatch):
+    """Test collect_suppressions finds pylint: disable."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def foo():
+            x = 1  # pylint: disable=invalid-name
+        """,
+    )
+
+    results = collect_suppressions()
+    assert len(results) >= 1
+    assert any("pylint: disable" in token for _, _, token in results)
+
+
+def test_collect_suppressions_tracks_line_numbers(tmp_path, monkeypatch):
+    """Test collect_suppressions tracks line numbers."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(
+        tmp_path / "module.py",
+        """
+        def foo():
+            a = 1  # noqa
+            b = 2
+            c = 3  # pylint: disable=foo
+        """,
+    )
+
+    results = collect_suppressions()
+    assert len(results) >= 2
+
+
+def test_collect_legacy_modules_legacy_suffix(tmp_path, monkeypatch):
+    """Test collect_legacy_modules finds _legacy.py suffix."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(tmp_path / "handler_legacy.py", "def foo(): pass")
+
+    results = collect_legacy_modules()
+    assert len(results) >= 1
+    assert any("legacy" in path.lower() for path, _, _ in results)
+
+
+def test_collect_legacy_modules_compat_suffix(tmp_path, monkeypatch):
+    """Test collect_legacy_modules finds _compat.py suffix."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(tmp_path / "utils_compat.py", "def foo(): pass")
+
+    results = collect_legacy_modules()
+    assert len(results) >= 1
+    assert any("compat" in path.lower() for path, _, _ in results)
+
+
+def test_collect_legacy_modules_deprecated_suffix(tmp_path, monkeypatch):
+    """Test collect_legacy_modules finds _deprecated.py suffix."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    write_module(tmp_path / "api_deprecated.py", "def foo(): pass")
+
+    results = collect_legacy_modules()
+    assert len(results) >= 1
+    assert any("deprecated" in path.lower() for path, _, _ in results)
+
+
+def test_collect_legacy_modules_legacy_directory(tmp_path, monkeypatch):
+    """Test collect_legacy_modules finds legacy directory."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    legacy_dir = tmp_path / "legacy"
+    legacy_dir.mkdir()
+    write_module(legacy_dir / "module.py", "def foo(): pass")
+
+    results = collect_legacy_modules()
+    assert len(results) >= 1
+    assert any(
+        "/legacy/" in path or "\\legacy\\" in path or path.startswith("legacy/") or path.startswith("legacy\\")
+        for path, _, _ in results
+    )
+
+
+def test_collect_legacy_modules_compat_directory(tmp_path, monkeypatch):
+    """Test collect_legacy_modules finds compat directory."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    compat_dir = tmp_path / "compat"
+    compat_dir.mkdir()
+    write_module(compat_dir / "module.py", "def foo(): pass")
+
+    results = collect_legacy_modules()
+    assert len(results) >= 1
+    assert any(
+        "/compat/" in path or "\\compat\\" in path or path.startswith("compat/") or path.startswith("compat\\")
+        for path, _, _ in results
+    )
+
+
+def test_collect_legacy_configs_json(tmp_path, monkeypatch):
+    """Test collect_legacy_configs finds legacy in JSON config."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.json",
+        '{"legacy_mode": true, "other": "value"}',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+    assert any("legacy" in reason.lower() for _, _, reason in results)
+
+
+def test_collect_legacy_configs_toml(tmp_path, monkeypatch):
+    """Test collect_legacy_configs finds legacy in TOML config."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.toml",
+        '[section]\nlegacy_flag = true\n',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+
+
+def test_collect_legacy_configs_yaml(tmp_path, monkeypatch):
+    """Test collect_legacy_configs finds legacy in YAML config."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.yaml",
+        'old_api: true\ncompat: enabled\n',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+
+
+def test_collect_legacy_configs_yml(tmp_path, monkeypatch):
+    """Test collect_legacy_configs finds legacy in .yml config."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.yml",
+        'deprecated: true\n',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+
+
+def test_collect_legacy_configs_ini(tmp_path, monkeypatch):
+    """Test collect_legacy_configs finds legacy in INI config."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.ini",
+        '[section]\nlegacy = true\n',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+
+
+def test_collect_legacy_configs_ignores_non_config_files(tmp_path, monkeypatch):
+    """Test collect_legacy_configs ignores non-config files."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(config_dir / "readme.txt", "legacy documentation")
+
+    results = collect_legacy_configs()
+    # Should not find legacy in .txt file
+    matching = [r for r in results if "readme.txt" in r[0]]
+    assert len(matching) == 0
+
+
+def test_collect_legacy_configs_handles_unicode_error(tmp_path, monkeypatch):
+    """Test collect_legacy_configs handles unicode decode errors."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    bad_file = config_dir / "bad.json"
+    bad_file.write_bytes(b"\xff\xfe\xff\xfe")
+
+    # Should not raise
+    collect_legacy_configs()
+
+
+def test_collect_legacy_configs_no_config_dir(tmp_path, monkeypatch):
+    """Test collect_legacy_configs handles missing config directory."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+
+    # No config directory exists
+    results = collect_legacy_configs()
+    assert results == []
+
+
+def test_collect_legacy_configs_tracks_line_numbers(tmp_path, monkeypatch):
+    """Test collect_legacy_configs tracks correct line numbers."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    monkeypatch.setattr("ci_tools.scripts.policy_collectors_text.ROOT", tmp_path)
+
+    config_dir = tmp_path / "config"
+    config_dir.mkdir()
+    write_file(
+        config_dir / "settings.json",
+        '{\n  "normal": true,\n  "legacy_mode": true,\n  "other": false\n}',
+    )
+
+    results = collect_legacy_configs()
+    assert len(results) >= 1
+    # Line 3 should contain legacy_mode
+    assert any(lineno == 3 for _, lineno, _ in results)
diff --git a/tests/test_policy_context.py b/tests/test_policy_context.py
new file mode 100644
index 0000000..e1f735c
--- /dev/null
+++ b/tests/test_policy_context.py
@@ -0,0 +1,602 @@
+"""Unit tests for policy_context module."""
+
+from __future__ import annotations
+
+import ast
+import textwrap
+from pathlib import Path
+
+import pytest
+
+from ci_tools.scripts.policy_context import (
+    BANNED_KEYWORDS,
+    BROAD_EXCEPT_SUPPRESSION,
+    BROAD_EXCEPTION_NAMES,
+    CONFIG_EXTENSIONS,
+    FLAGGED_TOKENS,
+    FORBIDDEN_SYNC_CALLS,
+    FUNCTION_LENGTH_THRESHOLD,
+    LEGACY_CONFIG_TOKENS,
+    LEGACY_GUARD_TOKENS,
+    LEGACY_SUFFIXES,
+    ROOT,
+    SCAN_DIRECTORIES,
+    SILENT_HANDLER_SUPPRESSION,
+    SUPPRESSION_PATTERNS,
+    FunctionEntry,
+    FunctionNormalizer,
+    ModuleContext,
+    _resolve_default_argument,
+    classify_handler,
+    get_call_qualname,
+    handler_contains_suppression,
+    handler_has_raise,
+    is_literal_none_guard,
+    is_logging_call,
+    is_non_none_literal,
+    iter_module_contexts,
+    iter_python_files,
+    normalize_function,
+    normalize_path,
+    parse_ast,
+)
+
+
+def test_constants_are_defined():
+    """Test that all expected constants are defined."""
+    assert isinstance(ROOT, Path)
+    assert isinstance(SCAN_DIRECTORIES, tuple)
+    assert isinstance(BANNED_KEYWORDS, tuple)
+    assert isinstance(FLAGGED_TOKENS, tuple)
+    assert isinstance(FUNCTION_LENGTH_THRESHOLD, int)
+    assert isinstance(BROAD_EXCEPT_SUPPRESSION, str)
+    assert isinstance(SILENT_HANDLER_SUPPRESSION, str)
+    assert isinstance(SUPPRESSION_PATTERNS, tuple)
+    assert isinstance(FORBIDDEN_SYNC_CALLS, tuple)
+    assert isinstance(LEGACY_GUARD_TOKENS, tuple)
+    assert isinstance(LEGACY_SUFFIXES, tuple)
+    assert isinstance(LEGACY_CONFIG_TOKENS, tuple)
+    assert isinstance(CONFIG_EXTENSIONS, tuple)
+    assert isinstance(BROAD_EXCEPTION_NAMES, set)
+
+
+def test_function_entry_dataclass():
+    """Test FunctionEntry dataclass creation."""
+    entry = FunctionEntry(
+        path=Path("/test/file.py"),
+        name="test_func",
+        lineno=10,
+        length=20,
+    )
+    assert entry.path == Path("/test/file.py")
+    assert entry.name == "test_func"
+    assert entry.lineno == 10
+    assert entry.length == 20
+
+
+def test_function_entry_is_frozen():
+    """Test FunctionEntry is immutable."""
+    entry = FunctionEntry(
+        path=Path("/test/file.py"),
+        name="test_func",
+        lineno=10,
+        length=20,
+    )
+    with pytest.raises(AttributeError):
+        entry.name = "other"
+
+
+def test_module_context_dataclass():
+    """Test ModuleContext dataclass creation."""
+    tree = ast.parse("x = 1")
+    ctx = ModuleContext(
+        path=Path("/test/file.py"),
+        rel_path="test/file.py",
+        tree=tree,
+        source="x = 1",
+        lines=["x = 1"],
+    )
+    assert ctx.path == Path("/test/file.py")
+    assert ctx.rel_path == "test/file.py"
+    assert ctx.tree is tree
+    assert ctx.source == "x = 1"
+    assert ctx.lines == ["x = 1"]
+
+
+def test_module_context_optional_fields():
+    """Test ModuleContext with optional fields omitted."""
+    tree = ast.parse("x = 1")
+    ctx = ModuleContext(
+        path=Path("/test/file.py"),
+        rel_path="test/file.py",
+        tree=tree,
+    )
+    assert ctx.source is None
+    assert ctx.lines is None
+
+
+def test_function_normalizer_visit_name():
+    """Test FunctionNormalizer normalizes Name nodes."""
+    source = "x = y + z"
+    tree = ast.parse(source)
+    normalizer = FunctionNormalizer()
+    normalized = normalizer.visit(tree)
+    code = ast.unparse(normalized)
+    assert "var" in code
+
+
+def test_function_normalizer_visit_arg():
+    """Test FunctionNormalizer normalizes arg nodes."""
+    source = "def foo(bar: int, baz: str) -> None: pass"
+    tree = ast.parse(source)
+    normalizer = FunctionNormalizer()
+    normalized = normalizer.visit(tree)
+    func = normalized.body[0]
+    assert all(arg.arg == "arg" for arg in func.args.args)
+
+
+def test_normalize_function_simple():
+    """Test normalize_function with simple function."""
+    source = textwrap.dedent("""
+        def foo(x, y):
+            return x + y
+    """)
+    tree = ast.parse(source)
+    func = tree.body[0]
+    result = normalize_function(func)
+    assert isinstance(result, str)
+    assert "FunctionDef" in result
+
+
+def test_normalize_function_with_docstring():
+    """Test normalize_function removes docstring."""
+    source = textwrap.dedent("""
+        def foo(x):
+            \"\"\"Docstring here.\"\"\"
+            return x + 1
+    """)
+    tree = ast.parse(source)
+    func = tree.body[0]
+    result = normalize_function(func)
+    # Docstring should be removed from normalized form
+    assert isinstance(result, str)
+
+
+def test_normalize_function_async():
+    """Test normalize_function with async function."""
+    source = textwrap.dedent("""
+        async def foo(x, y):
+            return x + y
+    """)
+    tree = ast.parse(source)
+    func = tree.body[0]
+    result = normalize_function(func)
+    assert isinstance(result, str)
+    assert "AsyncFunctionDef" in result
+
+
+def test_normalize_path(tmp_path, monkeypatch):
+    """Test normalize_path converts to relative path."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    test_file = tmp_path / "subdir" / "file.py"
+    result = normalize_path(test_file)
+    assert result == "subdir/file.py"
+
+
+def test_iter_python_files_finds_files(tmp_path):
+    """Test iter_python_files finds Python files."""
+    (tmp_path / "file1.py").write_text("x = 1")
+    (tmp_path / "file2.py").write_text("y = 2")
+    subdir = tmp_path / "subdir"
+    subdir.mkdir()
+    (subdir / "file3.py").write_text("z = 3")
+
+    files = list(iter_python_files([tmp_path]))
+    assert len(files) == 3
+    assert all(f.suffix == ".py" for f in files)
+
+
+def test_iter_python_files_ignores_non_python(tmp_path):
+    """Test iter_python_files ignores non-Python files."""
+    (tmp_path / "file.py").write_text("x = 1")
+    (tmp_path / "file.txt").write_text("text")
+    (tmp_path / "file.md").write_text("markdown")
+
+    files = list(iter_python_files([tmp_path]))
+    assert len(files) == 1
+    assert files[0].suffix == ".py"
+
+
+def test_iter_python_files_handles_nonexistent_path(tmp_path):
+    """Test iter_python_files handles non-existent base path."""
+    missing = tmp_path / "missing"
+    files = list(iter_python_files([missing]))
+    assert files == []
+
+
+def test_parse_ast_valid_syntax(tmp_path):
+    """Test parse_ast with valid Python syntax."""
+    test_file = tmp_path / "valid.py"
+    test_file.write_text("x = 1\ny = 2")
+    result = parse_ast(test_file)
+    assert isinstance(result, ast.Module)
+
+
+def test_parse_ast_invalid_syntax(tmp_path):
+    """Test parse_ast with invalid Python syntax."""
+    test_file = tmp_path / "invalid.py"
+    test_file.write_text("def foo(\n  syntax error")
+    result = parse_ast(test_file)
+    assert result is None
+
+
+def test_iter_module_contexts(tmp_path, monkeypatch):
+    """Test iter_module_contexts yields contexts."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    (tmp_path / "file1.py").write_text("x = 1")
+    (tmp_path / "file2.py").write_text("y = 2")
+
+    contexts = list(iter_module_contexts([tmp_path]))
+    assert len(contexts) == 2
+    assert all(isinstance(ctx.tree, ast.Module) for ctx in contexts)
+    assert all(ctx.source is None for ctx in contexts)
+    assert all(ctx.lines is None for ctx in contexts)
+
+
+def test_iter_module_contexts_with_source(tmp_path, monkeypatch):
+    """Test iter_module_contexts with include_source."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    (tmp_path / "file.py").write_text("x = 1")
+
+    contexts = list(iter_module_contexts([tmp_path], include_source=True))
+    assert len(contexts) == 1
+    assert contexts[0].source == "x = 1"
+
+
+def test_iter_module_contexts_with_lines(tmp_path, monkeypatch):
+    """Test iter_module_contexts with include_lines."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    (tmp_path / "file.py").write_text("x = 1\ny = 2")
+
+    contexts = list(iter_module_contexts([tmp_path], include_lines=True))
+    assert len(contexts) == 1
+    assert contexts[0].lines == ["x = 1", "y = 2"]
+
+
+def test_iter_module_contexts_skips_syntax_errors(tmp_path, monkeypatch):
+    """Test iter_module_contexts skips files with syntax errors."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    (tmp_path / "valid.py").write_text("x = 1")
+    (tmp_path / "invalid.py").write_text("def foo(\n  syntax error")
+
+    contexts = list(iter_module_contexts([tmp_path]))
+    assert len(contexts) == 1
+
+
+def test_iter_module_contexts_skips_unicode_errors(tmp_path, monkeypatch):
+    """Test iter_module_contexts skips files with unicode errors."""
+    monkeypatch.setattr("ci_tools.scripts.policy_context.ROOT", tmp_path)
+    (tmp_path / "valid.py").write_text("x = 1")
+    invalid = tmp_path / "invalid.py"
+    invalid.write_bytes(b"\xff\xfe\xff\xfe")
+
+    contexts = list(iter_module_contexts([tmp_path]))
+    assert len(contexts) == 1
+
+
+def test_resolve_default_argument_positional():
+    """Test _resolve_default_argument with positional args."""
+    source = "func(1, 2, 3)"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = _resolve_default_argument(call, positional_index=1, keyword_names={"key"})
+    assert isinstance(result, ast.Constant)
+    assert result.value == 2
+
+
+def test_resolve_default_argument_keyword():
+    """Test _resolve_default_argument with keyword args."""
+    source = "func(1, default=42)"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = _resolve_default_argument(call, positional_index=5, keyword_names={"default"})
+    assert isinstance(result, ast.Constant)
+    assert result.value == 42
+
+
+def test_resolve_default_argument_not_found():
+    """Test _resolve_default_argument when argument not found."""
+    source = "func(1, 2)"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = _resolve_default_argument(call, positional_index=5, keyword_names={"missing"})
+    assert result is None
+
+
+def test_get_call_qualname_simple():
+    """Test get_call_qualname with simple name."""
+    source = "foo()"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = get_call_qualname(call.func)
+    assert result == "foo"
+
+
+def test_get_call_qualname_attribute():
+    """Test get_call_qualname with attribute access."""
+    source = "obj.method()"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = get_call_qualname(call.func)
+    assert result == "obj.method"
+
+
+def test_get_call_qualname_nested_attribute():
+    """Test get_call_qualname with nested attribute access."""
+    source = "a.b.c.d()"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = get_call_qualname(call.func)
+    assert result == "a.b.c.d"
+
+
+def test_get_call_qualname_unsupported():
+    """Test get_call_qualname with unsupported node type."""
+    source = "(lambda: None)()"
+    tree = ast.parse(source)
+    call = tree.body[0].value
+    result = get_call_qualname(call.func)
+    assert result is None
+
+
+def test_is_non_none_literal_constant():
+    """Test is_non_none_literal with non-None constant."""
+    source = "42"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert is_non_none_literal(node) is True
+
+
+def test_is_non_none_literal_none():
+    """Test is_non_none_literal with None."""
+    source = "None"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert is_non_none_literal(node) is False
+
+
+def test_is_non_none_literal_not_constant():
+    """Test is_non_none_literal with non-constant."""
+    source = "x + 1"
+    tree = ast.parse(source)
+    node = tree.body[0].value
+    assert is_non_none_literal(node) is False
+
+
+def test_is_logging_call_true():
+    """Test is_logging_call recognizes logging calls."""
+    source = "logging.info('message')"
+    tree = ast.parse(source)
+    node = tree.body[0]
+    assert is_logging_call(node) is True
+
+
+def test_is_logging_call_false():
+    """Test is_logging_call returns false for non-logging calls."""
+    source = "print('message')"
+    tree = ast.parse(source)
+    node = tree.body[0]
+    assert is_logging_call(node) is False
+
+
+def test_is_logging_call_not_expr():
+    """Test is_logging_call with non-Expr node."""
+    source = "x = 1"
+    tree = ast.parse(source)
+    node = tree.body[0]
+    assert is_logging_call(node) is False
+
+
+def test_handler_has_raise_true():
+    """Test handler_has_raise detects raise statement."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception as e:
+            process(e)
+            raise
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    assert handler_has_raise(handler) is True
+
+
+def test_handler_has_raise_false():
+    """Test handler_has_raise returns false without raise."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:
+            pass
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    assert handler_has_raise(handler) is False
+
+
+def test_classify_handler_with_raise():
+    """Test classify_handler returns None for handlers with raise."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:
+            raise
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    assert classify_handler(handler) is None
+
+
+def test_classify_handler_empty():
+    """Test classify_handler detects empty handler."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:
+            pass
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "suppresses exception with pass" in result
+
+
+def test_classify_handler_continue():
+    """Test classify_handler detects continue."""
+    source = textwrap.dedent("""
+        for i in range(10):
+            try:
+                risky()
+            except Exception:
+                continue
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "suppresses exception with continue" in result
+
+
+def test_classify_handler_break():
+    """Test classify_handler detects break."""
+    source = textwrap.dedent("""
+        while True:
+            try:
+                risky()
+            except Exception:
+                break
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "suppresses exception with break" in result
+
+
+def test_classify_handler_literal_return():
+    """Test classify_handler detects literal return."""
+    source = textwrap.dedent("""
+        def foo():
+            try:
+                risky()
+            except Exception:
+                return None
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "suppresses exception with literal return" in result
+
+
+def test_classify_handler_logging():
+    """Test classify_handler detects logging without re-raise."""
+    source = textwrap.dedent("""
+        def foo():
+            try:
+                risky()
+            except Exception as e:
+                logging.error(str(e))
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "logs exception without re-raising" in result
+
+
+def test_classify_handler_no_reraise():
+    """Test classify_handler detects handler without re-raise."""
+    source = textwrap.dedent("""
+        def foo():
+            try:
+                risky()
+            except Exception:
+                handle_error()
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].body[0].handlers[0]
+    result = classify_handler(handler)
+    assert "exception handler without re-raise" in result
+
+
+def test_is_literal_none_guard_is():
+    """Test is_literal_none_guard with 'is None' check."""
+    source = "if x is None: pass"
+    tree = ast.parse(source)
+    test = tree.body[0].test
+    assert is_literal_none_guard(test) is True
+
+
+def test_is_literal_none_guard_eq():
+    """Test is_literal_none_guard with '== None' check."""
+    source = "if x == None: pass"
+    tree = ast.parse(source)
+    test = tree.body[0].test
+    assert is_literal_none_guard(test) is True
+
+
+def test_is_literal_none_guard_false():
+    """Test is_literal_none_guard with non-None check."""
+    source = "if x > 0: pass"
+    tree = ast.parse(source)
+    test = tree.body[0].test
+    assert is_literal_none_guard(test) is False
+
+
+def test_is_literal_none_guard_multiple_comparators():
+    """Test is_literal_none_guard with multiple comparators."""
+    source = "if 0 < x < 10: pass"
+    tree = ast.parse(source)
+    test = tree.body[0].test
+    assert is_literal_none_guard(test) is False
+
+
+def test_handler_contains_suppression_true():
+    """Test handler_contains_suppression finds suppression token."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:  # policy_guard: allow-broad-except
+            pass
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    lines = source.splitlines()
+    result = handler_contains_suppression(handler, lines, "policy_guard: allow-broad-except")
+    assert result is True
+
+
+def test_handler_contains_suppression_false():
+    """Test handler_contains_suppression returns false without token."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:
+            pass
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    lines = source.splitlines()
+    result = handler_contains_suppression(handler, lines, "policy_guard: allow-broad-except")
+    assert result is False
+
+
+def test_handler_contains_suppression_empty_lines():
+    """Test handler_contains_suppression with empty lines."""
+    source = textwrap.dedent("""
+        try:
+            risky()
+        except Exception:
+            pass
+    """)
+    tree = ast.parse(source)
+    handler = tree.body[0].handlers[0]
+    result = handler_contains_suppression(handler, [], "token")
+    assert result is False
diff --git a/tests/test_policy_guard.py b/tests/test_policy_guard.py
new file mode 100644
index 0000000..faa5f45
--- /dev/null
+++ b/tests/test_policy_guard.py
@@ -0,0 +1,137 @@
+"""Unit tests for policy_guard module."""
+
+from __future__ import annotations
+
+import sys
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts.policy_guard import PolicyViolation, main, purge_bytecode_artifacts
+
+
+def test_policy_violation_imported():
+    """Test PolicyViolation is correctly imported from policy_checks."""
+    assert PolicyViolation is not None
+    exc = PolicyViolation("test")
+    assert isinstance(exc, Exception)
+
+
+def test_purge_bytecode_artifacts_imported():
+    """Test purge_bytecode_artifacts is correctly imported."""
+    assert purge_bytecode_artifacts is not None
+    assert callable(purge_bytecode_artifacts)
+
+
+def test_main_delegates_to_policy_checks():
+    """Test main delegates to policy_checks.main."""
+    with patch("ci_tools.scripts.policy_guard._run_policy_checks", return_value=0) as mock_run:
+        result = main()
+        assert result == 0
+        mock_run.assert_called_once()
+
+
+def test_main_returns_exit_code():
+    """Test main returns the exit code from policy_checks."""
+    with patch("ci_tools.scripts.policy_guard._run_policy_checks", return_value=42):
+        result = main()
+        assert result == 42
+
+
+def test_main_propagates_exceptions():
+    """Test main propagates exceptions from policy_checks."""
+    with patch("ci_tools.scripts.policy_guard._run_policy_checks",
+               side_effect=RuntimeError("test error")):
+        with pytest.raises(RuntimeError) as exc:
+            main()
+        assert "test error" in str(exc.value)
+
+
+def test_module_exports():
+    """Test module exports expected symbols."""
+    from ci_tools.scripts import policy_guard
+
+    assert hasattr(policy_guard, "PolicyViolation")
+    assert hasattr(policy_guard, "purge_bytecode_artifacts")
+    assert hasattr(policy_guard, "main")
+
+
+def test_all_contains_expected_exports():
+    """Test __all__ contains expected exports."""
+    from ci_tools.scripts import policy_guard
+
+    assert "PolicyViolation" in policy_guard.__all__
+    assert "purge_bytecode_artifacts" in policy_guard.__all__
+    assert "main" in policy_guard.__all__
+
+
+def test_main_as_script_success():
+    """Test running module as script with successful checks."""
+    with patch("ci_tools.scripts.policy_guard.main", return_value=0):
+        with pytest.raises(SystemExit) as exc:
+            # Simulate running as __main__
+            exec(compile(
+                "import sys; from ci_tools.scripts.policy_guard import main; sys.exit(main())",
+                "<string>",
+                "exec"
+            ))
+        assert exc.value.code == 0
+
+
+def test_main_as_script_with_violation(capsys):
+    """Test running module as script with policy violation."""
+    def mock_main():
+        raise PolicyViolation("test violation")
+
+    with patch("ci_tools.scripts.policy_guard.main", side_effect=mock_main):
+        with pytest.raises(SystemExit) as exc:
+            # Simulate the __main__ block behavior
+            try:
+                mock_main()
+            except PolicyViolation as err:
+                print(err, file=sys.stderr)
+                raise SystemExit(1)
+
+        assert exc.value.code == 1
+
+
+def test_main_is_thin_wrapper():
+    """Test main is marked as thin wrapper (no cover pragma)."""
+    import inspect
+    from ci_tools.scripts import policy_guard
+
+    # Read source to verify pragma
+    source = inspect.getsource(policy_guard.main)
+    assert "pragma: no cover" in source
+
+
+def test_policy_violation_handling_in_main_block(capsys):
+    """Test PolicyViolation is caught and handled in __main__ block."""
+    test_error_message = "Policy check failed"
+
+    def mock_main():
+        raise PolicyViolation(test_error_message)
+
+    # Simulate the __main__ block
+    with pytest.raises(SystemExit) as exc:
+        try:
+            mock_main()
+        except PolicyViolation as err:
+            print(err, file=sys.stderr)
+            sys.exit(1)
+
+    assert exc.value.code == 1
+    captured = capsys.readouterr()
+    assert test_error_message in captured.err
+
+
+def test_main_with_zero_return():
+    """Test main returns 0 on success."""
+    with patch("ci_tools.scripts.policy_guard._run_policy_checks", return_value=0):
+        assert main() == 0
+
+
+def test_main_with_nonzero_return():
+    """Test main returns non-zero on failure."""
+    with patch("ci_tools.scripts.policy_guard._run_policy_checks", return_value=1):
+        assert main() == 1
diff --git a/tests/test_policy_rules.py b/tests/test_policy_rules.py
new file mode 100644
index 0000000..f081871
--- /dev/null
+++ b/tests/test_policy_rules.py
@@ -0,0 +1,409 @@
+"""Unit tests for policy_rules module."""
+
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts.policy_context import FunctionEntry
+from ci_tools.scripts.policy_rules import (
+    PolicyViolation,
+    _check_backward_compat,
+    _check_boolean_fallbacks,
+    _check_broad_excepts,
+    _check_bytecode_artifacts,
+    _check_conditional_literals,
+    _check_duplicate_functions,
+    _check_flagged_tokens,
+    _check_function_lengths,
+    _check_generic_raises,
+    _check_keyword_policy,
+    _check_legacy_artifacts,
+    _check_literal_fallbacks,
+    _check_silent_handlers,
+    _check_suppressions,
+    _check_sync_calls,
+    enforce_duplicate_functions,
+    enforce_function_lengths,
+    enforce_occurrences,
+    purge_bytecode_artifacts,
+)
+
+
+def write_module(path: Path, source: str) -> None:
+    """Helper to write a Python module."""
+    path.parent.mkdir(parents=True, exist_ok=True)
+    path.write_text(textwrap.dedent(source).strip() + "\n", encoding="utf-8")
+
+
+def test_policy_violation_exception():
+    """Test PolicyViolation is an Exception."""
+    exc = PolicyViolation("test message")
+    assert isinstance(exc, Exception)
+    assert str(exc) == "test message"
+
+
+def test_enforce_occurrences_no_violations():
+    """Test enforce_occurrences with no violations."""
+    enforce_occurrences([], "test message")
+    # Should not raise
+
+
+def test_enforce_occurrences_with_violations():
+    """Test enforce_occurrences with violations."""
+    discovered = [("file.py", 10), ("file.py", 20)]
+    with pytest.raises(PolicyViolation) as exc:
+        enforce_occurrences(discovered, "test issue")
+    assert "Policy violations detected" in str(exc.value)
+    assert "file.py:10" in str(exc.value)
+    assert "test issue" in str(exc.value)
+
+
+def test_enforce_occurrences_sorts_violations():
+    """Test enforce_occurrences sorts violations."""
+    discovered = [("z.py", 10), ("a.py", 5), ("m.py", 15)]
+    with pytest.raises(PolicyViolation) as exc:
+        enforce_occurrences(discovered, "issue")
+    message = str(exc.value)
+    # Check that a.py appears before z.py in sorted output
+    a_pos = message.index("a.py")
+    z_pos = message.index("z.py")
+    assert a_pos < z_pos
+
+
+def test_enforce_duplicate_functions_no_duplicates():
+    """Test enforce_duplicate_functions with no duplicates."""
+    enforce_duplicate_functions([])
+    # Should not raise
+
+
+def test_enforce_duplicate_functions_with_duplicates():
+    """Test enforce_duplicate_functions with duplicates."""
+    duplicates = [
+        [
+            FunctionEntry(Path("file1.py"), "helper", 10, 5),
+            FunctionEntry(Path("file2.py"), "helper", 20, 5),
+        ]
+    ]
+    with pytest.raises(PolicyViolation) as exc:
+        enforce_duplicate_functions(duplicates)
+    assert "Duplicate helper policy violations detected" in str(exc.value)
+    assert "file1.py:10" in str(exc.value)
+    assert "file2.py:20" in str(exc.value)
+
+
+def test_enforce_duplicate_functions_multiple_groups():
+    """Test enforce_duplicate_functions with multiple groups."""
+    duplicates = [
+        [
+            FunctionEntry(Path("a.py"), "func1", 10, 5),
+            FunctionEntry(Path("b.py"), "func1", 20, 5),
+        ],
+        [
+            FunctionEntry(Path("c.py"), "func2", 30, 5),
+            FunctionEntry(Path("d.py"), "func2", 40, 5),
+        ],
+    ]
+    with pytest.raises(PolicyViolation) as exc:
+        enforce_duplicate_functions(duplicates)
+    message = str(exc.value)
+    assert "func1" in message
+    assert "func2" in message
+
+
+def test_check_keyword_policy_no_violations():
+    """Test _check_keyword_policy with no violations."""
+    with patch("ci_tools.scripts.policy_rules.scan_keywords", return_value={}):
+        _check_keyword_policy()
+        # Should not raise
+
+
+def test_check_keyword_policy_with_violations():
+    """Test _check_keyword_policy with violations."""
+    mock_hits = {"legacy": {"file.py": [10, 20]}}
+    with patch("ci_tools.scripts.policy_rules.scan_keywords", return_value=mock_hits):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_keyword_policy()
+        assert "Banned keyword policy violations detected" in str(exc.value)
+        assert "keyword 'legacy'" in str(exc.value)
+
+
+def test_check_flagged_tokens_no_violations():
+    """Test _check_flagged_tokens with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_flagged_tokens", return_value=[]):
+        _check_flagged_tokens()
+        # Should not raise
+
+
+def test_check_flagged_tokens_with_violations():
+    """Test _check_flagged_tokens with violations."""
+    mock_tokens = [("file.py", 10, "TODO"), ("file.py", 20, "FIXME")]
+    with patch("ci_tools.scripts.policy_rules.collect_flagged_tokens", return_value=mock_tokens):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_flagged_tokens()
+        assert "Flagged annotations detected" in str(exc.value)
+        assert "TODO" in str(exc.value)
+        assert "FIXME" in str(exc.value)
+
+
+def test_check_function_lengths_no_violations():
+    """Test _check_function_lengths with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_long_functions", return_value=[]):
+        _check_function_lengths()
+        # Should not raise
+
+
+def test_check_function_lengths_with_violations():
+    """Test _check_function_lengths with violations."""
+    long_funcs = [FunctionEntry(Path("file.py"), "big_func", 10, 200)]
+    with patch("ci_tools.scripts.policy_rules.collect_long_functions", return_value=long_funcs):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_function_lengths()
+        assert "Function length policy violations detected" in str(exc.value)
+        assert "big_func" in str(exc.value)
+
+
+def test_enforce_function_lengths_no_violations():
+    """Test enforce_function_lengths with no violations."""
+    enforce_function_lengths([])
+    # Should not raise
+
+
+def test_enforce_function_lengths_with_violations():
+    """Test enforce_function_lengths with violations."""
+    entries = [FunctionEntry(Path("file.py"), "long_func", 10, 200)]
+    with pytest.raises(PolicyViolation) as exc:
+        enforce_function_lengths(entries, threshold=150)
+    assert "Function length policy violations detected" in str(exc.value)
+    assert "long_func" in str(exc.value)
+    assert "length 200 exceeds 150" in str(exc.value)
+
+
+def test_check_broad_excepts_no_violations():
+    """Test _check_broad_excepts with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_broad_excepts", return_value=[]):
+        _check_broad_excepts()
+        # Should not raise
+
+
+def test_check_broad_excepts_with_violations():
+    """Test _check_broad_excepts with violations."""
+    mock_excepts = [("file.py", 10), ("file.py", 20)]
+    with patch("ci_tools.scripts.policy_rules.collect_broad_excepts", return_value=mock_excepts):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_broad_excepts()
+        assert "Policy violations detected" in str(exc.value)
+        assert "broad exception handler" in str(exc.value)
+
+
+def test_check_silent_handlers_no_violations():
+    """Test _check_silent_handlers with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_silent_handlers", return_value=[]):
+        _check_silent_handlers()
+        # Should not raise
+
+
+def test_check_silent_handlers_with_violations():
+    """Test _check_silent_handlers with violations."""
+    mock_handlers = [("file.py", 10, "suppresses exception with pass")]
+    with patch("ci_tools.scripts.policy_rules.collect_silent_handlers", return_value=mock_handlers):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_silent_handlers()
+        assert "Silent exception handler detected" in str(exc.value)
+        assert "suppresses exception with pass" in str(exc.value)
+
+
+def test_check_generic_raises_no_violations():
+    """Test _check_generic_raises with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_generic_raises", return_value=[]):
+        _check_generic_raises()
+        # Should not raise
+
+
+def test_check_generic_raises_with_violations():
+    """Test _check_generic_raises with violations."""
+    mock_raises = [("file.py", 10), ("file.py", 20)]
+    with patch("ci_tools.scripts.policy_rules.collect_generic_raises", return_value=mock_raises):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_generic_raises()
+        assert "Policy violations detected" in str(exc.value)
+        assert "generic Exception raise" in str(exc.value)
+
+
+def test_check_literal_fallbacks_no_violations():
+    """Test _check_literal_fallbacks with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_literal_fallbacks", return_value=[]):
+        _check_literal_fallbacks()
+        # Should not raise
+
+
+def test_check_literal_fallbacks_with_violations():
+    """Test _check_literal_fallbacks with violations."""
+    mock_fallbacks = [("file.py", 10, "dict.get literal fallback")]
+    with patch("ci_tools.scripts.policy_rules.collect_literal_fallbacks", return_value=mock_fallbacks):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_literal_fallbacks()
+        assert "Fallback default usage detected" in str(exc.value)
+        assert "dict.get literal fallback" in str(exc.value)
+
+
+def test_check_boolean_fallbacks_no_violations():
+    """Test _check_boolean_fallbacks with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_bool_fallbacks", return_value=[]):
+        _check_boolean_fallbacks()
+        # Should not raise
+
+
+def test_check_boolean_fallbacks_with_violations():
+    """Test _check_boolean_fallbacks with violations."""
+    mock_fallbacks = [("file.py", 10), ("file.py", 20)]
+    with patch("ci_tools.scripts.policy_rules.collect_bool_fallbacks", return_value=mock_fallbacks):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_boolean_fallbacks()
+        assert "Policy violations detected" in str(exc.value)
+        assert "literal fallback via boolean 'or'" in str(exc.value)
+
+
+def test_check_conditional_literals_no_violations():
+    """Test _check_conditional_literals with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_conditional_literal_returns", return_value=[]):
+        _check_conditional_literals()
+        # Should not raise
+
+
+def test_check_conditional_literals_with_violations():
+    """Test _check_conditional_literals with violations."""
+    mock_literals = [("file.py", 10), ("file.py", 20)]
+    with patch("ci_tools.scripts.policy_rules.collect_conditional_literal_returns", return_value=mock_literals):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_conditional_literals()
+        assert "Policy violations detected" in str(exc.value)
+        assert "literal return inside None guard" in str(exc.value)
+
+
+def test_check_backward_compat_no_violations():
+    """Test _check_backward_compat with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_backward_compat_blocks", return_value=[]):
+        _check_backward_compat()
+        # Should not raise
+
+
+def test_check_backward_compat_with_violations():
+    """Test _check_backward_compat with violations."""
+    mock_compat = [("file.py", 10, "conditional legacy guard")]
+    with patch("ci_tools.scripts.policy_rules.collect_backward_compat_blocks", return_value=mock_compat):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_backward_compat()
+        assert "Backward compatibility code detected" in str(exc.value)
+        assert "conditional legacy guard" in str(exc.value)
+
+
+def test_check_legacy_artifacts_no_violations():
+    """Test _check_legacy_artifacts with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_legacy_modules", return_value=[]):
+        with patch("ci_tools.scripts.policy_rules.collect_legacy_configs", return_value=[]):
+            _check_legacy_artifacts()
+            # Should not raise
+
+
+def test_check_legacy_artifacts_modules():
+    """Test _check_legacy_artifacts with legacy modules."""
+    mock_modules = [("legacy_module.py", 1, "legacy module path")]
+    with patch("ci_tools.scripts.policy_rules.collect_legacy_modules", return_value=mock_modules):
+        with patch("ci_tools.scripts.policy_rules.collect_legacy_configs", return_value=[]):
+            with pytest.raises(PolicyViolation) as exc:
+                _check_legacy_artifacts()
+            assert "Legacy module detected" in str(exc.value)
+            assert "legacy module path" in str(exc.value)
+
+
+def test_check_legacy_artifacts_configs():
+    """Test _check_legacy_artifacts with legacy configs."""
+    mock_configs = [("config.json", 5, "legacy toggle in config")]
+    with patch("ci_tools.scripts.policy_rules.collect_legacy_modules", return_value=[]):
+        with patch("ci_tools.scripts.policy_rules.collect_legacy_configs", return_value=mock_configs):
+            with pytest.raises(PolicyViolation) as exc:
+                _check_legacy_artifacts()
+            assert "Legacy toggle detected in config" in str(exc.value)
+
+
+def test_check_sync_calls_no_violations():
+    """Test _check_sync_calls with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_forbidden_sync_calls", return_value=[]):
+        _check_sync_calls()
+        # Should not raise
+
+
+def test_check_sync_calls_with_violations():
+    """Test _check_sync_calls with violations."""
+    mock_calls = [("file.py", 10, "forbidden synchronous call 'time.sleep'")]
+    with patch("ci_tools.scripts.policy_rules.collect_forbidden_sync_calls", return_value=mock_calls):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_sync_calls()
+        assert "Synchronous call policy violations detected" in str(exc.value)
+        assert "time.sleep" in str(exc.value)
+
+
+def test_check_suppressions_no_violations():
+    """Test _check_suppressions with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_suppressions", return_value=[]):
+        _check_suppressions()
+        # Should not raise
+
+
+def test_check_suppressions_with_violations():
+    """Test _check_suppressions with violations."""
+    mock_suppressions = [("file.py", 10, "# noqa")]
+    with patch("ci_tools.scripts.policy_rules.collect_suppressions", return_value=mock_suppressions):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_suppressions()
+        assert "Suppression policy violations detected" in str(exc.value)
+        assert "# noqa" in str(exc.value)
+
+
+def test_check_duplicate_functions_no_violations():
+    """Test _check_duplicate_functions with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_duplicate_functions", return_value=[]):
+        _check_duplicate_functions()
+        # Should not raise
+
+
+def test_check_duplicate_functions_with_violations():
+    """Test _check_duplicate_functions with violations."""
+    mock_duplicates = [
+        [
+            FunctionEntry(Path("file1.py"), "helper", 10, 5),
+            FunctionEntry(Path("file2.py"), "helper", 20, 5),
+        ]
+    ]
+    with patch("ci_tools.scripts.policy_rules.collect_duplicate_functions", return_value=mock_duplicates):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_duplicate_functions()
+        assert "Duplicate helper policy violations detected" in str(exc.value)
+
+
+def test_check_bytecode_artifacts_no_violations():
+    """Test _check_bytecode_artifacts with no violations."""
+    with patch("ci_tools.scripts.policy_rules.collect_bytecode_artifacts", return_value=[]):
+        _check_bytecode_artifacts()
+        # Should not raise
+
+
+def test_check_bytecode_artifacts_with_violations():
+    """Test _check_bytecode_artifacts with violations."""
+    mock_artifacts = ["module.pyc", "__pycache__"]
+    with patch("ci_tools.scripts.policy_rules.collect_bytecode_artifacts", return_value=mock_artifacts):
+        with pytest.raises(PolicyViolation) as exc:
+            _check_bytecode_artifacts()
+        assert "Bytecode artifacts detected" in str(exc.value)
+        assert "module.pyc" in str(exc.value)
+
+
+def test_purge_bytecode_artifacts_delegates():
+    """Test purge_bytecode_artifacts delegates to collector."""
+    with patch("ci_tools.scripts.policy_rules.purge_bytecode_artifacts"):
+        purge_bytecode_artifacts()
+        # Function should exist and be callable
diff --git a/tests/test_process.py b/tests/test_process.py
new file mode 100644
index 0000000..d51f58e
--- /dev/null
+++ b/tests/test_process.py
@@ -0,0 +1,434 @@
+"""Unit tests for ci_tools.ci_runtime.process module."""
+
+from __future__ import annotations
+
+import subprocess
+from unittest.mock import MagicMock, Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.process import (
+    _run_command_buffered,
+    _run_command_streaming,
+    _stream_pipe,
+    run_command,
+    tail_text,
+    gather_git_diff,
+    gather_git_status,
+    gather_file_diff,
+    log_codex_interaction,
+)
+from ci_tools.ci_runtime.models import CommandResult
+
+
+class TestRunCommandBuffered:
+    """Tests for _run_command_buffered function."""
+
+    def test_success_returns_command_result(self):
+        """Test successful command execution returns CommandResult."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=0,
+                stdout="output text",
+                stderr="",
+            )
+            result = _run_command_buffered(
+                ["echo", "test"],
+                check=False,
+                env={},
+            )
+            assert isinstance(result, CommandResult)
+            assert result.returncode == 0
+            assert result.stdout == "output text"
+            assert result.stderr == ""
+
+    def test_failure_with_check_false_returns_result(self):
+        """Test failed command with check=False returns result."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=1,
+                stdout="",
+                stderr="error message",
+            )
+            result = _run_command_buffered(
+                ["false"],
+                check=False,
+                env={},
+            )
+            assert result.returncode == 1
+            assert result.stderr == "error message"
+
+    def test_failure_with_check_true_raises_exception(self):
+        """Test failed command with check=True raises CalledProcessError."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=127,
+                stdout="",
+                stderr="command not found",
+                args=["nonexistent"],
+            )
+            with pytest.raises(subprocess.CalledProcessError) as exc_info:
+                _run_command_buffered(
+                    ["nonexistent"],
+                    check=True,
+                    env={},
+                )
+            assert exc_info.value.returncode == 127
+
+    def test_captures_both_stdout_and_stderr(self):
+        """Test that both stdout and stderr are captured."""
+        with patch("subprocess.run") as mock_run:
+            mock_run.return_value = Mock(
+                returncode=0,
+                stdout="standard output",
+                stderr="standard error",
+            )
+            result = _run_command_buffered(
+                ["cmd"],
+                check=False,
+                env={"KEY": "value"},
+            )
+            assert result.stdout == "standard output"
+            assert result.stderr == "standard error"
+
+
+class TestStreamPipe:
+    """Tests for _stream_pipe helper function."""
+
+    def test_collects_and_forwards_lines(self):
+        """Test that lines are collected and forwarded to target."""
+        mock_pipe = MagicMock()
+        mock_pipe.readline.side_effect = ["line1\n", "line2\n", ""]
+        collector: list[str] = []
+        target = MagicMock()
+
+        _stream_pipe(mock_pipe, collector, target)
+
+        assert collector == ["line1\n", "line2\n"]
+        assert target.write.call_count == 2
+        target.write.assert_any_call("line1\n")
+        target.write.assert_any_call("line2\n")
+        target.flush.assert_called()
+
+    def test_closes_pipe_after_processing(self):
+        """Test that pipe is closed after processing."""
+        mock_pipe = MagicMock()
+        mock_pipe.readline.side_effect = [""]
+        collector: list[str] = []
+        target = MagicMock()
+
+        _stream_pipe(mock_pipe, collector, target)
+
+        mock_pipe.close.assert_called_once()
+
+
+class TestRunCommandStreaming:
+    """Tests for _run_command_streaming function."""
+
+    def test_success_streams_and_captures_output(self):
+        """Test successful streaming command captures output."""
+        with patch("subprocess.Popen") as mock_popen:
+            mock_process = MagicMock()
+            mock_process.stdout = MagicMock()
+            mock_process.stderr = MagicMock()
+            mock_process.stdout.readline.side_effect = ["out1\n", ""]
+            mock_process.stderr.readline.side_effect = ["err1\n", ""]
+            mock_process.wait.return_value = 0
+            mock_popen.return_value.__enter__.return_value = mock_process
+
+            result = _run_command_streaming(
+                ["echo", "test"],
+                check=False,
+                env={},
+            )
+
+            assert result.returncode == 0
+            assert "out1\n" in result.stdout
+            assert "err1\n" in result.stderr
+
+    def test_failure_with_check_true_raises_exception(self):
+        """Test streaming command failure with check=True raises."""
+        with patch("subprocess.Popen") as mock_popen:
+            mock_process = MagicMock()
+            mock_process.stdout = MagicMock()
+            mock_process.stderr = MagicMock()
+            mock_process.stdout.readline.side_effect = [""]
+            mock_process.stderr.readline.side_effect = ["error\n", ""]
+            mock_process.wait.return_value = 1
+            mock_process.args = ["false"]
+            mock_popen.return_value.__enter__.return_value = mock_process
+
+            with pytest.raises(subprocess.CalledProcessError) as exc_info:
+                _run_command_streaming(
+                    ["false"],
+                    check=True,
+                    env={},
+                )
+            assert exc_info.value.returncode == 1
+
+    def test_handles_none_pipes(self):
+        """Test handles when stdout or stderr are None."""
+        with patch("subprocess.Popen") as mock_popen:
+            mock_process = MagicMock()
+            mock_process.stdout = None
+            mock_process.stderr = None
+            mock_process.wait.return_value = 0
+            mock_popen.return_value.__enter__.return_value = mock_process
+
+            result = _run_command_streaming(
+                ["cmd"],
+                check=False,
+                env={},
+            )
+
+            assert result.returncode == 0
+            assert result.stdout == ""
+            assert result.stderr == ""
+
+
+class TestRunCommand:
+    """Tests for run_command public API."""
+
+    def test_buffered_mode_by_default(self):
+        """Test that buffered mode is used by default."""
+        with patch("ci_tools.ci_runtime.process._run_command_buffered") as mock_buffered:
+            mock_buffered.return_value = CommandResult(0, "out", "err")
+            result = run_command(["echo", "test"])
+            assert result.returncode == 0
+            mock_buffered.assert_called_once()
+
+    def test_streaming_mode_when_live_true(self):
+        """Test that streaming mode is used when live=True."""
+        with patch("ci_tools.ci_runtime.process._run_command_streaming") as mock_streaming:
+            mock_streaming.return_value = CommandResult(0, "out", "err")
+            result = run_command(["echo", "test"], live=True)
+            assert result.returncode == 0
+            mock_streaming.assert_called_once()
+
+    def test_merges_environment_variables(self):
+        """Test that custom env vars are merged with os.environ."""
+        with patch("ci_tools.ci_runtime.process._run_command_buffered") as mock_buffered:
+            with patch("os.environ", {"EXISTING": "value"}):
+                mock_buffered.return_value = CommandResult(0, "", "")
+                run_command(["cmd"], env={"NEW": "var"})
+                call_args = mock_buffered.call_args
+                env = call_args[1]["env"]
+                assert "EXISTING" in env
+                assert env["NEW"] == "var"
+
+    def test_converts_iterable_to_list(self):
+        """Test that iterable args are converted to list."""
+        with patch("ci_tools.ci_runtime.process._run_command_buffered") as mock_buffered:
+            mock_buffered.return_value = CommandResult(0, "", "")
+            run_command(("echo", "test"))
+            call_args = mock_buffered.call_args
+            assert isinstance(call_args[0][0], list)
+
+    def test_check_parameter_passed_through(self):
+        """Test that check parameter is passed to runner."""
+        with patch("ci_tools.ci_runtime.process._run_command_buffered") as mock_buffered:
+            mock_buffered.return_value = CommandResult(0, "", "")
+            run_command(["cmd"], check=True)
+            assert mock_buffered.call_args[1]["check"] is True
+
+
+class TestTailText:
+    """Tests for tail_text helper function."""
+
+    def test_returns_last_n_lines(self):
+        """Test returns last N lines from text."""
+        text = "line1\nline2\nline3\nline4\nline5"
+        result = tail_text(text, 3)
+        assert result == "line3\nline4\nline5"
+
+    def test_returns_all_lines_when_n_exceeds_count(self):
+        """Test returns all lines when N exceeds line count."""
+        text = "line1\nline2"
+        result = tail_text(text, 10)
+        assert result == "line1\nline2"
+
+    def test_handles_empty_string(self):
+        """Test handles empty string."""
+        result = tail_text("", 5)
+        assert result == ""
+
+    def test_handles_single_line(self):
+        """Test handles single line correctly."""
+        result = tail_text("oneline", 1)
+        assert result == "oneline"
+
+    def test_returns_all_when_zero_lines(self):
+        """Test returns all lines when requesting zero lines (edge case)."""
+        result = tail_text("line1\nline2", 0)
+        # Python list slicing [-0:] returns the entire list
+        assert result == "line1\nline2"
+
+
+class TestGatherGitDiff:
+    """Tests for gather_git_diff function."""
+
+    def test_gathers_unstaged_diff_by_default(self):
+        """Test gathers unstaged changes by default."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "diff content", "")
+            result = gather_git_diff()
+            assert result == "diff content"
+            mock_run.assert_called_once_with(["git", "diff"])
+
+    def test_gathers_staged_diff_when_specified(self):
+        """Test gathers staged changes when staged=True."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "staged diff", "")
+            result = gather_git_diff(staged=True)
+            assert result == "staged diff"
+            mock_run.assert_called_once_with(["git", "diff", "--cached"])
+
+    def test_returns_empty_when_no_diff(self):
+        """Test returns empty string when no changes."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "", "")
+            result = gather_git_diff()
+            assert result == ""
+
+
+class TestGatherGitStatus:
+    """Tests for gather_git_status function."""
+
+    def test_returns_short_status(self):
+        """Test returns short git status."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, " M file.py\n A new.py\n", "")
+            result = gather_git_status()
+            assert result == "M file.py\n A new.py"
+            mock_run.assert_called_once_with(["git", "status", "--short"])
+
+    def test_strips_whitespace(self):
+        """Test strips leading and trailing whitespace."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "  status  \n", "")
+            result = gather_git_status()
+            assert result == "status"
+
+    def test_returns_empty_on_clean_repo(self):
+        """Test returns empty string on clean repository."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "", "")
+            result = gather_git_status()
+            assert result == ""
+
+
+class TestGatherFileDiff:
+    """Tests for gather_file_diff function."""
+
+    def test_gathers_diff_for_single_file(self):
+        """Test gathers diff for specified file."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "diff for file.py", "")
+            result = gather_file_diff("src/module.py")
+            assert result == "diff for file.py"
+            mock_run.assert_called_once_with(["git", "diff", "src/module.py"])
+
+    def test_returns_empty_for_unchanged_file(self):
+        """Test returns empty string for unchanged file."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "", "")
+            result = gather_file_diff("unchanged.py")
+            assert result == ""
+
+    def test_accepts_relative_paths(self):
+        """Test accepts relative file paths."""
+        with patch("ci_tools.ci_runtime.process.run_command") as mock_run:
+            mock_run.return_value = CommandResult(0, "diff", "")
+            gather_file_diff("relative/path/file.py")
+            mock_run.assert_called_once()
+
+
+class TestLogCodexInteraction:
+    """Tests for log_codex_interaction function."""
+
+    def test_creates_log_directory(self, tmp_path):
+        """Test creates logs directory if it doesn't exist."""
+        with patch("ci_tools.ci_runtime.process.Path") as mock_path:
+            mock_log_dir = MagicMock()
+            mock_log_file = MagicMock()
+            mock_path.return_value = mock_log_dir
+            mock_log_dir.__truediv__ = MagicMock(return_value=mock_log_file)
+
+            log_codex_interaction("test", "prompt text", "response text")
+
+            mock_log_dir.mkdir.assert_called_once_with(parents=True, exist_ok=True)
+
+    def test_appends_interaction_to_log(self, tmp_path, monkeypatch):
+        """Test appends interaction to log file."""
+        monkeypatch.chdir(tmp_path)
+        log_path = tmp_path / "logs" / "codex_ci.log"
+
+        log_codex_interaction("patch request", "fix this", "here's the patch")
+
+        assert log_path.exists()
+        content = log_path.read_text(encoding="utf-8")
+        assert "--- patch request ---" in content
+        assert "Prompt:" in content
+        assert "fix this" in content
+        assert "Response:" in content
+        assert "here's the patch" in content
+
+    def test_strips_whitespace_from_content(self, tmp_path, monkeypatch):
+        """Test strips leading/trailing whitespace from logged content."""
+        monkeypatch.chdir(tmp_path)
+        log_path = tmp_path / "logs" / "codex_ci.log"
+
+        log_codex_interaction("test", "  prompt  \n", "  response  \n")
+
+        content = log_path.read_text(encoding="utf-8")
+        assert "prompt  \n" not in content
+        assert "prompt\n" in content
+
+    def test_multiple_interactions_appended(self, tmp_path, monkeypatch):
+        """Test multiple interactions are appended to same file."""
+        monkeypatch.chdir(tmp_path)
+        log_path = tmp_path / "logs" / "codex_ci.log"
+
+        log_codex_interaction("first", "prompt1", "response1")
+        log_codex_interaction("second", "prompt2", "response2")
+
+        content = log_path.read_text(encoding="utf-8")
+        assert "--- first ---" in content
+        assert "--- second ---" in content
+        assert content.count("Prompt:") == 2
+        assert content.count("Response:") == 2
+
+    def test_handles_empty_strings(self, tmp_path, monkeypatch):
+        """Test handles empty prompt or response strings."""
+        monkeypatch.chdir(tmp_path)
+        log_path = tmp_path / "logs" / "codex_ci.log"
+
+        log_codex_interaction("empty", "", "")
+
+        assert log_path.exists()
+        content = log_path.read_text(encoding="utf-8")
+        assert "--- empty ---" in content
+
+
+class TestCommandResult:
+    """Tests for CommandResult dataclass properties."""
+
+    def test_ok_property_true_on_success(self):
+        """Test ok property returns True when returncode is 0."""
+        result = CommandResult(0, "output", "")
+        assert result.ok is True
+
+    def test_ok_property_false_on_failure(self):
+        """Test ok property returns False when returncode is non-zero."""
+        result = CommandResult(1, "", "error")
+        assert result.ok is False
+
+    def test_combined_output_merges_stdout_stderr(self):
+        """Test combined_output concatenates stdout and stderr."""
+        result = CommandResult(0, "stdout text", "stderr text")
+        assert result.combined_output == "stdout textstderr text"
+
+    def test_combined_output_with_empty_streams(self):
+        """Test combined_output with empty stdout or stderr."""
+        result = CommandResult(0, "", "")
+        assert result.combined_output == ""
diff --git a/tests/test_structure_guard.py b/tests/test_structure_guard.py
new file mode 100644
index 0000000..07ac42b
--- /dev/null
+++ b/tests/test_structure_guard.py
@@ -0,0 +1,433 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import structure_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_parse_args_defaults():
+    """Test argument parsing with defaults."""
+    args = structure_guard.parse_args([])
+    assert args.root == Path("src")
+    assert args.max_class_lines == 100
+    assert args.exclude == []
+
+
+def test_parse_args_custom_values():
+    """Test argument parsing with custom values."""
+    args = structure_guard.parse_args(
+        ["--root", "custom", "--max-class-lines", "50", "--exclude", "tests"]
+    )
+    assert args.root == Path("custom")
+    assert args.max_class_lines == 50
+    assert args.exclude == [Path("tests")]
+
+
+def test_iter_python_files_single_file(tmp_path: Path):
+    """Test iter_python_files with a single file."""
+    py_file = tmp_path / "test.py"
+    py_file.write_text("# test")
+
+    files = list(structure_guard.iter_python_files(py_file))
+    assert len(files) == 1
+    assert files[0] == py_file
+
+
+def test_iter_python_files_non_python_file(tmp_path: Path):
+    """Test iter_python_files with a non-Python file."""
+    txt_file = tmp_path / "test.txt"
+    txt_file.write_text("# test")
+
+    files = list(structure_guard.iter_python_files(txt_file))
+    assert len(files) == 0
+
+
+def test_iter_python_files_directory(tmp_path: Path):
+    """Test iter_python_files with a directory."""
+    (tmp_path / "file1.py").write_text("# file1")
+    (tmp_path / "file2.py").write_text("# file2")
+    (tmp_path / "subdir").mkdir()
+    (tmp_path / "subdir" / "file3.py").write_text("# file3")
+
+    files = list(structure_guard.iter_python_files(tmp_path))
+    assert len(files) == 3
+
+
+def test_is_excluded_basic():
+    """Test basic exclusion logic."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/src").resolve()]
+    assert structure_guard.is_excluded(path, exclusions) is True
+
+
+def test_is_excluded_no_match():
+    """Test exclusion with no match."""
+    path = Path("/project/src/module.py").resolve()
+    exclusions = [Path("/project/tests").resolve()]
+    assert structure_guard.is_excluded(path, exclusions) is False
+
+
+def test_class_line_span_basic(tmp_path: Path):
+    """Test class_line_span with basic class."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def method(self):
+                pass
+        """
+    ).strip()
+
+    tree = structure_guard.ast.parse(source)
+    class_node = tree.body[0]
+    start, end = structure_guard.class_line_span(class_node)
+
+    assert start == 1
+    assert end == 3
+
+
+def test_class_line_span_no_end_lineno(tmp_path: Path):
+    """Test class_line_span when end_lineno is None."""
+    source = "class Foo: pass"
+    tree = structure_guard.ast.parse(source)
+    class_node = tree.body[0]
+
+    # Simulate missing end_lineno
+    if hasattr(class_node, "end_lineno"):
+        delattr(class_node, "end_lineno")
+
+    start, end = structure_guard.class_line_span(class_node)
+    assert start == 1
+    assert end >= start
+
+
+def test_class_line_span_nested_content(tmp_path: Path):
+    """Test class_line_span with nested content."""
+    source = textwrap.dedent(
+        """
+        class Foo:
+            def method1(self):
+                x = 1
+                return x
+
+            def method2(self):
+                y = 2
+                return y
+        """
+    ).strip()
+
+    tree = structure_guard.ast.parse(source)
+    class_node = tree.body[0]
+    start, end = structure_guard.class_line_span(class_node)
+
+    assert start == 1
+    assert end == 8
+
+
+def test_scan_file_within_limit(tmp_path: Path):
+    """Test scanning a file within the line limit."""
+    py_file = tmp_path / "small.py"
+    write_module(
+        py_file,
+        """
+        class SmallClass:
+            def method(self):
+                return 1
+        """,
+    )
+
+    violations = structure_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+
+
+def test_scan_file_exceeds_limit(tmp_path: Path):
+    """Test scanning a file that exceeds the limit."""
+    py_file = tmp_path / "large.py"
+    methods = "\n".join([f"    def method_{i}(self):\n        pass" for i in range(20)])
+    content = f"class LargeClass:\n{methods}"
+    py_file.write_text(content)
+
+    violations = structure_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 1
+    assert violations[0][0] == py_file
+    assert violations[0][1] == "LargeClass"
+    assert violations[0][2] == 1  # Line number
+    assert violations[0][3] > 10  # Line count
+
+
+def test_scan_file_multiple_classes(tmp_path: Path):
+    """Test scanning a file with multiple classes."""
+    py_file = tmp_path / "multi.py"
+    write_module(
+        py_file,
+        """
+        class SmallClass:
+            def method(self):
+                pass
+
+        class LargeClass:
+            def method1(self):
+                x = 1
+                y = 2
+                z = 3
+                return x + y + z
+
+            def method2(self):
+                pass
+
+            def method3(self):
+                pass
+        """,
+    )
+
+    violations = structure_guard.scan_file(py_file, limit=5)
+    assert len(violations) == 1
+    assert violations[0][1] == "LargeClass"
+
+
+def test_scan_file_syntax_error(tmp_path: Path):
+    """Test scan_file with syntax error."""
+    py_file = tmp_path / "bad.py"
+    py_file.write_text("class Foo:\n    def method(self\n")  # Missing closing paren
+
+    with pytest.raises(RuntimeError, match="failed to parse Python source"):
+        structure_guard.scan_file(py_file, limit=10)
+
+
+def test_main_success_no_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    write_module(
+        root / "small.py",
+        """
+        class SmallClass:
+            def method(self):
+                return 1
+        """,
+    )
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root), "--max-class-lines", "10"])
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert captured.err == ""
+
+
+def test_main_detects_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+    py_file = root / "large.py"
+
+    methods = "\n".join([f"    def method_{i}(self):\n        pass" for i in range(20)])
+    content = f"class LargeClass:\n{methods}"
+    py_file.write_text(content)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root), "--max-class-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Oversized classes detected" in captured.err
+    assert "LargeClass" in captured.err
+
+
+def test_main_respects_exclusions(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function respects exclusion patterns."""
+    root = tmp_path / "src"
+    excluded = root / "excluded"
+    root.mkdir()
+    excluded.mkdir(parents=True)
+
+    large_class = "class LargeClass:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "included.py").write_text(large_class)
+    (excluded / "excluded.py").write_text(large_class)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(
+            ["--root", str(root), "--max-class-lines", "10", "--exclude", str(excluded)]
+        )
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "included.py" in captured.err
+    assert "excluded.py" not in captured.err
+
+
+def test_main_handles_multiple_violations(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles multiple violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_class = "class LargeClass:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "file1.py").write_text(large_class)
+    (root / "file2.py").write_text(large_class)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root), "--max-class-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "file1.py" in captured.err
+    assert "file2.py" in captured.err
+
+
+def test_main_prints_violations_sorted(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function prints violations in sorted order."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_class = "class LargeClass:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "zebra.py").write_text(large_class)
+    (root / "alpha.py").write_text(large_class)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root), "--max-class-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    err_lines = [
+        line for line in captured.err.split("\n") if "alpha.py" in line or "zebra.py" in line
+    ]
+    assert len(err_lines) == 2
+    assert "alpha.py" in err_lines[0]
+    assert "zebra.py" in err_lines[1]
+
+
+def test_main_scan_file_error(tmp_path: Path, capsys: pytest.CaptureFixture, monkeypatch):
+    """Test main function handles scan_file errors."""
+    root = tmp_path / "src"
+    root.mkdir()
+    (root / "test.py").write_text("class Foo: pass")
+
+    def mock_scan_file(path, limit):
+        raise RuntimeError("Test error")
+
+    monkeypatch.setattr(structure_guard, "scan_file", mock_scan_file)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root)])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Test error" in captured.err
+
+
+def test_scan_file_no_classes(tmp_path: Path):
+    """Test scanning a file with no classes."""
+    py_file = tmp_path / "no_classes.py"
+    write_module(
+        py_file,
+        """
+        def function():
+            pass
+
+        x = 1
+        """,
+    )
+
+    violations = structure_guard.scan_file(py_file, limit=10)
+    assert len(violations) == 0
+
+
+def test_class_line_span_single_line_class():
+    """Test class_line_span with single-line class."""
+    source = "class Foo: pass"
+    tree = structure_guard.ast.parse(source)
+    class_node = tree.body[0]
+    start, end = structure_guard.class_line_span(class_node)
+
+    assert start == 1
+    assert end == 1
+
+
+def test_main_handles_relative_paths(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function handles relative paths correctly."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    large_class = "class LargeClass:\n" + "\n".join(
+        [f"    def method_{i}(self):\n        pass" for i in range(20)]
+    )
+    (root / "module.py").write_text(large_class)
+
+    with patch("pathlib.Path.cwd", return_value=tmp_path):
+        result = structure_guard.main(["--root", str(root), "--max-class-lines", "10"])
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "module.py" in captured.err
+    assert "LargeClass" in captured.err
+
+
+def test_scan_file_class_with_decorators(tmp_path: Path):
+    """Test scanning a class with decorators."""
+    py_file = tmp_path / "decorated.py"
+    write_module(
+        py_file,
+        """
+        @dataclass
+        class DecoratedClass:
+            field1: int
+            field2: str
+
+            def method(self):
+                pass
+        """,
+    )
+
+    violations = structure_guard.scan_file(py_file, limit=5)
+    assert len(violations) == 1
+
+
+def test_scan_file_nested_classes(tmp_path: Path):
+    """Test scanning file with nested classes."""
+    py_file = tmp_path / "nested.py"
+    write_module(
+        py_file,
+        """
+        class OuterClass:
+            def method1(self):
+                pass
+
+            class InnerClass:
+                def inner_method(self):
+                    pass
+        """,
+    )
+
+    violations = structure_guard.scan_file(py_file, limit=3)
+    # Both outer and inner should be checked
+    assert len(violations) >= 1
+
+
+def test_iter_python_files_empty_directory(tmp_path: Path):
+    """Test iter_python_files with empty directory."""
+    files = list(structure_guard.iter_python_files(tmp_path))
+    assert len(files) == 0
+
+
+def test_is_excluded_multiple_exclusions():
+    """Test exclusion with multiple patterns."""
+    path = Path("/project/tests/test_module.py").resolve()
+    exclusions = [
+        Path("/project/vendor").resolve(),
+        Path("/project/tests").resolve(),
+    ]
+    assert structure_guard.is_excluded(path, exclusions) is True
diff --git a/tests/test_unused_module_guard.py b/tests/test_unused_module_guard.py
new file mode 100644
index 0000000..4cd1037
--- /dev/null
+++ b/tests/test_unused_module_guard.py
@@ -0,0 +1,601 @@
+from __future__ import annotations
+
+import textwrap
+from pathlib import Path
+from unittest.mock import patch
+
+import pytest
+
+from ci_tools.scripts import unused_module_guard
+
+
+def write_module(path: Path, content: str) -> None:
+    """Helper to write Python module content."""
+    path.write_text(textwrap.dedent(content).strip() + "\n", encoding="utf-8")
+
+
+def test_import_collector_simple_import():
+    """Test ImportCollector with simple import."""
+    source = "import foo"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+
+
+def test_import_collector_dotted_import():
+    """Test ImportCollector with dotted import."""
+    source = "import foo.bar.baz"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+    assert "foo.bar" in collector.imports
+    assert "foo.bar.baz" in collector.imports
+
+
+def test_import_collector_from_import():
+    """Test ImportCollector with from import."""
+    source = "from foo.bar import baz"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+    assert "foo.bar" in collector.imports
+
+
+def test_import_collector_strips_src_prefix():
+    """Test ImportCollector strips src. prefix."""
+    source = "import src.foo.bar"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+    assert "foo.bar" in collector.imports
+    assert "src.foo" not in collector.imports
+
+
+def test_import_collector_from_import_strips_src():
+    """Test ImportCollector strips src. prefix from from imports."""
+    source = "from src.foo.bar import baz"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+    assert "foo.bar" in collector.imports
+
+
+def test_import_collector_multiple_imports():
+    """Test ImportCollector with multiple imports."""
+    source = textwrap.dedent(
+        """
+        import foo
+        import bar.baz
+        from qux import quux
+        """
+    ).strip()
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    assert "foo" in collector.imports
+    assert "bar" in collector.imports
+    assert "bar.baz" in collector.imports
+    assert "qux" in collector.imports
+
+
+def test_import_collector_from_import_no_module():
+    """Test ImportCollector with from import without module."""
+    source = "from . import foo"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    # Should not crash
+    assert isinstance(collector.imports, set)
+
+
+def test_collect_all_imports(tmp_path: Path):
+    """Test collecting all imports from directory."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(
+        root / "module1.py",
+        """
+        import foo
+        from bar import baz
+        """,
+    )
+    write_module(
+        root / "module2.py",
+        """
+        import qux
+        """,
+    )
+
+    imports = unused_module_guard.collect_all_imports(root)
+
+    assert "foo" in imports
+    assert "bar" in imports
+    assert "qux" in imports
+
+
+def test_collect_all_imports_skips_pycache(tmp_path: Path):
+    """Test that __pycache__ is skipped."""
+    root = tmp_path / "src"
+    pycache = root / "__pycache__"
+    root.mkdir()
+    pycache.mkdir()
+
+    write_module(root / "module.py", "import foo")
+    (pycache / "module.pyc").write_bytes(b"compiled")
+
+    imports = unused_module_guard.collect_all_imports(root)
+    assert "foo" in imports
+
+
+def test_collect_all_imports_handles_syntax_errors(tmp_path: Path):
+    """Test that syntax errors are handled gracefully."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    (root / "bad.py").write_text("import foo\nclass Bar:\n    def method(self\n")
+    write_module(root / "good.py", "import baz")
+
+    imports = unused_module_guard.collect_all_imports(root)
+    assert "baz" in imports
+
+
+def test_collect_all_imports_handles_unicode_errors(tmp_path: Path):
+    """Test that unicode errors are handled gracefully."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    (root / "bad.py").write_bytes(b"\xff\xfe\x00\x00")
+    write_module(root / "good.py", "import baz")
+
+    imports = unused_module_guard.collect_all_imports(root)
+    assert "baz" in imports
+
+
+def test_get_module_name_basic(tmp_path: Path):
+    """Test getting module name from file path."""
+    root = tmp_path / "src"
+    root.mkdir()
+    file_path = root / "module.py"
+
+    module_name = unused_module_guard.get_module_name(file_path, root)
+    assert module_name == "module"
+
+
+def test_get_module_name_nested(tmp_path: Path):
+    """Test getting module name from nested path."""
+    root = tmp_path / "src"
+    file_path = root / "foo" / "bar" / "baz.py"
+
+    module_name = unused_module_guard.get_module_name(file_path, root)
+    assert module_name == "foo.bar.baz"
+
+
+def test_get_module_name_init(tmp_path: Path):
+    """Test getting module name from __init__.py."""
+    root = tmp_path / "src"
+    file_path = root / "foo" / "__init__.py"
+
+    module_name = unused_module_guard.get_module_name(file_path, root)
+    assert module_name == "foo"
+
+
+def test_get_module_name_root_init(tmp_path: Path):
+    """Test getting module name from root __init__.py."""
+    root = tmp_path / "src"
+    file_path = root / "__init__.py"
+
+    module_name = unused_module_guard.get_module_name(file_path, root)
+    assert module_name == ""
+
+
+def test_is_false_positive_temperature():
+    """Test false positive detection for temperature patterns."""
+    assert unused_module_guard._is_false_positive("max_temp") is True
+    assert unused_module_guard._is_false_positive("temperature") is True
+    assert unused_module_guard._is_false_positive("_temp_only") is False
+
+
+def test_is_false_positive_phase_2():
+    """Test false positive detection for phase_2 pattern."""
+    assert unused_module_guard._is_false_positive("phase_2") is True
+    assert unused_module_guard._is_false_positive("feature_v2") is True
+    assert unused_module_guard._is_false_positive("_2_only") is False
+
+
+def test_duplicate_reason_suspicious():
+    """Test detecting suspicious duplicate patterns."""
+    assert unused_module_guard._duplicate_reason("module_old") is not None
+    assert unused_module_guard._duplicate_reason("module_backup") is not None
+    assert unused_module_guard._duplicate_reason("module_refactored") is not None
+    assert unused_module_guard._duplicate_reason("module_temp") is not None
+
+
+def test_duplicate_reason_not_suspicious():
+    """Test that normal names are not flagged."""
+    assert unused_module_guard._duplicate_reason("module") is None
+    assert unused_module_guard._duplicate_reason("normal_name") is None
+
+
+def test_duplicate_reason_false_positive():
+    """Test that false positives are not flagged."""
+    assert unused_module_guard._duplicate_reason("max_temp") is None
+    assert unused_module_guard._duplicate_reason("phase_2") is None
+
+
+def test_find_suspicious_duplicates(tmp_path: Path):
+    """Test finding suspicious duplicate files."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    (root / "module.py").write_text("# normal")
+    (root / "module_old.py").write_text("# old")
+    (root / "module_backup.py").write_text("# backup")
+
+    duplicates = unused_module_guard.find_suspicious_duplicates(root)
+
+    assert len(duplicates) == 2
+    paths = [str(d[0].name) for d in duplicates]
+    assert "module_old.py" in paths
+    assert "module_backup.py" in paths
+    assert "module.py" not in paths
+
+
+def test_find_suspicious_duplicates_skips_pycache(tmp_path: Path):
+    """Test that __pycache__ is skipped."""
+    root = tmp_path / "src"
+    pycache = root / "__pycache__"
+    root.mkdir()
+    pycache.mkdir()
+
+    (pycache / "module_old.pyc").write_bytes(b"compiled")
+
+    duplicates = unused_module_guard.find_suspicious_duplicates(root)
+    assert len(duplicates) == 0
+
+
+def test_should_skip_file_pycache():
+    """Test that __pycache__ files are skipped."""
+    py_file = Path("/project/src/__pycache__/module.py")
+    assert unused_module_guard._should_skip_file(py_file, []) is True
+
+
+def test_should_skip_file_main():
+    """Test that __main__.py is skipped."""
+    py_file = Path("/project/src/__main__.py")
+    assert unused_module_guard._should_skip_file(py_file, []) is True
+
+
+def test_should_skip_file_exclude_pattern():
+    """Test that exclude patterns work."""
+    py_file = Path("/project/src/test_module.py")
+    assert unused_module_guard._should_skip_file(py_file, ["test_"]) is True
+
+
+def test_should_skip_file_normal():
+    """Test that normal files are not skipped."""
+    py_file = Path("/project/src/module.py")
+    assert unused_module_guard._should_skip_file(py_file, []) is False
+
+
+def test_module_is_imported_exact_match():
+    """Test module_is_imported with exact match."""
+    all_imports = {"foo", "bar.baz"}
+    assert unused_module_guard._module_is_imported("foo", "foo", all_imports) is True
+
+
+def test_module_is_imported_stem_match():
+    """Test module_is_imported with stem match."""
+    all_imports = {"foo", "bar"}
+    assert unused_module_guard._module_is_imported("bar.baz", "baz", all_imports) is True
+
+
+def test_module_is_imported_partial_match():
+    """Test module_is_imported with partial match."""
+    all_imports = {"foo.bar"}
+    assert unused_module_guard._module_is_imported("foo.bar.baz", "baz", all_imports) is True
+
+
+def test_module_is_imported_no_match():
+    """Test module_is_imported with no match."""
+    all_imports = {"foo", "bar"}
+    assert unused_module_guard._module_is_imported("qux.quux", "quux", all_imports) is False
+
+
+def test_module_is_imported_empty_name():
+    """Test module_is_imported with empty name."""
+    all_imports = {"foo"}
+    assert unused_module_guard._module_is_imported("", "file", all_imports) is True
+
+
+def test_find_unused_modules(tmp_path: Path):
+    """Test finding unused modules."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "used.py", "def foo(): pass")
+    write_module(root / "unused.py", "def bar(): pass")
+    write_module(
+        root / "main.py",
+        """
+        import used
+        """,
+    )
+
+    unused = unused_module_guard.find_unused_modules(root, exclude_patterns=["__init__.py"])
+
+    # unused.py should be flagged as unused
+    assert len(unused) >= 1
+    unused_names = [str(u[0].name) for u in unused]
+    assert "unused.py" in unused_names
+
+
+def test_find_unused_modules_excludes_init(tmp_path: Path):
+    """Test that __init__.py is excluded."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "__init__.py", "# init")
+
+    unused = unused_module_guard.find_unused_modules(root, exclude_patterns=["__init__.py"])
+
+    # __init__.py should not be in unused list
+    unused_names = [str(u[0].name) for u in unused]
+    assert "__init__.py" not in unused_names
+
+
+def test_find_unused_modules_with_parent_imports(tmp_path: Path):
+    """Test that imports from parent directory are considered."""
+    parent = tmp_path
+    root = parent / "src"
+    root.mkdir()
+
+    write_module(root / "module.py", "def foo(): pass")
+    write_module(parent / "other.py", "from src import module")
+
+    unused = unused_module_guard.find_unused_modules(root, exclude_patterns=["__init__.py"])
+
+    # module.py should not be flagged as unused
+    unused_names = [str(u[0].name) for u in unused]
+    assert "module.py" not in unused_names
+
+
+def test_main_no_unused_modules(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with no unused modules."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "used.py", "def foo(): pass")
+    write_module(
+        root / "main.py",
+        """
+        import used
+        """,
+    )
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(root)]):
+        result = unused_module_guard.main()
+
+    assert result == 0
+    captured = capsys.readouterr()
+    assert "No unused modules found" in captured.out
+
+
+def test_main_detects_unused_modules(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects unused modules."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "unused.py", "def bar(): pass")
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(root)]):
+        result = unused_module_guard.main()
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Unused modules detected" in captured.out
+    assert "unused.py" in captured.out
+
+
+def test_main_detects_suspicious_duplicates(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function detects suspicious duplicates."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "module.py", "def foo(): pass")
+    write_module(root / "module_old.py", "def foo_old(): pass")
+    write_module(
+        root / "main.py",
+        """
+        import module
+        import module_old
+        """,
+    )
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(root)]):
+        result = unused_module_guard.main()
+
+    # Should pass in non-strict mode
+    assert result == 0
+    captured = capsys.readouterr()
+    assert "Suspicious duplicate" in captured.out
+
+
+def test_main_strict_mode_fails_on_duplicates(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function in strict mode fails on duplicates."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "module.py", "def foo(): pass")
+    write_module(root / "module_old.py", "def foo_old(): pass")
+    write_module(
+        root / "main.py",
+        """
+        import module
+        import module_old
+        """,
+    )
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(root), "--strict"]):
+        result = unused_module_guard.main()
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Suspicious duplicate" in captured.out
+
+
+def test_main_root_does_not_exist(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function when root does not exist."""
+    missing = tmp_path / "missing"
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(missing)]):
+        result = unused_module_guard.main()
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "does not exist" in captured.err
+
+
+def test_main_custom_exclude_patterns(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test main function with custom exclude patterns."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "test_module.py", "def test(): pass")
+    write_module(root / "module.py", "def foo(): pass")
+
+    with patch(
+        "sys.argv",
+        ["unused_module_guard.py", "--root", str(root), "--exclude", "test_", "module.py"],
+    ):
+        result = unused_module_guard.main()
+
+    # Both should be excluded
+    assert result == 0
+
+
+def test_find_unused_modules_nested_packages(tmp_path: Path):
+    """Test finding unused modules in nested packages."""
+    root = tmp_path / "src"
+    pkg = root / "package"
+    root.mkdir()
+    pkg.mkdir()
+
+    write_module(pkg / "__init__.py", "")
+    write_module(pkg / "used.py", "def foo(): pass")
+    write_module(pkg / "unused.py", "def bar(): pass")
+    write_module(
+        root / "main.py",
+        """
+        from package import used
+        """,
+    )
+
+    unused = unused_module_guard.find_unused_modules(root, exclude_patterns=["__init__.py"])
+
+    unused_names = [str(u[0].name) for u in unused]
+    assert "unused.py" in unused_names
+    assert "used.py" not in unused_names
+
+
+def test_import_collector_aliased_import():
+    """Test ImportCollector with aliased imports."""
+    source = "import foo.bar as fb"
+    tree = unused_module_guard.ast.parse(source)
+    collector = unused_module_guard.ImportCollector()
+    collector.visit(tree)
+
+    # Should still track the original module name
+    assert "foo" in collector.imports
+    assert "foo.bar" in collector.imports
+
+
+def test_suspicious_patterns_coverage():
+    """Test coverage of all suspicious patterns."""
+    patterns = [
+        "_refactored",
+        "_slim",
+        "_optimized",
+        "_old",
+        "_backup",
+        "_copy",
+        "_new",
+        "_temp",
+        "_v2",
+        "_2",
+    ]
+
+    for pattern in patterns:
+        filename = f"module{pattern}"
+        reason = unused_module_guard._duplicate_reason(filename)
+        assert reason is not None, f"Pattern {pattern} should be detected"
+
+
+def test_collect_all_imports_empty_directory(tmp_path: Path):
+    """Test collecting imports from empty directory."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    imports = unused_module_guard.collect_all_imports(root)
+    assert len(imports) == 0
+
+
+def test_find_suspicious_duplicates_empty_directory(tmp_path: Path):
+    """Test finding duplicates in empty directory."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    duplicates = unused_module_guard.find_suspicious_duplicates(root)
+    assert len(duplicates) == 0
+
+
+def test_main_prints_tip(tmp_path: Path, capsys: pytest.CaptureFixture):
+    """Test that main prints helpful tip on violations."""
+    root = tmp_path / "src"
+    root.mkdir()
+
+    write_module(root / "unused.py", "def bar(): pass")
+
+    with patch("sys.argv", ["unused_module_guard.py", "--root", str(root)]):
+        result = unused_module_guard.main()
+
+    assert result == 1
+    captured = capsys.readouterr()
+    assert "Tip:" in captured.out
+    assert ".gitignore" in captured.out
+
+
+def test_collect_all_imports_with_parent(tmp_path: Path):
+    """Test _collect_all_imports_with_parent includes parent directory."""
+    parent = tmp_path
+    root = parent / "src"
+    root.mkdir()
+
+    write_module(root / "child.py", "import foo")
+    write_module(parent / "parent.py", "import bar")
+
+    imports = unused_module_guard._collect_all_imports_with_parent(root)
+
+    assert "foo" in imports
+    assert "bar" in imports
+
+
+def test_module_is_imported_with_partial_paths():
+    """Test module matching with various partial paths."""
+    all_imports = {"foo.bar.baz"}
+
+    # Should match any partial path
+    assert unused_module_guard._module_is_imported("foo.bar.baz.qux", "qux", all_imports) is True
+    assert unused_module_guard._module_is_imported("foo.bar", "bar", all_imports) is True
diff --git a/tests/test_workflow.py b/tests/test_workflow.py
new file mode 100644
index 0000000..126ba98
--- /dev/null
+++ b/tests/test_workflow.py
@@ -0,0 +1,833 @@
+"""Unit tests for ci_tools.ci_runtime.workflow module."""
+
+from __future__ import annotations
+
+import os
+from unittest.mock import Mock, patch
+
+import pytest
+
+from ci_tools.ci_runtime.workflow import (
+    _resolve_model_choice,
+    _resolve_reasoning_choice,
+    _derive_runtime_flags,
+    configure_runtime,
+    perform_dry_run,
+    _collect_worktree_diffs,
+    _worktree_is_clean,
+    _stage_if_needed,
+    _warn_missing_staged_changes,
+    _maybe_request_commit_message,
+    _maybe_push_or_notify,
+    finalize_worktree,
+    run_repair_iterations,
+    parse_args,
+    main,
+)
+from ci_tools.ci_runtime.models import (
+    CiAbort,
+    ModelSelectionAbort,
+    PatchLifecycleAbort,
+    ReasoningEffortAbort,
+    RuntimeOptions,
+)
+
+
+class TestResolveModelChoice:
+    """Tests for _resolve_model_choice function."""
+
+    def test_accepts_required_model(self):
+        """Test accepting the required model."""
+        result = _resolve_model_choice("gpt-5-codex")
+        assert result == "gpt-5-codex"
+        assert os.environ["OPENAI_MODEL"] == "gpt-5-codex"
+
+    def test_rejects_unsupported_model(self):
+        """Test rejecting unsupported model."""
+        with pytest.raises(ModelSelectionAbort) as exc_info:
+            _resolve_model_choice("gpt-4")
+        assert "requires" in str(exc_info.value)
+        assert "gpt-5-codex" in str(exc_info.value)
+
+    def test_uses_env_var_when_no_arg(self):
+        """Test using OPENAI_MODEL env var when no argument provided."""
+        with patch.dict(os.environ, {"OPENAI_MODEL": "gpt-5-codex"}):
+            result = _resolve_model_choice(None)
+            assert result == "gpt-5-codex"
+
+    def test_defaults_to_required_model(self):
+        """Test defaulting to required model when no arg or env."""
+        with patch.dict(os.environ, {}, clear=True):
+            result = _resolve_model_choice(None)
+            assert result == "gpt-5-codex"
+
+    def test_rejects_env_var_with_wrong_model(self):
+        """Test rejecting wrong model from environment variable."""
+        with patch.dict(os.environ, {"OPENAI_MODEL": "gpt-3.5-turbo"}):
+            with pytest.raises(ModelSelectionAbort):
+                _resolve_model_choice(None)
+
+
+class TestResolveReasoningChoice:
+    """Tests for _resolve_reasoning_choice function."""
+
+    def test_accepts_valid_low(self):
+        """Test accepting 'low' reasoning effort."""
+        result = _resolve_reasoning_choice("low")
+        assert result == "low"
+        assert os.environ["OPENAI_REASONING_EFFORT"] == "low"
+
+    def test_accepts_valid_medium(self):
+        """Test accepting 'medium' reasoning effort."""
+        result = _resolve_reasoning_choice("medium")
+        assert result == "medium"
+
+    def test_accepts_valid_high(self):
+        """Test accepting 'high' reasoning effort."""
+        result = _resolve_reasoning_choice("high")
+        assert result == "high"
+
+    def test_rejects_invalid_choice(self):
+        """Test rejecting invalid reasoning effort."""
+        with pytest.raises(ReasoningEffortAbort) as exc_info:
+            _resolve_reasoning_choice("extreme")
+        assert "expected one of" in str(exc_info.value)
+
+    def test_uses_env_var_when_no_arg(self):
+        """Test using OPENAI_REASONING_EFFORT env var."""
+        with patch.dict(os.environ, {"OPENAI_REASONING_EFFORT": "MEDIUM"}):
+            result = _resolve_reasoning_choice(None)
+            assert result == "medium"
+
+    def test_defaults_to_high(self):
+        """Test defaulting to 'high' when no arg or env."""
+        with patch.dict(os.environ, {}, clear=True):
+            result = _resolve_reasoning_choice(None)
+            assert result == "high"
+
+    def test_case_insensitive_env_var(self):
+        """Test environment variable is case-insensitive."""
+        with patch.dict(os.environ, {"OPENAI_REASONING_EFFORT": "LOW"}):
+            result = _resolve_reasoning_choice(None)
+            assert result == "low"
+
+
+class TestDeriveRuntimeFlags:
+    """Tests for _derive_runtime_flags function."""
+
+    def test_automation_mode_enabled_for_ci_sh(self):
+        """Test automation mode enabled when command is ci.sh."""
+        args = Mock(auto_stage=False, commit_message=False)
+        command_tokens = ["./scripts/ci.sh"]
+        (
+            automation_mode,
+            command_env,
+            auto_stage_enabled,
+            commit_message_enabled,
+            auto_push_enabled,
+        ) = _derive_runtime_flags(args, command_tokens)
+        assert automation_mode is True
+        assert command_env == {"CI_AUTOMATION": "1"}
+        assert auto_stage_enabled is True
+        assert commit_message_enabled is True
+        assert auto_push_enabled is True
+
+    def test_automation_mode_disabled_for_other_commands(self):
+        """Test automation mode disabled for non-ci.sh commands."""
+        args = Mock(auto_stage=False, commit_message=False)
+        command_tokens = ["pytest"]
+        (
+            automation_mode,
+            command_env,
+            auto_stage_enabled,
+            commit_message_enabled,
+            auto_push_enabled,
+        ) = _derive_runtime_flags(args, command_tokens)
+        assert automation_mode is False
+        assert command_env == {}
+        assert auto_stage_enabled is False
+        assert commit_message_enabled is False
+        assert auto_push_enabled is False
+
+    def test_auto_stage_flag_overrides(self):
+        """Test auto_stage flag overrides default."""
+        args = Mock(auto_stage=True, commit_message=False)
+        command_tokens = ["pytest"]
+        (
+            automation_mode,
+            command_env,
+            auto_stage_enabled,
+            commit_message_enabled,
+            auto_push_enabled,
+        ) = _derive_runtime_flags(args, command_tokens)
+        assert auto_stage_enabled is True
+
+    def test_commit_message_flag_overrides(self):
+        """Test commit_message flag overrides default."""
+        args = Mock(auto_stage=False, commit_message=True)
+        command_tokens = ["make", "test"]
+        (
+            automation_mode,
+            command_env,
+            auto_stage_enabled,
+            commit_message_enabled,
+            auto_push_enabled,
+        ) = _derive_runtime_flags(args, command_tokens)
+        assert commit_message_enabled is True
+
+    def test_empty_command_tokens(self):
+        """Test handling of empty command tokens."""
+        args = Mock(auto_stage=False, commit_message=False)
+        command_tokens = []
+        (
+            automation_mode,
+            command_env,
+            auto_stage_enabled,
+            commit_message_enabled,
+            auto_push_enabled,
+        ) = _derive_runtime_flags(args, command_tokens)
+        assert automation_mode is False
+
+
+class TestConfigureRuntime:
+    """Tests for configure_runtime function."""
+
+    @patch("ci_tools.ci_runtime.workflow.load_env_settings")
+    @patch("ci_tools.ci_runtime.workflow._resolve_model_choice")
+    @patch("ci_tools.ci_runtime.workflow._resolve_reasoning_choice")
+    def test_creates_runtime_options(
+        self, mock_reasoning, mock_model, mock_load_env
+    ):
+        """Test creating RuntimeOptions from parsed args."""
+        mock_model.return_value = "gpt-5-codex"
+        mock_reasoning.return_value = "high"
+
+        args = Mock(
+            command="./scripts/ci.sh",
+            env_file="~/.env",
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            patch_approval_mode="prompt",
+            auto_stage=False,
+            commit_message=False,
+        )
+
+        options = configure_runtime(args)
+
+        assert isinstance(options, RuntimeOptions)
+        assert options.command_tokens == ["./scripts/ci.sh"]
+        assert options.model_name == "gpt-5-codex"
+        assert options.reasoning_effort == "high"
+        assert options.patch_approval_mode == "prompt"
+        mock_load_env.assert_called_once_with("~/.env")
+
+    @patch("ci_tools.ci_runtime.workflow.load_env_settings")
+    @patch("ci_tools.ci_runtime.workflow._resolve_model_choice")
+    @patch("ci_tools.ci_runtime.workflow._resolve_reasoning_choice")
+    def test_handles_automation_mode(
+        self, mock_reasoning, mock_model, mock_load_env
+    ):
+        """Test automation mode flags are set correctly."""
+        mock_model.return_value = "gpt-5-codex"
+        mock_reasoning.return_value = "high"
+
+        args = Mock(
+            command="./scripts/ci.sh",
+            env_file="~/.env",
+            model=None,
+            reasoning_effort=None,
+            patch_approval_mode="auto",
+            auto_stage=False,
+            commit_message=False,
+        )
+
+        options = configure_runtime(args)
+
+        assert options.automation_mode is True
+        assert options.auto_stage_enabled is True
+        assert options.commit_message_enabled is True
+        assert options.auto_push_enabled is True
+
+    @patch("ci_tools.ci_runtime.workflow.load_env_settings")
+    @patch("ci_tools.ci_runtime.workflow._resolve_model_choice")
+    @patch("ci_tools.ci_runtime.workflow._resolve_reasoning_choice")
+    def test_parses_command_with_spaces(
+        self, mock_reasoning, mock_model, mock_load_env
+    ):
+        """Test parsing command with spaces and arguments."""
+        mock_model.return_value = "gpt-5-codex"
+        mock_reasoning.return_value = "medium"
+
+        args = Mock(
+            command='pytest tests/ -v --cov=src',
+            env_file="~/.env",
+            model=None,
+            reasoning_effort=None,
+            patch_approval_mode="prompt",
+            auto_stage=False,
+            commit_message=False,
+        )
+
+        options = configure_runtime(args)
+
+        assert options.command_tokens == ["pytest", "tests/", "-v", "--cov=src"]
+
+
+class TestPerformDryRun:
+    """Tests for perform_dry_run function."""
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    def test_executes_command_when_dry_run_enabled(self, mock_run):
+        """Test executing CI command once in dry-run mode."""
+        mock_run.return_value = Mock(returncode=0)
+        args = Mock(dry_run=True, command="./scripts/ci.sh")
+        options = Mock(command_tokens=["./scripts/ci.sh"], command_env={})
+
+        result = perform_dry_run(args, options)
+
+        assert result == 0
+        mock_run.assert_called_once_with(
+            ["./scripts/ci.sh"], live=True, env={}
+        )
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    def test_returns_failure_code(self, mock_run):
+        """Test returning failure exit code in dry-run mode."""
+        mock_run.return_value = Mock(returncode=1)
+        args = Mock(dry_run=True, command="pytest")
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        result = perform_dry_run(args, options)
+
+        assert result == 1
+
+    def test_returns_none_when_dry_run_disabled(self):
+        """Test returning None when dry-run is disabled."""
+        args = Mock(dry_run=False)
+        options = Mock()
+
+        result = perform_dry_run(args, options)
+
+        assert result is None
+
+
+class TestCollectWorktreeDiffs:
+    """Tests for _collect_worktree_diffs function."""
+
+    @patch("ci_tools.ci_runtime.workflow.gather_git_diff")
+    def test_collects_unstaged_and_staged_diffs(self, mock_gather):
+        """Test collecting both unstaged and staged diffs."""
+        mock_gather.side_effect = ["unstaged content", "staged content"]
+
+        unstaged, staged = _collect_worktree_diffs()
+
+        assert unstaged == "unstaged content"
+        assert staged == "staged content"
+        assert mock_gather.call_count == 2
+        mock_gather.assert_any_call(staged=False)
+        mock_gather.assert_any_call(staged=True)
+
+
+class TestWorktreeIsClean:
+    """Tests for _worktree_is_clean function."""
+
+    def test_clean_when_no_diffs(self):
+        """Test worktree is clean with no diffs."""
+        assert _worktree_is_clean("", "") is True
+
+    def test_not_clean_with_unstaged_diff(self):
+        """Test worktree not clean with unstaged changes."""
+        assert _worktree_is_clean("diff content", "") is False
+
+    def test_not_clean_with_staged_diff(self):
+        """Test worktree not clean with staged changes."""
+        assert _worktree_is_clean("", "diff content") is False
+
+    def test_not_clean_with_both_diffs(self):
+        """Test worktree not clean with both types of changes."""
+        assert _worktree_is_clean("unstaged", "staged") is False
+
+
+class TestStageIfNeeded:
+    """Tests for _stage_if_needed function."""
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.gather_git_diff")
+    def test_stages_all_changes_when_enabled(self, mock_gather, mock_run):
+        """Test staging all changes when auto-stage is enabled."""
+        mock_gather.return_value = "new staged diff"
+        options = Mock(auto_stage_enabled=True)
+
+        result = _stage_if_needed(options, "old staged diff")
+
+        assert result == "new staged diff"
+        mock_run.assert_called_once_with(["git", "add", "-A"], check=True)
+
+    def test_returns_existing_staged_diff_when_disabled(self):
+        """Test returning existing diff when auto-stage is disabled."""
+        options = Mock(auto_stage_enabled=False)
+
+        result = _stage_if_needed(options, "existing diff")
+
+        assert result == "existing diff"
+
+
+class TestWarnMissingStagedChanges:
+    """Tests for _warn_missing_staged_changes function."""
+
+    def test_prints_warning_to_stderr(self, capsys):
+        """Test warning is printed to stderr."""
+        _warn_missing_staged_changes()
+
+        captured = capsys.readouterr()
+        assert "No staged changes detected" in captured.err
+        assert "Stage files before requesting a commit message" in captured.err
+
+
+class TestMaybeRequestCommitMessage:
+    """Tests for _maybe_request_commit_message function."""
+
+    @patch("ci_tools.ci_runtime.workflow.request_commit_message")
+    def test_requests_commit_message_when_enabled(self, mock_request):
+        """Test requesting commit message when enabled."""
+        mock_request.return_value = ("feat: add feature", ["Details here"])
+        options = Mock(
+            commit_message_enabled=True,
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            auto_push_enabled=False,
+        )
+
+        summary, body = _maybe_request_commit_message(
+            options, "staged diff", "extra context"
+        )
+
+        assert summary == "feat: add feature"
+        assert body == ["Details here"]
+        mock_request.assert_called_once_with(
+            model="gpt-5-codex",
+            reasoning_effort="high",
+            staged_diff="staged diff",
+            extra_context="extra context",
+            detailed=False,
+        )
+
+    def test_returns_none_when_disabled(self):
+        """Test returning None when commit message is disabled."""
+        options = Mock(commit_message_enabled=False)
+
+        summary, body = _maybe_request_commit_message(options, "diff", "")
+
+        assert summary is None
+        assert body == []
+
+    @patch("ci_tools.ci_runtime.workflow.request_commit_message")
+    def test_detailed_mode_for_auto_push(self, mock_request):
+        """Test detailed mode enabled for auto-push."""
+        mock_request.return_value = ("commit", [])
+        options = Mock(
+            commit_message_enabled=True,
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            auto_push_enabled=True,
+        )
+
+        _maybe_request_commit_message(options, "diff", "")
+
+        mock_request.assert_called_once()
+        assert mock_request.call_args[1]["detailed"] is True
+
+
+class TestMaybePushOrNotify:
+    """Tests for _maybe_push_or_notify function."""
+
+    @patch("ci_tools.ci_runtime.workflow.commit_and_push")
+    def test_commits_and_pushes_when_auto_push_enabled(self, mock_commit):
+        """Test committing and pushing when auto-push is enabled."""
+        options = Mock(auto_push_enabled=True)
+
+        _maybe_push_or_notify(options, "commit summary", ["body line"])
+
+        mock_commit.assert_called_once_with(
+            "commit summary", ["body line"], push=True
+        )
+
+    @patch("ci_tools.ci_runtime.workflow.commit_and_push")
+    def test_uses_default_summary_when_none(self, mock_commit):
+        """Test using default summary when None provided."""
+        options = Mock(auto_push_enabled=True)
+
+        _maybe_push_or_notify(options, None, [])
+
+        mock_commit.assert_called_once_with("Automated commit", [], push=True)
+
+    def test_prints_notification_when_auto_push_disabled(self, capsys):
+        """Test printing notification when auto-push is disabled."""
+        options = Mock(auto_push_enabled=False)
+
+        _maybe_push_or_notify(options, "summary", ["body"])
+
+        captured = capsys.readouterr()
+        assert "Commit message ready" in captured.out
+
+    def test_no_notification_when_no_summary(self, capsys):
+        """Test no notification when summary is None."""
+        options = Mock(auto_push_enabled=False)
+
+        _maybe_push_or_notify(options, None, [])
+
+        captured = capsys.readouterr()
+        assert captured.out == ""
+
+
+class TestFinalizeWorktree:
+    """Tests for finalize_worktree function."""
+
+    @patch("ci_tools.ci_runtime.workflow._collect_worktree_diffs")
+    def test_returns_zero_when_worktree_clean(self, mock_collect):
+        """Test returning 0 when worktree is clean."""
+        mock_collect.return_value = ("", "")
+        args = Mock()
+        options = Mock()
+
+        result = finalize_worktree(args, options)
+
+        assert result == 0
+
+    @patch("ci_tools.ci_runtime.workflow._collect_worktree_diffs")
+    @patch("ci_tools.ci_runtime.workflow._stage_if_needed")
+    def test_warns_when_no_staged_changes_after_staging(
+        self, mock_stage, mock_collect
+    ):
+        """Test warning when no staged changes after staging."""
+        mock_collect.return_value = ("unstaged", "")
+        mock_stage.return_value = ""
+        args = Mock(commit_extra_context="")
+        options = Mock(auto_stage_enabled=True, commit_message_enabled=False)
+
+        result = finalize_worktree(args, options)
+
+        assert result == 0
+
+    @patch("ci_tools.ci_runtime.workflow._collect_worktree_diffs")
+    @patch("ci_tools.ci_runtime.workflow._stage_if_needed")
+    @patch("ci_tools.ci_runtime.workflow._maybe_request_commit_message")
+    @patch("ci_tools.ci_runtime.workflow._maybe_push_or_notify")
+    def test_complete_workflow_with_staged_changes(
+        self, mock_push, mock_request, mock_stage, mock_collect
+    ):
+        """Test complete finalization workflow with staged changes."""
+        mock_collect.return_value = ("", "staged diff")
+        mock_stage.return_value = "staged diff"
+        mock_request.return_value = ("summary", ["body"])
+        args = Mock(commit_extra_context="context")
+        options = Mock(
+            auto_stage_enabled=False,
+            commit_message_enabled=True,
+            model_name="gpt-5-codex",
+            reasoning_effort="high",
+            auto_push_enabled=False,
+        )
+
+        result = finalize_worktree(args, options)
+
+        assert result == 0
+        mock_request.assert_called_once()
+        mock_push.assert_called_once_with(options, "summary", ["body"])
+
+
+class TestRunRepairIterations:
+    """Tests for run_repair_iterations function."""
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.extract_coverage_deficits")
+    def test_succeeds_on_first_iteration(self, mock_coverage, mock_run):
+        """Test successful CI on first iteration."""
+        mock_run.return_value = Mock(returncode=0, ok=True, combined_output="")
+        mock_coverage.return_value = None
+        args = Mock(command="pytest", max_iterations=5)
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        run_repair_iterations(args, options)
+
+        assert mock_run.call_count == 1
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.extract_coverage_deficits")
+    @patch("ci_tools.ci_runtime.workflow.build_failure_context")
+    @patch("ci_tools.ci_runtime.workflow.request_and_apply_patches")
+    def test_iterates_until_success(
+        self, mock_patches, mock_failure, mock_coverage, mock_run
+    ):
+        """Test iterating until CI succeeds."""
+        # First two iterations fail, third succeeds
+        mock_run.side_effect = [
+            Mock(returncode=1, ok=False, combined_output="fail1"),
+            Mock(returncode=1, ok=False, combined_output="fail2"),
+            Mock(returncode=0, ok=True, combined_output="success"),
+        ]
+        mock_coverage.return_value = None
+        mock_failure.return_value = Mock()
+        args = Mock(command="pytest", max_iterations=5)
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        run_repair_iterations(args, options)
+
+        assert mock_run.call_count == 3
+        assert mock_patches.call_count == 2
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.extract_coverage_deficits")
+    @patch("ci_tools.ci_runtime.workflow.build_failure_context")
+    @patch("ci_tools.ci_runtime.workflow.request_and_apply_patches")
+    def test_raises_when_max_iterations_exceeded(
+        self, mock_patches, mock_failure, mock_coverage, mock_run
+    ):
+        """Test raising exception when max iterations exceeded."""
+        mock_run.return_value = Mock(returncode=1, ok=False, combined_output="fail")
+        mock_coverage.return_value = None
+        mock_failure.return_value = Mock()
+        args = Mock(command="pytest", max_iterations=2, log_tail=200)
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        with pytest.raises(PatchLifecycleAbort) as exc_info:
+            run_repair_iterations(args, options)
+
+        assert "unable to obtain a valid patch" in str(exc_info.value)
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.extract_coverage_deficits")
+    @patch("ci_tools.ci_runtime.workflow.build_failure_context")
+    @patch("ci_tools.ci_runtime.workflow.request_and_apply_patches")
+    def test_handles_coverage_deficits(
+        self, mock_patches, mock_failure, mock_coverage, mock_run
+    ):
+        """Test handling coverage deficits even when CI passes."""
+        mock_run.side_effect = [
+            Mock(returncode=0, ok=True, combined_output="coverage: 70%"),
+            Mock(returncode=0, ok=True, combined_output="coverage: 82%"),
+        ]
+        mock_coverage.side_effect = [Mock(deficits=[Mock()]), None]
+        mock_failure.return_value = Mock()
+        args = Mock(command="pytest", max_iterations=5)
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        run_repair_iterations(args, options)
+
+        assert mock_run.call_count == 2
+        assert mock_patches.call_count == 1
+
+    @patch("ci_tools.ci_runtime.workflow.run_command")
+    @patch("ci_tools.ci_runtime.workflow.extract_coverage_deficits")
+    @patch("ci_tools.ci_runtime.workflow.build_failure_context")
+    @patch("ci_tools.ci_runtime.workflow.request_and_apply_patches")
+    def test_passes_correct_iteration_number(
+        self, mock_patches, mock_failure, mock_coverage, mock_run
+    ):
+        """Test correct iteration number passed to patch function."""
+        mock_run.side_effect = [
+            Mock(returncode=1, ok=False, combined_output="fail"),
+            Mock(returncode=0, ok=True, combined_output="success"),
+        ]
+        mock_coverage.return_value = None
+        mock_failure.return_value = Mock()
+        args = Mock(command="pytest", max_iterations=5)
+        options = Mock(command_tokens=["pytest"], command_env={})
+
+        run_repair_iterations(args, options)
+
+        # Check iteration numbers in calls
+        calls = mock_patches.call_args_list
+        assert calls[0][1]["iteration"] == 1
+
+
+class TestParseArgs:
+    """Tests for parse_args function."""
+
+    def test_default_values(self):
+        """Test default argument values."""
+        args = parse_args([])
+
+        assert args.command == "./scripts/ci.sh"
+        assert args.max_iterations == 5
+        assert args.log_tail == 200
+        assert args.patch_approval_mode == "prompt"
+        assert args.dry_run is False
+        assert args.auto_stage is False
+        assert args.commit_message is False
+
+    def test_custom_command(self):
+        """Test custom command argument."""
+        args = parse_args(["--command", "make test"])
+
+        assert args.command == "make test"
+
+    def test_max_iterations(self):
+        """Test max-iterations argument."""
+        args = parse_args(["--max-iterations", "10"])
+
+        assert args.max_iterations == 10
+
+    def test_log_tail(self):
+        """Test log-tail argument."""
+        args = parse_args(["--log-tail", "500"])
+
+        assert args.log_tail == 500
+
+    def test_model_argument(self):
+        """Test model argument."""
+        args = parse_args(["--model", "gpt-5-codex"])
+
+        assert args.model == "gpt-5-codex"
+
+    def test_reasoning_effort_choices(self):
+        """Test reasoning effort choices."""
+        for choice in ["low", "medium", "high"]:
+            args = parse_args(["--reasoning-effort", choice])
+            assert args.reasoning_effort == choice
+
+    def test_patch_approval_mode_choices(self):
+        """Test patch approval mode choices."""
+        for mode in ["prompt", "auto"]:
+            args = parse_args(["--patch-approval-mode", mode])
+            assert args.patch_approval_mode == mode
+
+    def test_boolean_flags(self):
+        """Test boolean flag arguments."""
+        args = parse_args(
+            ["--dry-run", "--auto-stage", "--commit-message"]
+        )
+
+        assert args.dry_run is True
+        assert args.auto_stage is True
+        assert args.commit_message is True
+
+    def test_env_file_default(self):
+        """Test env-file default value."""
+        args = parse_args([])
+
+        assert args.env_file == "~/.env"
+
+    def test_env_file_custom(self):
+        """Test custom env-file argument."""
+        args = parse_args(["--env-file", "/custom/.env"])
+
+        assert args.env_file == "/custom/.env"
+
+    def test_commit_extra_context(self):
+        """Test commit-extra-context argument."""
+        args = parse_args(["--commit-extra-context", "Fix bug #123"])
+
+        assert args.commit_extra_context == "Fix bug #123"
+
+    def test_max_patch_lines(self):
+        """Test max-patch-lines argument."""
+        args = parse_args(["--max-patch-lines", "2000"])
+
+        assert args.max_patch_lines == 2000
+
+    def test_patch_retries(self):
+        """Test patch-retries argument."""
+        args = parse_args(["--patch-retries", "3"])
+
+        assert args.patch_retries == 3
+
+
+class TestMain:
+    """Tests for main function."""
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    @patch("ci_tools.ci_runtime.workflow.perform_dry_run")
+    def test_dry_run_exits_early(self, mock_dry_run, mock_config, mock_parse):
+        """Test dry-run mode exits early."""
+        mock_parse.return_value = Mock()
+        mock_config.return_value = Mock()
+        mock_dry_run.return_value = 0
+
+        result = main([])
+
+        assert result == 0
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    @patch("ci_tools.ci_runtime.workflow.perform_dry_run")
+    @patch("ci_tools.ci_runtime.workflow.run_repair_iterations")
+    @patch("ci_tools.ci_runtime.workflow.finalize_worktree")
+    def test_successful_workflow(
+        self, mock_finalize, mock_repair, mock_dry_run, mock_config, mock_parse
+    ):
+        """Test successful workflow execution."""
+        mock_parse.return_value = Mock()
+        mock_config.return_value = Mock()
+        mock_dry_run.return_value = None
+        mock_finalize.return_value = 0
+
+        result = main([])
+
+        assert result == 0
+        mock_repair.assert_called_once()
+        mock_finalize.assert_called_once()
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    @patch("ci_tools.ci_runtime.workflow.perform_dry_run")
+    @patch("ci_tools.ci_runtime.workflow.run_repair_iterations")
+    def test_handles_keyboard_interrupt(
+        self, mock_repair, mock_dry_run, mock_config, mock_parse
+    ):
+        """Test handling Ctrl-C gracefully."""
+        mock_parse.return_value = Mock()
+        mock_config.return_value = Mock()
+        mock_dry_run.return_value = None
+        mock_repair.side_effect = KeyboardInterrupt()
+
+        result = main([])
+
+        assert result == 130
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    @patch("ci_tools.ci_runtime.workflow.perform_dry_run")
+    @patch("ci_tools.ci_runtime.workflow.run_repair_iterations")
+    def test_handles_ci_abort(
+        self, mock_repair, mock_dry_run, mock_config, mock_parse
+    ):
+        """Test handling CiAbort exception."""
+        mock_parse.return_value = Mock()
+        mock_config.return_value = Mock()
+        mock_dry_run.return_value = None
+        mock_repair.side_effect = CiAbort(detail="test abort", code=42)
+
+        result = main([])
+
+        assert result == 42
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    def test_handles_model_selection_abort(self, mock_config, mock_parse):
+        """Test handling model selection abort."""
+        mock_parse.return_value = Mock()
+        mock_config.side_effect = ModelSelectionAbort(detail="wrong model", code=1)
+
+        result = main([])
+
+        assert result == 1
+
+    @patch("ci_tools.ci_runtime.workflow.parse_args")
+    @patch("ci_tools.ci_runtime.workflow.configure_runtime")
+    @patch("ci_tools.ci_runtime.workflow.perform_dry_run")
+    @patch("ci_tools.ci_runtime.workflow.run_repair_iterations")
+    @patch("ci_tools.ci_runtime.workflow.finalize_worktree")
+    def test_passes_custom_argv(
+        self, mock_finalize, mock_repair, mock_dry_run, mock_config, mock_parse
+    ):
+        """Test passing custom argv to parse_args."""
+        mock_parse.return_value = Mock()
+        mock_config.return_value = Mock()
+        mock_dry_run.return_value = None
+        mock_finalize.return_value = 0
+
+        main(["--command", "pytest"])
+
+        mock_parse.assert_called_once_with(["--command", "pytest"])

        ```

        Provide a single-line commit message in past tense (no trailing punctuation).
Use the diff shown above instead of running shell commands such as `diff --git`.
Response:
Refined CI automation toolchain and expanded guard coverage

